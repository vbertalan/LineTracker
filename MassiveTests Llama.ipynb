{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for reading HuggingFace token\n",
    "\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name meta-llama/Llama-2-7b-chat-hf. Creating a new one with MEAN pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-2-7b-chat-hf is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:333\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(\u001b[38;5;28mstr\u001b[39m(e), response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json (Request ID: Root=1-6669fa90-2b8f1d7374bce9e72ae2aa33;4c89c307-c65d-4169-8754-4287730c5ad8)\n\nPlease enable access to public gated repositories in your fine-grained token settings to view this repository.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1371\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1369\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m         \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1371\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1372\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1375\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n\u001b[1;32m   1377\u001b[0m \u001b[38;5;66;03m# From now on, etag and commit_hash are not None.\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 16\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhuggingface\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HuggingFaceEmbedding\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# loads BAAI/bge-small-en\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# embed_model = HuggingFaceEmbedding()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# loads BAAI/bge-small-en-v1.5\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m embed_model \u001b[38;5;241m=\u001b[39m \u001b[43mHuggingFaceEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Llama-2-7b-chat-hf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_huggingface_token\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_index/embeddings/huggingface/base.py:86\u001b[0m, in \u001b[0;36mHuggingFaceEmbedding.__init__\u001b[0;34m(self, model_name, tokenizer_name, pooling, max_length, query_instruction, text_instruction, normalize, model, tokenizer, embed_batch_size, cache_folder, trust_remote_code, device, callback_manager, **model_kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe `model_name` argument must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquery\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_instruction\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_query_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_instruction\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mget_text_instruct_for_model_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_length:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model\u001b[38;5;241m.\u001b[39mmax_seq_length \u001b[38;5;241m=\u001b[39m max_length\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:205\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, cache_folder, trust_remote_code, revision, token, use_auth_token, truncate_dim)\u001b[0m\n\u001b[1;32m    197\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    198\u001b[0m             model_name_or_path,\n\u001b[1;32m    199\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m             trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[1;32m    203\u001b[0m         )\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    210\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    214\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1197\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code)\u001b[0m\n\u001b[1;32m   1189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;124;03mCreates a simple Transformer + Mean Pooling model and returns the modules\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo sentence-transformers model found with name \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. Creating a new one with MEAN pooling.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   1194\u001b[0m         model_name_or_path\n\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m )\n\u001b[0;32m-> 1197\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtoken\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrust_remote_code\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrevision\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1203\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [transformer_model, pooling_model]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:35\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, cache_dir, tokenizer_args, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_keys \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_seq_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdo_lower_case\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_lower_case \u001b[38;5;241m=\u001b[39m do_lower_case\n\u001b[0;32m---> 35\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     39\u001b[0m     tokenizer_name_or_path \u001b[38;5;28;01mif\u001b[39;00m tokenizer_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model_name_or_path,\n\u001b[1;32m     40\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_args,\n\u001b[1;32m     42\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:425\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors:\n\u001b[1;32m    424\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this file, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory containing a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Llama-2-7b-chat-hf is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "# !pip install llama-index\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "#models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "#models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"meta-llama/Llama-2-7b-chat-hf\", token=get_huggingface_token())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/vbertalan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 1.62k/1.62k [00:00<00:00, 6.97MB/s]\n",
      "tokenizer.model: 100%|██████████| 500k/500k [00:00<00:00, 7.08MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.84M/1.84M [00:00<00:00, 10.3MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 414/414 [00:00<00:00, 1.24MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/vbertalan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import llm as llm_embedding\n",
    "\n",
    "def get_huggingface_llama():\n",
    "    f = open(\"huggingface_llama.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "def embeddings_robin(logs):    \n",
    "    models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "    model_name = models_names[0]\n",
    "    init_embedder = llm_embedding.generate_embeddings_llm(model_name=model_name,token=get_huggingface_llama(), use_cpu=True)\n",
    "    ## using pooling by mean\n",
    "    pooling_fn = llm_embedding.get_pooling_function(\"mean\")\n",
    "    #pooling_fn = lambda embedding:embedding\n",
    "    embedder = lambda logs: init_embedder(logs, pooling_fn,limit_tokens=100,precision=np.float16)# type: ignore\n",
    "    return (embedder)\n",
    "\n",
    "logs = [\"teste de hello world\"]\n",
    "print(embeddings_robin(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 614/614 [00:00<00:00, 1.97MB/s]\n",
      "model.safetensors.index.json: 100%|██████████| 26.8k/26.8k [00:00<00:00, 33.3MB/s]\n",
      "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_huggingface_llama():\n",
    "    f = open(\"huggingface_llama.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=get_huggingface_llama())\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "384\n",
      "[[ 6.76569194e-02  6.34959415e-02  4.87130657e-02  7.93049932e-02\n",
      "   3.74480300e-02  2.65274337e-03  3.93749289e-02 -7.09844287e-03\n",
      "   5.93614802e-02  3.15369740e-02  6.00980595e-02 -5.29052056e-02\n",
      "   4.06067893e-02 -2.59308517e-02  2.98427530e-02  1.12685491e-03\n",
      "   7.35148564e-02 -5.03818914e-02 -1.22386612e-01  2.37028673e-02\n",
      "   2.97264997e-02  4.24768440e-02  2.56337337e-02  1.99521589e-03\n",
      "  -5.69190979e-02 -2.71598622e-02 -3.29036154e-02  6.60248473e-02\n",
      "   1.19007125e-01 -4.58791181e-02 -7.26214945e-02 -3.25840190e-02\n",
      "   5.23413830e-02  4.50552814e-02  8.25299695e-03  3.67023870e-02\n",
      "  -1.39415003e-02  6.53919056e-02 -2.64272653e-02  2.06416167e-04\n",
      "  -1.36643285e-02 -3.62809561e-02 -1.95043534e-02 -2.89737992e-02\n",
      "   3.94270234e-02 -8.84090662e-02  2.62428960e-03  1.36714028e-02\n",
      "   4.83063199e-02 -3.11565325e-02 -1.17329165e-01 -5.11689857e-02\n",
      "  -8.85288268e-02 -2.18962487e-02  1.42986663e-02  4.44167629e-02\n",
      "  -1.34815052e-02  7.43393004e-02  2.66382117e-02 -1.98762156e-02\n",
      "   1.79191120e-02 -1.06052393e-02 -9.04262811e-02  2.13269107e-02\n",
      "   1.41204923e-01 -6.47173868e-03 -1.40389206e-03 -1.53609915e-02\n",
      "  -8.73571783e-02  7.22174048e-02  2.01403555e-02  4.25587222e-02\n",
      "  -3.49014066e-02  3.19576589e-04 -8.02970976e-02 -3.27472873e-02\n",
      "   2.85268184e-02 -5.13657965e-02  1.09389238e-01  8.19328427e-02\n",
      "  -9.84039754e-02 -9.34094638e-02 -1.51292300e-02  4.51248698e-02\n",
      "   4.94171754e-02 -2.51868200e-02  1.57077573e-02 -1.29290760e-01\n",
      "   5.31896111e-03  4.02343087e-03 -2.34572571e-02 -6.72982410e-02\n",
      "   2.92280912e-02 -2.60845274e-02  1.30625358e-02 -3.11662946e-02\n",
      "  -4.82713394e-02 -5.58859669e-02 -3.87504995e-02  1.20010801e-01\n",
      "  -1.03923976e-02  4.89705168e-02  5.53536862e-02  4.49359193e-02\n",
      "  -4.00969479e-03 -1.02959722e-01 -2.92968377e-02 -5.83401918e-02\n",
      "   2.70472262e-02 -2.20169313e-02 -7.22241178e-02 -4.13869806e-02\n",
      "  -1.93297379e-02  2.73331045e-03  2.76908133e-04 -9.67588201e-02\n",
      "  -1.00574739e-01 -1.41922571e-02 -8.07892084e-02  4.53925356e-02\n",
      "   2.45040916e-02  5.97613975e-02 -7.38185570e-02  1.19843697e-02\n",
      "  -6.63403496e-02 -7.69044459e-02  3.85158174e-02 -5.59361926e-33\n",
      "   2.80012954e-02 -5.60784787e-02 -4.86601703e-02  2.15569399e-02\n",
      "   6.01980388e-02 -4.81403321e-02 -3.50246839e-02  1.93314105e-02\n",
      "  -1.75151210e-02 -3.89210247e-02 -3.81060434e-03 -1.70287918e-02\n",
      "   2.82099620e-02  1.28290718e-02  4.71600965e-02  6.21029623e-02\n",
      "  -6.43589273e-02  1.29285604e-01 -1.31230829e-02  5.23069352e-02\n",
      "  -3.73680554e-02  2.89093722e-02 -1.68981366e-02 -2.37330087e-02\n",
      "  -3.33491601e-02 -5.16762510e-02  1.55356824e-02  2.08803173e-02\n",
      "  -1.25371814e-02  4.59579118e-02  3.72720696e-02  2.80566756e-02\n",
      "  -5.90005033e-02 -1.16987964e-02  4.92182337e-02  4.70328815e-02\n",
      "   7.35487789e-02 -3.70529927e-02  3.98458075e-03  1.06412666e-02\n",
      "  -1.61581644e-04 -5.27166389e-02  2.75927223e-02 -3.92922200e-02\n",
      "   8.44718143e-02  4.86860089e-02 -4.85870428e-03  1.79948714e-02\n",
      "  -4.28570546e-02  1.23375515e-02  6.39954442e-03  4.04822566e-02\n",
      "   1.48887634e-02 -1.53941205e-02  7.62947574e-02  2.37042997e-02\n",
      "   4.45238017e-02  5.08195013e-02 -2.31257663e-03 -1.88737381e-02\n",
      "  -1.23335747e-02  4.66001891e-02 -5.63438497e-02  6.29927218e-02\n",
      "  -3.15534808e-02  3.24912034e-02  2.34673079e-02 -6.55438155e-02\n",
      "   2.01709028e-02  2.57081948e-02 -1.23868436e-02 -8.36496241e-03\n",
      "  -6.64378032e-02  9.43074003e-02 -3.57093178e-02 -3.42483446e-02\n",
      "  -6.66357623e-03 -8.01525172e-03 -3.09710968e-02  4.33012061e-02\n",
      "  -8.21397360e-03 -1.50794998e-01  3.07692029e-02  4.00718227e-02\n",
      "  -3.79293859e-02  1.93215930e-03  4.00530212e-02 -8.77074748e-02\n",
      "  -3.68491486e-02  8.57955962e-03 -3.19251902e-02 -1.25258164e-02\n",
      "   7.35539496e-02  1.34738500e-03  2.05919128e-02  2.71097999e-33\n",
      "  -5.18577285e-02  5.78360334e-02 -9.18985307e-02  3.94421481e-02\n",
      "   1.05576508e-01 -1.96912363e-02  6.18402846e-02 -7.63464794e-02\n",
      "   2.40880270e-02  9.40048918e-02 -1.16535529e-01  3.71198319e-02\n",
      "   5.22424839e-02 -3.95854004e-03  5.72214201e-02  5.32849506e-03\n",
      "   1.24016851e-01  1.39022367e-02 -1.10249836e-02  3.56053375e-02\n",
      "  -3.30754742e-02  8.16573426e-02 -1.52003039e-02  6.05584905e-02\n",
      "  -6.01397082e-02  3.26102749e-02 -3.48296389e-02 -1.69881918e-02\n",
      "  -9.74907801e-02 -2.71483641e-02  1.74712867e-03 -7.68982321e-02\n",
      "  -4.31858376e-02 -1.89985149e-02 -2.91661117e-02  5.77488393e-02\n",
      "   2.41821446e-02 -1.16902078e-02 -6.21435009e-02  2.84351856e-02\n",
      "  -2.37558430e-04 -2.51783095e-02  4.39636875e-03  8.12840462e-02\n",
      "   3.64184231e-02 -6.04006313e-02 -3.65517251e-02 -7.93748423e-02\n",
      "  -5.08533372e-03  6.69698864e-02 -1.17784336e-01  3.23743857e-02\n",
      "  -4.71251197e-02 -1.34459455e-02 -9.48445052e-02  8.24953709e-03\n",
      "  -1.06748296e-02 -6.81881756e-02  1.11820037e-03  2.48020291e-02\n",
      "  -6.35889322e-02  2.84492467e-02 -2.61303578e-02  8.58110711e-02\n",
      "   1.14682183e-01 -5.35344705e-02 -5.63587807e-02  4.26009223e-02\n",
      "   1.09454030e-02  2.09579561e-02  1.00131243e-01  3.26051563e-02\n",
      "  -1.84208736e-01 -3.93208824e-02 -6.91454783e-02 -6.38104603e-02\n",
      "  -6.56386465e-02 -6.41251123e-03 -4.79612909e-02 -7.68133104e-02\n",
      "   2.95384042e-02 -2.29948610e-02  4.17036973e-02 -2.50047408e-02\n",
      "  -4.54512890e-03 -4.17136811e-02 -1.32289426e-02 -6.38357848e-02\n",
      "  -2.46472657e-03 -1.37338545e-02  1.68976802e-02 -6.30397648e-02\n",
      "   8.98880586e-02  4.18170057e-02 -1.85687207e-02 -1.80442132e-08\n",
      "  -1.67998020e-02 -3.21577899e-02  6.30384311e-02 -4.13092822e-02\n",
      "   4.44820188e-02  2.02471670e-03  6.29592985e-02 -5.17370412e-03\n",
      "  -1.00443950e-02 -3.05640362e-02  3.52672487e-02  5.58581576e-02\n",
      "  -4.67124619e-02  3.45102623e-02  3.29578184e-02  4.30115052e-02\n",
      "   2.94360910e-02 -3.03164441e-02 -1.71107706e-02  7.37485215e-02\n",
      "  -5.47910221e-02  2.77515352e-02  6.20162115e-03  1.58800762e-02\n",
      "   3.42978872e-02 -5.15753916e-03  2.35080142e-02  7.53135085e-02\n",
      "   1.92843806e-02  3.36197056e-02  5.09103984e-02  1.52497023e-01\n",
      "   1.64207779e-02  2.70528477e-02  3.75162289e-02  2.18554195e-02\n",
      "   5.66333272e-02 -3.95747162e-02  7.12313280e-02 -5.41376807e-02\n",
      "   1.03767740e-03  2.11853981e-02 -3.56308930e-02  1.09017007e-01\n",
      "   2.76520546e-03  3.13996971e-02  1.38420798e-03 -3.45737673e-02\n",
      "  -4.59277630e-02  2.88083535e-02  7.16907298e-03  4.84685116e-02\n",
      "   2.61018164e-02 -9.44073126e-03  2.82169096e-02  3.48723345e-02\n",
      "   3.69098485e-02 -8.58950149e-03 -3.53205800e-02 -2.47856248e-02\n",
      "  -1.91921443e-02  3.80707309e-02  5.99653721e-02 -4.22287434e-02]\n",
      " [ 8.64385664e-02  1.02762625e-01  5.39454166e-03  2.04444164e-03\n",
      "  -9.96333454e-03  2.53855214e-02  4.92875576e-02 -3.06265596e-02\n",
      "   6.87254444e-02  1.01365438e-02  7.75397271e-02 -9.00807083e-02\n",
      "   6.10617455e-03 -5.69898672e-02  1.41714960e-02  2.80491635e-02\n",
      "  -8.68464559e-02  7.64399245e-02 -1.03491269e-01 -6.77437708e-02\n",
      "   6.99947551e-02  8.44251588e-02 -7.24916626e-03  1.04770456e-02\n",
      "   1.34020764e-02  6.77576512e-02 -9.42085981e-02 -3.71689610e-02\n",
      "   5.22617437e-02 -3.10853198e-02 -9.63406563e-02  1.57717150e-02\n",
      "   2.57866848e-02  7.85245001e-02  7.89949521e-02  1.91516727e-02\n",
      "   1.64356511e-02  3.10086948e-03  3.81311402e-02  2.37090643e-02\n",
      "   1.05389738e-02 -4.40644845e-02  4.41738293e-02 -2.58727986e-02\n",
      "   6.15378730e-02 -4.05427553e-02 -8.64140615e-02  3.19722705e-02\n",
      "  -8.90733209e-04 -2.44436972e-02 -9.19721425e-02  2.33939476e-02\n",
      "  -8.30293596e-02  4.41510677e-02 -2.49692462e-02  6.23020530e-02\n",
      "  -1.30351551e-03  7.51395524e-02  2.46384740e-02 -6.47244602e-02\n",
      "  -1.17727757e-01  3.83391753e-02 -9.11767855e-02  6.35446012e-02\n",
      "   7.62739480e-02 -8.80241394e-02  9.54553578e-03 -4.69717197e-02\n",
      "  -8.41741189e-02  3.88823561e-02 -1.14393584e-01  6.28859131e-03\n",
      "  -3.49361598e-02  2.39750519e-02 -3.31317261e-02 -1.57243796e-02\n",
      "  -3.78955342e-02 -8.81249085e-03  7.06118420e-02  3.28066535e-02\n",
      "   2.03673192e-03 -1.12278983e-01  6.79721963e-03  1.22765750e-02\n",
      "   3.35303769e-02 -1.36200516e-02 -2.25489754e-02 -2.25229338e-02\n",
      "  -2.03194171e-02  5.04297577e-02 -7.48653263e-02 -8.22822303e-02\n",
      "   7.65962750e-02  4.93392013e-02 -3.75553183e-02  1.44635001e-02\n",
      "  -5.72457612e-02 -1.79954562e-02  1.09697923e-01  1.19462743e-01\n",
      "   8.09255231e-04  6.17057867e-02  3.26322466e-02 -1.30780131e-01\n",
      "  -1.48636624e-01 -6.16233051e-02  4.33885828e-02  2.67129298e-02\n",
      "   1.39785958e-02 -3.94002423e-02 -2.52711661e-02  3.87744536e-03\n",
      "   3.58664691e-02 -6.15420528e-02  3.76660712e-02  2.67564524e-02\n",
      "  -3.82659324e-02 -3.54793333e-02 -2.39227526e-02  8.67977440e-02\n",
      "  -1.84062943e-02  7.71039277e-02  1.39863451e-03  7.00383261e-02\n",
      "  -4.77877669e-02 -7.89819956e-02  5.10814488e-02 -2.99868370e-33\n",
      "  -3.91646214e-02 -2.56207981e-03  1.65210702e-02  9.48940404e-03\n",
      "  -5.66219166e-02  6.57782704e-02 -4.77003157e-02  1.11661851e-02\n",
      "  -5.73558249e-02 -9.16257780e-03 -2.17520837e-02 -5.59531674e-02\n",
      "  -1.11422362e-02  9.32793692e-02  1.66764930e-02 -1.36723602e-02\n",
      "   4.34388295e-02  1.87245058e-03  7.29947072e-03  5.16331904e-02\n",
      "   4.80608270e-02  1.35341465e-01 -1.71739198e-02 -1.29698440e-02\n",
      "  -7.50109479e-02  2.61107404e-02  2.69802567e-02  7.83056254e-04\n",
      "  -4.87270132e-02  1.17842704e-02 -4.59580645e-02 -4.83213998e-02\n",
      "  -1.95671115e-02  1.93889067e-02  1.98807288e-02  1.67432595e-02\n",
      "   9.87801328e-02 -2.74088066e-02  2.34808959e-02  3.70228500e-03\n",
      "  -6.14515059e-02 -1.21228513e-03 -9.50473733e-03  9.25155543e-03\n",
      "   2.38443986e-02  8.61231759e-02  2.26790085e-02  5.45122894e-04\n",
      "   3.47130075e-02  6.25464413e-03 -6.92773936e-03  3.92400622e-02\n",
      "   1.15674874e-02  3.26279774e-02  6.22155480e-02  2.76114717e-02\n",
      "   1.86883938e-02  3.55805717e-02  4.11795788e-02  1.54782003e-02\n",
      "   4.22691330e-02  3.82248759e-02  1.00313481e-02 -2.83246078e-02\n",
      "   4.47052754e-02 -4.10458632e-02 -4.50550066e-03 -5.44734448e-02\n",
      "   2.62321252e-02  1.79862399e-02 -1.23118773e-01 -4.66952212e-02\n",
      "  -1.35913352e-02  6.46710172e-02  3.57346004e-03 -1.22233927e-02\n",
      "  -1.79382395e-02 -2.55502239e-02  2.37224586e-02  4.08665929e-03\n",
      "  -6.51475564e-02  4.43651304e-02  4.68595959e-02 -3.25174816e-02\n",
      "   4.02269000e-03 -3.97603400e-03  1.11939711e-02 -9.95597541e-02\n",
      "   3.33168469e-02  8.01060349e-02  9.42692310e-02 -6.38294220e-02\n",
      "   3.23151238e-02 -5.13553470e-02 -7.49871833e-03  5.30049183e-34\n",
      "  -4.13194373e-02  9.49646533e-02 -1.06401399e-01  4.96590436e-02\n",
      "  -3.41913253e-02 -3.16746123e-02 -1.71556156e-02  1.70100667e-03\n",
      "   5.79758286e-02 -1.21777912e-03 -1.68536138e-02 -5.16912788e-02\n",
      "   5.52998371e-02 -3.42647545e-02  3.08179576e-02 -3.10481042e-02\n",
      "   9.27532464e-02  3.72663662e-02 -2.37397738e-02  4.45893817e-02\n",
      "   1.46154044e-02  1.16239339e-01 -5.00112884e-02  3.88716385e-02\n",
      "   4.24748752e-03  2.56976541e-02  3.27243544e-02  4.29907553e-02\n",
      "  -1.36145046e-02  2.56121922e-02  1.06262183e-02 -8.46864507e-02\n",
      "  -9.52981934e-02  1.08399875e-01 -7.51600266e-02 -1.37773426e-02\n",
      "   6.37338161e-02 -4.49672807e-03 -3.25321443e-02  6.23613857e-02\n",
      "   3.48053128e-02 -3.54922451e-02 -2.00222172e-02  3.66607681e-02\n",
      "  -2.48837210e-02  1.01818396e-02 -7.01233074e-02 -4.31950651e-02\n",
      "   2.95332745e-02 -2.94974161e-04 -3.45386341e-02  1.46675929e-02\n",
      "  -9.83970314e-02 -4.70488034e-02 -8.85499455e-03 -8.89914334e-02\n",
      "   3.50995734e-02 -1.29601985e-01 -4.98865731e-02 -6.12047613e-02\n",
      "  -5.97797260e-02  9.46319848e-03  4.91217636e-02 -7.75027052e-02\n",
      "   8.09726790e-02 -4.79257628e-02  2.34378967e-03  7.57030770e-02\n",
      "  -2.40175724e-02 -1.52546624e-02  4.86738458e-02 -3.85968685e-02\n",
      "  -7.04831332e-02 -1.20348185e-02 -3.88790965e-02 -7.76016936e-02\n",
      "  -1.07243787e-02  1.04187708e-02 -2.13754103e-02 -9.17386711e-02\n",
      "  -1.11344634e-02 -2.96065994e-02  2.46458426e-02  4.65714000e-03\n",
      "  -1.63449533e-02 -3.95219661e-02  7.73373321e-02 -2.84732878e-02\n",
      "  -3.69940768e-03  8.27665329e-02 -1.10409278e-02  3.13983634e-02\n",
      "   5.35094403e-02  5.75145818e-02 -3.17622051e-02 -1.52911266e-08\n",
      "  -7.99661428e-02 -4.76796851e-02 -8.59788656e-02  5.69616705e-02\n",
      "  -4.08866666e-02  2.23832056e-02 -4.64446843e-03 -3.80131081e-02\n",
      "  -3.10670529e-02 -1.07278386e-02  1.97698455e-02  7.77002797e-03\n",
      "  -6.09471602e-03 -3.86376008e-02  2.80271769e-02  6.78138286e-02\n",
      "  -2.35350970e-02  3.21747512e-02  8.02538637e-03 -2.39106994e-02\n",
      "  -1.21996622e-03  3.14599276e-02 -5.24924025e-02 -8.06812383e-03\n",
      "   3.14769661e-03  5.11496887e-02 -4.44104634e-02  6.36013374e-02\n",
      "   3.85083891e-02  3.30432914e-02 -4.18728217e-03  4.95592840e-02\n",
      "  -5.69605082e-02 -6.49709720e-03 -2.49793138e-02 -1.60866622e-02\n",
      "   6.62289560e-02 -2.06310730e-02  1.08045764e-01  1.68546960e-02\n",
      "   1.43813202e-02 -1.32126845e-02 -1.29387438e-01  6.95216581e-02\n",
      "  -5.55772670e-02 -6.75413683e-02 -5.45816869e-03 -6.13592146e-03\n",
      "   3.90841067e-02 -6.28779754e-02  3.74063253e-02 -1.16571216e-02\n",
      "   1.29150720e-02 -5.52495345e-02  5.16075715e-02 -4.30839881e-03\n",
      "   5.80247380e-02  1.86944846e-02  2.27810554e-02  3.21666189e-02\n",
      "   5.37978895e-02  7.02849403e-02  7.49312416e-02 -8.41774866e-02]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "384\n",
      "<class 'list'>\n",
      "[-0.0032757080625742674, -0.011690792627632618, 0.041559260338544846, -0.038148075342178345, 0.024183077737689018]\n",
      "[-0.07509218156337738, -0.05722711607813835, 0.0042403400875627995, -0.06307529658079147, -0.026070691645145416]\n",
      "[-0.05335576459765434, -0.06176184117794037, -0.001220749574713409, 0.007031992543488741, 0.007789912633597851]\n"
     ]
    }
   ],
   "source": [
    "embeddings = embed_model.get_text_embedding(\"Hello World!\")\n",
    "embeddings2 = embed_model.get_text_embedding(\"Second line\")\n",
    "embeddings3 = embed_model.get_text_embedding(\"Third line\")\n",
    "print(len(embeddings))\n",
    "print(type(embeddings))\n",
    "print(embeddings[:5])\n",
    "print(embeddings2[:5])\n",
    "print(embeddings3[:5])\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    return vectors_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbertalan/.local/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "################################## LIBRARIES ################################## \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import DrainMethod\n",
    "import contextlib\n",
    "import hdbscan\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "############################## AUXILIARY METHODS ############################## \n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "def creates_lists(clusterer):\n",
    "    ## General Parameters\n",
    "\n",
    "    cluster_idxs = []\n",
    "    cluster_lines = []\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "    # Creates blank lists\n",
    "    for elem in range (clusterer.labels_.max()+1):\n",
    "        cluster_idxs.append([])\n",
    "        cluster_lines.append([])\n",
    "\n",
    "    # Populate the lists with cluster elements\n",
    "    for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "        if elem != -1:\n",
    "            cluster_idxs[elem].append(idx[0])\n",
    "            cluster_lines[elem].append(elem_df[idx[0]])\n",
    "        \n",
    "    return (cluster_idxs, cluster_lines)\n",
    "\n",
    "################################# MAIN METHODS ################################ \n",
    "\n",
    "# Parse logs using Drain\n",
    "\n",
    "def parse_logs(st=0.5, depth=5):\n",
    "    st = st # Drain similarity threshold\n",
    "    depth = depth # Max depth of the parsing tree\n",
    "\n",
    "    ## Code\n",
    "    parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "    parser.parse(log_file)\n",
    "\n",
    "    parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   \n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "# Creates distance matrix, using Euclidean distance\n",
    "def create_distance_matrix(vector_df):\n",
    "    # Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "    tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "    #Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(tfidf_distance)\n",
    "    max_val = np.max(tfidf_distance)\n",
    "    tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "    return (tfidf_distance)\n",
    "\n",
    "# Creates variable matrix, using Jaccard distance\n",
    "def create_variable_matrix():\n",
    "    ## General Parameters\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "    # Breaks the string into lists\n",
    "    for i, line in var_df.items():\n",
    "        var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "    # Transforms variable list to variable sparse matrix\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    var_df = mlb.fit_transform(var_df)\n",
    "    var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "    return (var_distance)\n",
    "\n",
    "def creates_closeness_matrix(tfidf_distance):\n",
    "    # Creates Count Matrix using line numbers from log lines as the counter\n",
    "    count_list = []\n",
    "    n = len(tfidf_distance)\n",
    "    count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "            count_list.append(i)\n",
    "\n",
    "    # Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "    count_array = np.array(count_list)\n",
    "    for x in count_array:\n",
    "        for y in count_array:\n",
    "            count_distance[x,y] = abs(x-y)\n",
    "    # Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(count_distance)\n",
    "    max_val = np.max(count_distance)\n",
    "    count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "    return (count_distance)\n",
    "\n",
    "def saves_matrices(distance_mat, variable_mat, closeness_mat):\n",
    "    np.save(\"tfidf_distance_\" + logName + \".csv\", distance_mat)\n",
    "    np.save(\"var_distance_\" + logName + \".csv\", variable_mat)\n",
    "    np.save(\"count_distance_\" + logName + \".csv\", closeness_mat)\n",
    "\n",
    "def loads_matrices():\n",
    "    tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "    count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "    var_distance = np.load(\"var_distance_\" + logName + \".csv\") \n",
    "    return (tfidf_distance, count_distance, var_distance)\n",
    "\n",
    "def joins_matrices(tfidf_distance, var_distance, count_distance, alpha, beta, gamma):\n",
    "\n",
    "    if alpha+beta+gamma > 1:\n",
    "        raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "    # New matrices, corrected by the weights\n",
    "    tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "    var_distance_wtd = np.dot(beta, var_distance)\n",
    "    count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "    # Sums remaining matrices\n",
    "    unified_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)\n",
    "    return (unified_matrix)\n",
    "\n",
    "def cluster_hdbscan(unified_matrix, cluster_size, mn_samples, cluster_selection_epsilon):\n",
    "    ## Clusters with HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size,min_samples=mn_samples,metric='precomputed',\n",
    "                                cluster_selection_epsilon=cluster_selection_epsilon, alpha=1.0, leaf_size=40, \n",
    "                                allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                                gen_min_span_tree=True)\n",
    "\n",
    "    clusterer.fit(unified_matrix)\n",
    "\n",
    "    ## Checks number of outliers\n",
    "    cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "    return (clusterer)\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        umap_model = UMAP(init='random')\n",
    "        cluster_model = KMedoids(n_clusters = 1)\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "        sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\", token=get_huggingface_token())\n",
    "        topic_model = BERTopic(embedding_model=sentence_model, hdbscan_model=cluster_model, \n",
    "                               vectorizer_model=vectorizer_model, umap_model=umap_model, \n",
    "                               top_n_words=10)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return (summary)\n",
    "\n",
    "def bertopic_previous_clustering(clusterer):\n",
    "    cluster_idxs, cluster_lines = creates_lists(clusterer)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    ## Creates list of boolean values, representing summarized topics\n",
    "    for idx in range(clusterer.labels_.max()):\n",
    "        cluster_topic.append(None)\n",
    "\n",
    "    for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "        ## For each cluster, maps topics, and defines them as the summary\n",
    "        if (cluster_topic[elem-1] == None):\n",
    "            summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "            cluster_topic[elem-1] = summary\n",
    "        \n",
    "        if elem == -1:\n",
    "            topic_summaries.append(\"\")\n",
    "        else:\n",
    "            topic_summaries.append(cluster_topic[elem-1])\n",
    "        \n",
    "        target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "        with open (target_file, \"w\") as f:\n",
    "            for line in topic_summaries:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "def create_new_bertopic_model():\n",
    "    lines = []\n",
    "    with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "        for line in line_file:\n",
    "            lines.append(line)\n",
    "\n",
    "    umap_model = UMAP(init='random')\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\", token=get_huggingface_token())\n",
    "    topic_model = BERTopic(embedding_model=sentence_model, vectorizer_model=vectorizer_model, \n",
    "                        umap_model=umap_model, top_n_words=10)\n",
    "    topics, probs = topic_model.fit_transform(lines)\n",
    "    return (topic_model)\n",
    "\n",
    "def bertopic_new_clustering():\n",
    "\n",
    "    topic_model = create_new_bertopic_model()\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    for elem in topic_model.topics_:\n",
    "        \n",
    "        line_topic = topic_model.get_topic(elem)\n",
    "        words = [i[0] for i in line_topic]\n",
    "        summary = ' '.join(words)\n",
    "        topic_summaries.append(summary)\n",
    "\n",
    "\n",
    "    target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (target_file, \"w\") as f:\n",
    "        for line in topic_summaries:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "def calculates_metrics():\n",
    "    \n",
    "    from rouge import Rouge \n",
    "    rouge = Rouge()\n",
    "\n",
    "    count_precision = 0\n",
    "    count_recall = 0\n",
    "    count_f1 = 0\n",
    "    total_lines = 2000\n",
    "\n",
    "    target_file = \"_bert_topics_tests.txt\"\n",
    "\n",
    "    # Opens external files with ground truth summaries and created topics\n",
    "    with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "        open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "        for line_summary, line_topic in zip(summaries, topics):\n",
    "            line_summary = line_summary[:-2]\n",
    "            line_summaries = line_summary.split(\";\")\n",
    "\n",
    "            for summary in line_summaries:\n",
    "                current_precision = 0\n",
    "                current_recall = 0\n",
    "                current_f1 = 0\n",
    "                metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']  \n",
    "\n",
    "                ## If the summary improves the f1 score, saves its metrics\n",
    "                if (current_f1 < metrics['f']):\n",
    "                    current_precision = metrics['p']\n",
    "                    current_recall = metrics['r']\n",
    "                    current_f1 = metrics['f']\n",
    "            \n",
    "            count_precision += current_precision\n",
    "            count_recall += current_recall        \n",
    "            count_f1 += current_f1\n",
    "\n",
    "    final_precision = count_precision/total_lines\n",
    "    final_recall = count_recall/total_lines\n",
    "    final_f1 = count_f1/total_lines\n",
    "\n",
    "    final = \"The precision is {}, the recall is {}, the f1 score is {}\".format(final_precision, final_recall, final_f1)\n",
    "    print (final)\n",
    "    return (final)\n",
    "\n",
    "def previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon):\n",
    "    parameters = (\"Testing previous clustering with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, min cluster size {}, min samples {} and cluster selection epsilon {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size,min_samples, cluster_selection_epsilon))\n",
    "    print(parameters)\n",
    "    parse_logs(drain_st, drain_depth)\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "    clustering = cluster_hdbscan(joint_matrix, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "    topic_summaries = bertopic_previous_clustering(clustering)\n",
    "    final = calculates_metrics()\n",
    "    return (final)\n",
    "\n",
    "def new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon):\n",
    "    parameters = (\"Testing new clustering with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, min cluster size {}, min samples {} and cluster selection epsilon {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size,min_samples, cluster_selection_epsilon))\n",
    "    print(parameters)\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "    clustering = cluster_hdbscan(joint_matrix, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "    topic_summaries = bertopic_new_clustering()\n",
    "    final = calculates_metrics()\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.06799999999999994, the recall is 0.16934999999999992, the f1 score is 0.09437618911321943\n",
      "Testing previous clustering with drain st 0.3, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.06216666666666657, the recall is 0.12985000000000005, the f1 score is 0.0812571418434352\n",
      "Testing previous clustering with drain st 0.1, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.0, the recall is 0.0, the f1 score is 0.0\n",
      "Testing previous clustering with drain st 0.5, drain depth 3, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.06799999999999994, the recall is 0.16934999999999992, the f1 score is 0.09437618911321943\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.06799999999999994, the recall is 0.16934999999999992, the f1 score is 0.09437618911321943\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 1, beta 0, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.07359404761904716, the recall is 0.18912499999999915, the f1 score is 0.10314706805417456\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0, beta 1, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.04886527777777765, the recall is 0.11933333333333342, the f1 score is 0.067342656315845\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.2, beta 0.1, gamma 0.7, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.028888888888888804, the recall is 0.06900000000000002, the f1 score is 0.04021977977181789\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.9, beta 0.1, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.07187976190476202, the recall is 0.17174999999999987, the f1 score is 0.09854180698372765\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.5, beta 0.5, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.06929801587301596, the recall is 0.16041666666666718, the f1 score is 0.0942057708525766\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.1, beta 0.9, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.09171170634920686, the recall is 0.20509047619047688, the f1 score is 0.1240935378071072\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 10, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.05778333333333337, the recall is 0.13893333333333352, the f1 score is 0.08029285607085875\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 10 and cluster selection epsilon 0.75\n",
      "The precision is 0.05778333333333337, the recall is 0.13893333333333352, the f1 score is 0.08029285607085875\n",
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.5\n",
      "The precision is 0.07998293650793627, the recall is 0.17157499999999992, the f1 score is 0.10608088982460125\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22396190476190647, the recall is 0.5504452380952409, the f1 score is 0.3108054871466108\n",
      "Testing new clustering with drain st 0.3, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22216190476190634, the recall is 0.5461357142857172, the f1 score is 0.3083181999435203\n",
      "Testing new clustering with drain st 0.1, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22051547619047757, the recall is 0.5416833333333361, the f1 score is 0.30610756156181956\n",
      "Testing new clustering with drain st 0.5, drain depth 3, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.2228424603174617, the recall is 0.54696904761905, the f1 score is 0.3093391289944493\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22120238095238295, the recall is 0.5455023809523839, the f1 score is 0.30745449701791505\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 1, beta 0, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.2182154761904787, the recall is 0.5376833333333371, the f1 score is 0.30314052860401147\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0, beta 1, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22222261904762053, the recall is 0.5469607142857167, the f1 score is 0.3085546302102324\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.2, beta 0.1, gamma 0.7, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22593492063492204, the recall is 0.549052380952384, the f1 score is 0.3132937243822387\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.9, beta 0.1, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.21925992063492206, the recall is 0.5348023809523834, the f1 score is 0.3039032119889503\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.5, beta 0.5, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22248948412698602, the recall is 0.5468023809523836, the f1 score is 0.3092053627707821\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.1, beta 0.9, gamma 0, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.2215047619047638, the recall is 0.5481595238095264, the f1 score is 0.3081232870005161\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 10, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.22161190476190645, the recall is 0.5453023809523831, the f1 score is 0.30760408070933737\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 10 and cluster selection epsilon 0.75\n",
      "The precision is 0.22281190476190643, the recall is 0.5498261904761932, the f1 score is 0.30955085941981947\n",
      "Testing new clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.5\n",
      "The precision is 0.22311190476190643, the recall is 0.5485023809523839, the f1 score is 0.30967863949163055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The precision is 0.22311190476190643, the recall is 0.5485023809523839, the f1 score is 0.30967863949163055'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)\n",
    "\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing previous clustering with drain st 0.5, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n",
      "The precision is 0.09753333333333203, the recall is 0.292, the f1 score is 0.1460346437193026\n",
      "Testing previous clustering with drain st 0.3, drain depth 5, alpha 0.7, beta 0.2, gamma 0.1, min cluster size 5, min samples 5 and cluster selection epsilon 0.75\n"
     ]
    }
   ],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)\n",
    "\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "384\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
