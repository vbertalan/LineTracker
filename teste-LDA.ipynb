{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jq: error (at <stdin>:11): null (null) has no keys\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## importa CSV da Ciena\n",
    "\n",
    "with open('ciena-mini.txt') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jq error at stdin11 null null has no keys\n",
      "\n",
      "jq error at stdin11 null null has no keys\n",
      "\n",
      "/localdisk/6500_repo/onid/setup/setenv: line 55: toolsetup: command not found\n",
      "\n",
      "localdisk6500repoonidsetupsetenv line 55 toolsetup command not found\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*boot*po': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidbootpo no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*bld*po': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidbldpo no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*BASE*po': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidbasepo no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*SWD': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidswd no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*load': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidload no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*raw': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidraw no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*tar': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidtar no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*sym': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidsym no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*sim': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidsim no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/onid/*bin': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoonidbin no such file or directory\n",
      "\n",
      "jq: error (at <stdin>:12): null (null) has no keys\n",
      "\n",
      "jq error at stdin12 null null has no keys\n",
      "\n",
      "/bin/bash: cleartool: command not found\n",
      "\n",
      "binbash cleartool command not found\n",
      "\n",
      "sed: can't read dw_stubsA.d: No such file or directory\n",
      "\n",
      "sed cant read dwstubsad no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/ome/vobs/equinox_ne_build/basebuild/EQBASE/ppc/TCSBASE69_P12/*boot*po': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoomevobsequinoxnebuildbasebuildeqbaseppctcsbase69p12bootpo no such file or directory\n",
      "\n",
      "cp: cannot stat '/localdisk/6500_repo/ome/vobs/equinox_ne_build/basebuild/EQBASE/ppc/TCSBASE69_P12/*bld*po': No such file or directory\n",
      "\n",
      "cp cannot stat localdisk6500repoomevobsequinoxnebuildbasebuildeqbaseppctcsbase69p12bldpo no such file or directory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## remove pontuação\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "cont = 0\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    print(lines[i])\n",
    "    lines[i] = lines[i].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    lines[i] = lines[i].lower()\n",
    "    \n",
    "    print(lines[i])\n",
    "    cont += 1\n",
    "    if cont > 16:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vbert\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jq', 'error', 'at', 'stdin', 'null', 'null', 'has', 'no', 'keys']\n"
     ]
    }
   ],
   "source": [
    "## constroi dados\n",
    "\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "#data = lines.tolist()\n",
    "data_words = list(sent_to_words(lines))\n",
    "# remove stop words\n",
    "#data_words = remove_stopwords(data_words)\n",
    "print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 2), (7, 1)]\n"
     ]
    }
   ],
   "source": [
    "## cria corpora\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.034*\"wl\" + 0.034*\"such\" + 0.033*\"no\" + 0.033*\"in\" + 0.033*\"head\" + '\n",
      "  '0.032*\"path\" + 0.032*\"fatal\" + 0.031*\"cards\" + 0.031*\"models\" + '\n",
      "  '0.030*\"sources\"'),\n",
      " (1,\n",
      "  '0.049*\"corebase\" + 0.046*\"vobs\" + 0.046*\"no\" + 0.045*\"localdisk\" + '\n",
      "  '0.044*\"unable\" + 0.044*\"untar\" + 0.044*\"attribute\" + 0.044*\"type\" + '\n",
      "  '0.043*\"unzip\" + 0.043*\"basebuild\"'),\n",
      " (2,\n",
      "  '0.047*\"artifact\" + 0.046*\"archive\" + 0.046*\"no\" + 0.045*\"type\" + '\n",
      "  '0.045*\"eqbase\" + 0.045*\"attribute\" + 0.045*\"instance\" + 0.045*\"untar\" + '\n",
      "  '0.045*\"to\" + 0.045*\"ome\"'),\n",
      " (3,\n",
      "  '0.048*\"no\" + 0.040*\"ome\" + 0.039*\"ppc\" + 0.039*\"vobs\" + 0.037*\"eqbase\" + '\n",
      "  '0.037*\"has\" + 0.037*\"basebuild\" + 0.037*\"localdisk\" + 0.036*\"unzip\" + '\n",
      "  '0.036*\"archive\"'),\n",
      " (4,\n",
      "  '0.064*\"no\" + 0.056*\"directory\" + 0.052*\"such\" + 0.046*\"or\" + 0.046*\"file\" + '\n",
      "  '0.034*\"cannot\" + 0.034*\"build\" + 0.031*\"tmp\" + 0.031*\"bc\" + 0.030*\"log\"'),\n",
      " (5,\n",
      "  '0.057*\"corebase\" + 0.047*\"no\" + 0.040*\"ome\" + 0.039*\"vobs\" + '\n",
      "  '0.036*\"localdisk\" + 0.036*\"basebuild\" + 0.036*\"to\" + 0.036*\"instance\" + '\n",
      "  '0.035*\"has\" + 0.034*\"untar\"'),\n",
      " (6,\n",
      "  '0.056*\"tcsbase\" + 0.046*\"no\" + 0.041*\"localdisk\" + 0.040*\"basebuild\" + '\n",
      "  '0.040*\"artifact\" + 0.039*\"has\" + 0.039*\"untar\" + 0.039*\"unzip\" + '\n",
      "  '0.038*\"vobs\" + 0.038*\"to\"'),\n",
      " (7,\n",
      "  '0.088*\"such\" + 0.088*\"directory\" + 0.088*\"no\" + 0.088*\"file\" + 0.088*\"or\" + '\n",
      "  '0.083*\"build\" + 0.077*\"tmp\" + 0.077*\"bc\" + 0.077*\"log\" + 0.059*\"cat\"'),\n",
      " (8,\n",
      "  '0.054*\"no\" + 0.040*\"such\" + 0.030*\"in\" + 0.028*\"path\" + 0.028*\"head\" + '\n",
      "  '0.028*\"fatal\" + 0.028*\"cards\" + 0.025*\"hi\" + 0.025*\"sources\" + '\n",
      "  '0.025*\"models\"'),\n",
      " (9,\n",
      "  '0.070*\"ome\" + 0.054*\"no\" + 0.046*\"vobs\" + 0.044*\"localdisk\" + 0.035*\"such\" '\n",
      "  '+ 0.033*\"file\" + 0.032*\"or\" + 0.031*\"directory\" + 0.031*\"cannot\" + '\n",
      "  '0.028*\"build\"')]\n"
     ]
    }
   ],
   "source": [
    "## roda LDA\n",
    "\n",
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 10\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyLDAvis.gensim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m## visualiza com pydavis\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgensim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m \n\u001b[0;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyLDAvis\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyLDAvis.gensim'"
     ]
    }
   ],
   "source": [
    "## visualiza com pydavis\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "\n",
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('./results/ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, './results/ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
