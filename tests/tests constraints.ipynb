{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: module: command not found\n",
      "Found existing installation: torch 2.0.1\n",
      "Uninstalling torch-2.0.1:\n",
      "  Successfully uninstalled torch-2.0.1\n",
      "Found existing installation: transformers 4.37.1\n",
      "Uninstalling transformers-4.37.1:\n",
      "  Successfully uninstalled transformers-4.37.1\n",
      "rm: cannot remove '/home/vbertalan/.local/lib/python3.9/site-packages/torch-2.0.1+computecanada.dist-info/': No such file or directory\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[33mWARNING: Requirement '/home/vbertalan/torch-2.0.1+computecanada-cp39-cp39-linux_x86_64.whl' looks like a filename, but the file does not exist\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[31mERROR: torch-2.0.1+computecanada-cp39-cp39-linux_x86_64.whl is not a supported wheel on this platform.\u001b[0m\u001b[31m\n",
      "\u001b[0mDefaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy==1.24.4 in /home/vbertalan/.local/lib/python3.10/site-packages (1.24.4)\n",
      "Requirement already satisfied: tqdm in /home/vbertalan/.local/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: scikit-learn in /home/vbertalan/.local/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Collecting torch==2.0.1\n",
      "  Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n",
      "Collecting transformers==4.37.1\n",
      "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
      "Requirement already satisfied: rich in /home/vbertalan/.local/lib/python3.10/site-packages (13.7.0)\n",
      "Requirement already satisfied: filelock in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (1.12)\n",
      "Requirement already satisfied: networkx in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from torch==2.0.1) (2.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/vbertalan/.local/lib/python3.10/site-packages (from transformers==4.37.1) (0.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from transformers==4.37.1) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers==4.37.1) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/vbertalan/.local/lib/python3.10/site-packages (from transformers==4.37.1) (2023.12.25)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers==4.37.1) (2.25.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/vbertalan/.local/lib/python3.10/site-packages (from transformers==4.37.1) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/vbertalan/.local/lib/python3.10/site-packages (from transformers==4.37.1) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (59.6.0)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1) (0.37.1)\n",
      "Requirement already satisfied: cmake in /home/vbertalan/.local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (3.28.3)\n",
      "Requirement already satisfied: lit in /home/vbertalan/.local/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1) (17.0.6)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from rich) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from rich) (2.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/vbertalan/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.37.1) (2024.2.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/vbertalan/.local/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2->torch==2.0.1) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/vbertalan/.local/lib/python3.10/site-packages (from sympy->torch==2.0.1) (1.3.0)\n",
      "Using cached torch-2.0.1-cp310-cp310-manylinux1_x86_64.whl (619.9 MB)\n",
      "Using cached transformers-4.37.1-py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: transformers, torch\n",
      "Successfully installed torch-2.0.1 transformers-4.37.1\n"
     ]
    }
   ],
   "source": [
    "!module --ignore_cache load python/3.9.6\n",
    "!pip uninstall torch -y\n",
    "!pip uninstall transformers -y\n",
    "!rm -r ~/.local/lib/python3.9/site-packages/torch-2.0.1+computecanada.dist-info/\n",
    "!pip install ~/torch-2.0.1+computecanada-cp39-cp39-linux_x86_64.whl\n",
    "!pip install numpy==1.24.4 tqdm scikit-learn torch==2.0.1 transformers==4.37.1 rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbertalan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import linetracker.embeddings.llm as llm_embedding\n",
    "import linetracker.parser.variables_matrix as ev\n",
    "import linetracker.clustering.kmedoid as clustK\n",
    "import linetracker.embeddings.distances as d\n",
    "import linetracker.line_distance as ld\n",
    "import linetracker.parser.parser as p\n",
    "import linetracker.main as m\n",
    "\n",
    "from rich.console import Console\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get example log files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "splits = m2.get_split_build_logs(\"../data/splitted_event_ids.json\")# type: ignore\n",
    "print(\"Sizes available: \",np.unique([len(s) for s in splits.values()]))\n",
    "```\n",
    "```\n",
    "Sizes available:  [  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
    "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
    "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
    "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  75\n",
    "  77 127 129 130 131 132 145 147 148]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# get log file of size specified\n",
    "sizes = [2, 3, 10, 148]\n",
    "splits_samples = {}\n",
    "with h5py.File(\"../data/trat3_production_1650_1700_20231411_v1.hdf5\") as fp:\n",
    "    splits_samples = {}\n",
    "    for k,split in splits.items():\n",
    "        if len(split) not in sizes or len(split) in splits_samples:\n",
    "            continue\n",
    "        L = []\n",
    "        for e in tqdm.tqdm(split):\n",
    "            L.append({**fp[e].attrs})\n",
    "        splits_samples[len(split)] = [k,L]\n",
    "```\n",
    "```\n",
    "  0%|          | 0/10 [00:00<?, ?it/s]\n",
    "100%|██████████| 10/10 [00:00<00:00, 31.80it/s]\n",
    "100%|██████████| 2/2 [00:00<00:00, 38.69it/s]\n",
    "100%|██████████| 3/3 [00:00<00:00, 38.14it/s]\n",
    "100%|██████████| 148/148 [00:03<00:00, 45.37it/s]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "class CustomEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        print(type(obj),obj)\n",
    "        if isinstance(obj, np.int32):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return self.default(obj.tolist())\n",
    "        if isinstance(obj, list):\n",
    "            return [self.default(e) for e in obj.tolist()]\n",
    "        if isinstance(obj, dict):\n",
    "            return {k:self.default(v) for k,v in obj.items()}\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "with open(\"./data.json\", \"w\") as fp:\n",
    "    json.dump(splits_samples, fp, cls=CustomEncoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Execute the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['37734']\n"
     ]
    }
   ],
   "source": [
    "with open(\"CienaFiles/pots-emotr.json\") as fp:\n",
    "    splits_samples = json.load(fp)\n",
    "splits_samples = {k:v for k,v in sorted(splits_samples.items(),key=lambda x:int(x[0]))}\n",
    "print(list(splits_samples.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the doc\n",
    "\n",
    "```python\n",
    "def execute_full_pipeline(\n",
    "    logs: List[LogData],\n",
    "    triplet_coefficient: TripletCoef,\n",
    "    parser: Callable[[List[LogData]], List[p.ParsedLine]],\n",
    "    embedder: Callable[[List[str]], Generator[np.ndarray, None, None]],\n",
    "    embedding_distance_fn: Callable[[List[np.ndarray]], np.ndarray],\n",
    "    line_distance_fn: Callable[[List[LogData]], np.ndarray],\n",
    "    clustering_fn: Callable[[np.ndarray], c.ClusteringAlgorithmOutput],\n",
    "    float_precision: type = np.float32,\n",
    ") -> c.ClusteringAlgorithmOutput:\n",
    "    \"\"\"Cluster logs provided in argument into groups of related log lines\n",
    "    # Arguments\n",
    "    - logs: List[LogData], the log lines\n",
    "    - triplet_coefficient: TripletCoef, the three coefficients to use to ponderate the matrices\n",
    "    - parser: Callable[[List[LogData]], List[p.ParsedLine]], a function that from the list of logs lines can generate for each line\n",
    "    - embedder: Callable[[List[str]], Generator[np.ndarray, None, None]], the function that can generate embeddings from logs\n",
    "    - embedding_distance_fn: Callable[[List[np.ndarray]], np.ndarray], given all embeddings of each log lines of the same log file, generate the normalized (between 0 and 1) distances between all embeddings\n",
    "    - line_distance_fn: Callable[[List[str]],np.ndarray], a function that can generate a matrix with the distance between each log line\n",
    "    - clustering_fn:  Callable[[np.ndarray], c.ClusteringAlgorithmOutput], taking the combined matrix with the coefficients provided, clusters the logs\n",
    "    - float_precision: type = np.float32, the precision to use for all floating point matrices\n",
    "    \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/vbertalan/.cache/huggingface/token\n",
      "Login successful\n",
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/vbertalan/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at BAAI/bge-large-en-v1.5 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# build the functions for the pipeline\n",
    "# parsing\n",
    "parser = lambda logs:p.get_parsing_drainparser([e['text'] for e in logs],reg_expressions=[r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\"],depth=3,similarity_threshold=0.1,max_children=5)\n",
    "#parser = lambda logs:p.get_parsing_drainparser([e['text'] for e in logs],reg_expressions=[],depth=3,similarity_threshold=0.1,max_children=5)\n",
    "# creating embeddings\n",
    "models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "model_name = models_names[2]\n",
    "init_embedder = llm_embedding.generate_embeddings_llm(model_name=model_name,token=\"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\", use_cpu=True)\n",
    "## using pooling by mean\n",
    "pooling_fn = llm_embedding.get_pooling_function(\"mean\")\n",
    "#pooling_fn = lambda embedding:embedding\n",
    "embedder = lambda logs: init_embedder(logs, pooling_fn,limit_tokens=100,precision=np.float16)# type: ignore\n",
    "embedding_distance_fn = d.normalized_cosine_distance\n",
    "line_distance_fn = ld.get_absolute_line_distance_matrix\n",
    "# clustering model\n",
    "clustering_fn = lambda combined_matrix: clustK.get_clustering_kmedoid(combined_matrix)[0]['clustering']\n",
    "float_precision = np.float16\n",
    "triplet_coefficient = m.TripletCoef(coef_variables_matrix=0.0, coef_embeddings_matrix=1, coef_count_matrix=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will execute each step of the pipeline:\n",
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "    # 3. build the embeddings\n",
    "    embeddings: List[np.ndarray] = [embedding for embedding in embedder(logs_texts)]\n",
    "    # 4. build the distance matrix\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    del embeddings\n",
    "    # 5. build the count matrix\n",
    "    count_matrix = line_distance_fn(logs).astype(float_precision)\n",
    "    # 6. merge the matrices with triplet coefficient\n",
    "    combined_matrix = combine_matrices(\n",
    "        TripletMatrix(\n",
    "            variables_matrix=variables_distance_matrix,\n",
    "            embeddings_matrix=embeddings_distance_matrix,\n",
    "            count_matrix=count_matrix,\n",
    "        ),\n",
    "        triplet_coef=triplet_coefficient,\n",
    "    ).astype(float_precision)\n",
    "    # note: values will be between 0 and 3 (addition of 3 matrices normalized between 0 and 3)\n",
    "    del variables_distance_matrix\n",
    "    del embeddings_distance_matrix\n",
    "    # 7. run the clustering algorithm with the constraints\n",
    "    clustering_output = clustering_fn(combined_matrix)\n",
    "    # 8. return the result\n",
    "    return clustering_output\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step:\n",
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------pots-emotr---------------------------------------------\n",
      "['2023-10-19T15:21:07EDT', '/localdisk/6500_repo/tools/copera/bin/runjob.py.main(166)', 'Run', 'Job', 'Type:', '_6500_build.pots Name: pots-emotr on on-uno-potsbuildnvme002.6500.ciena.cloud'] :  2023-10-19T15:21:07EDT - INFO - /localdisk/6500_repo/tools/copera/bin/runjob.py.main(166) : Run Job Type: _6500_build.pots Name: pots-emotr on on-uno-potsbuildnvme002.6500.ciena.cloud\n",
      "\n",
      "[] :  {'test': None, 'help': None}\n",
      "\n",
      "['2023-10-19T15:21:07EDT', '/localdisk/6500_repo/tools/copera/bin/runjob.py.main(169)', 'Job', 'stdout/stderr', 'logs see: /nfs/appdata/6500_build/rel/6500_main/transport/240698/logs/_6500_build.pots/pots-emotr'] :  2023-10-19T15:21:07EDT - INFO - /localdisk/6500_repo/tools/copera/bin/runjob.py.main(169) : Job stdout/stderr logs see: /nfs/appdata/6500_build/rel/6500_main/transport/240698/logs/_6500_build.pots/pots-emotr\n",
      "\n",
      "[] :  2023-10-19T15:21:07EDT - INFO - /localdisk/6500_repo/tools/copera/bin/runjob.py.main(178) : Copera ID: copera_api|240698|host=https://6500.ciena.com/build/copera/a\n",
      "\n",
      "[] :  RELEASE_NUMBER=auto_derive\n",
      "\n",
      "[] :  JENKINS_NODE_COOKIE=0b6f50e5-0988-4e7b-a719-4f2da70d6874\n",
      "\n",
      "[] :  SUBMITTED_IN_VERSION=6500_16.5\n",
      "\n",
      "[] :  XDG_SESSION_ID=30\n",
      "\n",
      "[] :  BUILD_URL=https://6500.ciena.com/build/jenkins-uno/job/ciena-6500/job/builder/206483/\n",
      "\n",
      "[] :  SHELL=/bin/bash\n",
      "\n",
      "[] :  HUDSON_SERVER_COOKIE=d9fc683325a9cc19\n",
      "\n",
      "[] :  STAGE_NAME=load_build\n",
      "\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "dict_parsed_variables = {}\n",
    "# for each of our log file with different number of lines, we apply step 1 and save the result\n",
    "for size,[build_log_name, logs] in splits_samples.items():\n",
    "    print(f\"{build_log_name:-^100}\")\n",
    "    parsed_logs = parser(logs)\n",
    "    # Apply step 1\n",
    "    logs_texts = [e['text'] for e in logs]\n",
    "    parsed_variables = [e['variables'] for e in parsed_logs]\n",
    "    # Show the result (truncated)\n",
    "    for i,(text, variables) in enumerate(zip(logs_texts, parsed_variables)):\n",
    "        print(variables,\": \",text)\n",
    "        if i > 10:\n",
    "            print(\"...\")\n",
    "            break\n",
    "    # And save the result\n",
    "    dict_parsed_variables[size] = parsed_variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then step2: \n",
    "```python\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_variables_distance_matrix = {}\n",
    "for size,parsed_variables in dict_parsed_variables.items():\n",
    "    variables_distance_matrix = ev.get_variable_matrix(parsed_events=parsed_variables)\n",
    "    dict_variables_distance_matrix[size] = variables_distance_matrix\n",
    "    # print(f\"{size:-^100}\")\n",
    "    # for i,v in enumerate(parsed_variables):\n",
    "    #     print(i,\":\",v)\n",
    "    #     if i > 10:\n",
    "    #         print(\"...\")\n",
    "    #         break\n",
    "    # print(dict_variables_distance_matrix[size])\n",
    "    # print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then step 3\n",
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "    # 3. build the embeddings\n",
    "    embeddings: np.ndarray = np.array(\n",
    "        [embedding for embedding in embedder(logs_texts)]\n",
    "    ).astype(float_precision)\n",
    "    # 4. build the distance matrix\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    del embeddings\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [00:08,  7.22it/s]"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "dict_embeddings_distance_matrix = {}\n",
    "for size,[build_log_name, logs] in splits_samples.items():\n",
    "    # print(f\"{size:-^100}\")\n",
    "    logs_texts = [e['text'] for e in logs]\n",
    "    start = time.perf_counter()\n",
    "    embeddings = np.array(\n",
    "        [embedding for embedding in tqdm(embedder(logs_texts))]\n",
    "    ).astype(float_precision)\n",
    "    diff = time.perf_counter()-start\n",
    "    # print(f\"{embeddings.shape=}, obtained in {diff} second ({diff/embeddings.shape[0]} s/embedding)\")\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    # print(\"*\"*100)\n",
    "    # print(embeddings_distance_matrix)\n",
    "    dict_embeddings_distance_matrix[size] = embeddings_distance_matrix\n",
    "    del embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "    # 3. build the embeddings\n",
    "    embeddings: List[np.ndarray] = [embedding for embedding in embedder(logs_texts)]\n",
    "    # 4. build the distance matrix\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    del embeddings\n",
    "    # 5. build the count matrix\n",
    "    count_matrix = line_distance_fn(logs).astype(float_precision)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------10-------------------------------------------------\n",
      "[[0.     0.1111 0.2222 0.3333 0.4443 0.5557 0.6665 0.778  0.8887 1.    ]\n",
      " [0.1111 0.     0.1111 0.2222 0.3333 0.4443 0.5557 0.6665 0.778  0.8887]\n",
      " [0.2222 0.1111 0.     0.1111 0.2222 0.3333 0.4443 0.5557 0.6665 0.778 ]\n",
      " [0.3333 0.2222 0.1111 0.     0.1111 0.2222 0.3333 0.4443 0.5557 0.6665]\n",
      " [0.4443 0.3333 0.2222 0.1111 0.     0.1111 0.2222 0.3333 0.4443 0.5557]\n",
      " [0.5557 0.4443 0.3333 0.2222 0.1111 0.     0.1111 0.2222 0.3333 0.4443]\n",
      " [0.6665 0.5557 0.4443 0.3333 0.2222 0.1111 0.     0.1111 0.2222 0.3333]\n",
      " [0.778  0.6665 0.5557 0.4443 0.3333 0.2222 0.1111 0.     0.1111 0.2222]\n",
      " [0.8887 0.778  0.6665 0.5557 0.4443 0.3333 0.2222 0.1111 0.     0.1111]\n",
      " [1.     0.8887 0.778  0.6665 0.5557 0.4443 0.3333 0.2222 0.1111 0.    ]]\n",
      "CPU times: user 2.4 ms, sys: 0 ns, total: 2.4 ms\n",
      "Wall time: 2.33 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splits_samples = {k:splits_samples[k] for k in sorted(splits_samples)}#type: ignore\n",
    "dict_count_matrix = {}\n",
    "for size,[build_log_name, logs] in splits_samples.items():\n",
    "    print(f\"{size:-^100}\")\n",
    "    count_matrix = line_distance_fn(logs).astype(float_precision)\n",
    "    dict_count_matrix[size] = count_matrix\n",
    "    print(count_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "    # 3. build the embeddings\n",
    "    embeddings: np.ndarray = np.array(\n",
    "        [embedding for embedding in embedder(logs_texts)]\n",
    "    ).astype(float_precision)\n",
    "    # 4. build the distance matrix\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    del embeddings\n",
    "    # 5. build the count matrix\n",
    "    count_matrix = line_distance_fn(logs).astype(float_precision)\n",
    "    # 6. merge the matrices with triplet coefficient\n",
    "    combined_matrix = combine_matrices(\n",
    "        TripletMatrix(\n",
    "            variables_matrix=variables_distance_matrix,\n",
    "            embeddings_matrix=embeddings_distance_matrix,\n",
    "            count_matrix=count_matrix,\n",
    "        ),\n",
    "        triplet_coef=triplet_coefficient,\n",
    "    ).astype(float_precision)\n",
    "    # note: values will be between 0 and 3 (addition of 3 matrices normalized between 0 and 3)\n",
    "    del variables_distance_matrix\n",
    "    del embeddings_distance_matrix\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "dico_combined_matrix = {}\n",
    "for size in dict_count_matrix:\n",
    "    print(size)\n",
    "    for i,mat in enumerate([dict_variables_distance_matrix[size],dict_embeddings_distance_matrix[size],dict_count_matrix[size]]):\n",
    "        assert np.unique(np.diag(mat)).tolist() == [0], f\"Error for matrix {i}\\n{mat}\"\n",
    "    dico_combined_matrix[size] = m.combine_matrices(\n",
    "        m.TripletMatrix(\n",
    "            variables_matrix=dict_variables_distance_matrix[size],\n",
    "            embeddings_matrix=dict_embeddings_distance_matrix[size],\n",
    "            count_matrix=dict_count_matrix[size],\n",
    "        ),\n",
    "        triplet_coef=triplet_coefficient,\n",
    "    ).astype(float_precision)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To finally have\n",
    "```python\n",
    "    # 1. parse the logs\n",
    "    parsed_logs: List[p.ParsedLine] = parser(logs)\n",
    "    logs_texts = [e[\"text\"] for e in logs]\n",
    "    parsed_variables = [e[\"variables\"] for e in parsed_logs]\n",
    "    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)\n",
    "    variables_distance_matrix = e.get_variable_matrix(parsed_variables).astype(float_precision)\n",
    "    # 3. build the embeddings\n",
    "    embeddings: np.ndarray = np.array(\n",
    "        [embedding for embedding in embedder(logs_texts)]\n",
    "    ).astype(float_precision)\n",
    "    # 4. build the distance matrix\n",
    "    embeddings_distance_matrix = embedding_distance_fn(embeddings).astype(\n",
    "        float_precision\n",
    "    )\n",
    "    del embeddings\n",
    "    # 5. build the count matrix\n",
    "    count_matrix = line_distance_fn(logs).astype(float_precision)\n",
    "    # 6. merge the matrices with triplet coefficient\n",
    "    combined_matrix = combine_matrices(\n",
    "        TripletMatrix(\n",
    "            variables_matrix=variables_distance_matrix,\n",
    "            embeddings_matrix=embeddings_distance_matrix,\n",
    "            count_matrix=count_matrix,\n",
    "        ),\n",
    "        triplet_coef=triplet_coefficient,\n",
    "    ).astype(float_precision)\n",
    "    # note: values will be between 0 and 3 (addition of 3 matrices normalized between 0 and 3)\n",
    "    del variables_distance_matrix\n",
    "    del embeddings_distance_matrix\n",
    "    # 7. run the clustering algorithm with the constraints\n",
    "    clustering_output = clustering_fn(combined_matrix)\n",
    "    # 8. return the result\n",
    "    return clustering_output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "dico_clustering_output = {}\n",
    "for size,matrix in dico_combined_matrix.items():\n",
    "    print(np.diag(matrix))\n",
    "    dico_clustering_output[size] = clustering_fn(matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we show the text with each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorsys\n",
    "\n",
    "def generate_hsv_palette(num_colors, saturation=1.0, value=1.0):\n",
    "    colors = []\n",
    "    hue_step = 1.0 / num_colors\n",
    "\n",
    "    for i in range(num_colors):\n",
    "        hue = i * hue_step\n",
    "        rgb = colorsys.hsv_to_rgb(hue, saturation, value)\n",
    "        rgb = tuple(int(x * 255) for x in rgb)\n",
    "        colors.append(rgb)\n",
    "\n",
    "    return colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">-------------------------------------------------10-------------------------------------------------</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[37m-------------------------------------------------10-------------------------------------------------\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">000-2.0: 2023-10-27 07:05:36 sed: can't read *.h.temp: No such file or directory</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m000-2.0: 2023-10-27 07:05:36 sed: can't read *.h.temp: No such file or directory\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">001-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m001-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">002-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m002-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">003-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m003-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">004-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m004-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">005-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m005-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">006-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m006-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">007-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m007-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">008-2.0: 2023-10-27 07:05:36 mv: cannot stat '*.h': No such file or directory</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m008-2.0: 2023-10-27 07:05:36 mv: cannot stat '*.h': No such file or directory\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">009-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m009-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console = Console(color_system=\"auto\", highlight=False, force_jupyter=True)\n",
    "dico_clustering_output = {s:dico_clustering_output[s] for s in sorted(dico_clustering_output,key=lambda x:int(x))}\n",
    "print(list(dico_clustering_output.keys()))\n",
    "for size, _ in dico_clustering_output.items():\n",
    "    console.print(f\"{size:-^100}\", style=f\"white\", end=\"\" )\n",
    "    clustering_output = list(dico_clustering_output[size].values())\n",
    "    unique_clusters =  np.unique(clustering_output)\n",
    "    mapping = {clust:col for clust,col in zip(unique_clusters,generate_hsv_palette(len(unique_clusters),saturation=0.75))}\n",
    "    for line_id, (log,cluster) in enumerate(zip(splits_samples[size][1], clustering_output)):\n",
    "        text = f\"{line_id:03d}-{cluster}: {log['text']}\"\n",
    "        r,g,b = mapping[cluster]\n",
    "        console.print(text, style=f\"rgb({r},{g},{b})\", end=\"\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to apply a must-link and a cannot-link constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# settings contraints\n",
    "clustering_fn = lambda combined_matrix: clustK.get_clustering_kmedoid(combined_matrix, must_link=[[3,4]], cannot_link=[[8,9]])[0]['clustering']\n",
    "\n",
    "# running clustering method\n",
    "dico_clustering_output = {}\n",
    "for size,matrix in dico_combined_matrix.items():\n",
    "    print(np.diag(matrix))\n",
    "    dico_clustering_output[size] = clustering_fn(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0\">-------------------------------------------------10-------------------------------------------------</span></pre>\n"
      ],
      "text/plain": [
       "\u001b[37m-------------------------------------------------10-------------------------------------------------\u001b[0m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">000-2.0: 2023-10-27 07:05:36 sed: can't read *.h.temp: No such file or directory</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m000-2.0: 2023-10-27 07:05:36 sed: can't read *.h.temp: No such file or directory\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">001-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m001-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">002-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m002-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">003-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m003-2.0: 2023-10-27 07:05:36 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">004-2.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m004-2.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">005-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m005-2.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">006-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m006-1.0: 2023-10-27 07:05:14 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">007-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m007-1.0: 2023-10-27 07:06:21 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #3fffff; text-decoration-color: #3fffff\">008-2.0: 2023-10-27 07:05:36 mv: cannot stat '*.h': No such file or directory</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;63;255;255m008-2.0: 2023-10-27 07:05:36 mv: cannot stat '*.h': No such file or directory\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff3f3f; text-decoration-color: #ff3f3f\">009-1.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;63;63m009-1.0: 2023-10-27 07:05:35 tput: No value for $TERM and no -T specified\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "console = Console(color_system=\"auto\", highlight=False, force_jupyter=True)\n",
    "dico_clustering_output = {s:dico_clustering_output[s] for s in sorted(dico_clustering_output,key=lambda x:int(x))}\n",
    "print(list(dico_clustering_output.keys()))\n",
    "for size, _ in dico_clustering_output.items():\n",
    "    console.print(f\"{size:-^100}\", style=f\"white\", end=\"\" )\n",
    "    clustering_output = list(dico_clustering_output[size].values())\n",
    "    unique_clusters =  np.unique(clustering_output)\n",
    "    mapping = {clust:col for clust,col in zip(unique_clusters,generate_hsv_palette(len(unique_clusters),saturation=0.75))}\n",
    "    for line_id, (log,cluster) in enumerate(zip(splits_samples[size][1], clustering_output)):\n",
    "        text = f\"{line_id:03d}-{cluster}: {log['text']}\"\n",
    "        r,g,b = mapping[cluster]\n",
    "        console.print(text, style=f\"rgb({r},{g},{b})\", end=\"\" )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
