{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "import linetracker.main as m\n",
    "import linetracker.parser.parser as p\n",
    "import linetracker.embeddings.llm as llm_embedding\n",
    "import linetracker.embeddings.distances as d\n",
    "import linetracker.line_distance as ld\n",
    "import linetracker.parser.variables_matrix as ev\n",
    "import linetracker.clustering.kmedoid as clustK\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import time\n",
    "import tqdm\n",
    "from rich.console import Console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2', '3', '10', '148']\n"
     ]
    }
   ],
   "source": [
    "with open(\"data.json\") as fp:\n",
    "    splits_samples = json.load(fp)\n",
    "splits_samples = {k:v for k,v in sorted(splits_samples.items(),key=lambda x:int(x[0]))}\n",
    "print(list(splits_samples.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/robin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/robin/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "If you want to use `BertLMHeadModel` as a standalone, add `is_decoder=True.`\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at BAAI/bge-large-zh-v1.5 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# build the functions for the pipeline\n",
    "parser = lambda logs:p.get_parsing_drainparser([e['text'] for e in logs],depth=3,similarity_threshold=0.1,max_children=5)\n",
    "models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-zh-v1.5\"]\n",
    "model_name = models_names[2]\n",
    "init_embedder = llm_embedding.generate_embeddings_llm(model_name=model_name,token=\"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\", use_cpu=True)\n",
    "pooling_fn = llm_embedding.get_pooling_function()\n",
    "embedder = lambda logs: init_embedder(logs, pooling_fn,limit_tokens=100,precision=np.float16)# type: ignore\n",
    "embedding_distance_fn = d.normalized_cosine_distance\n",
    "line_distance_fn = ld.get_absolute_line_distance_matrix\n",
    "clustering_fn = lambda combined_matrix: clustK.get_clustering_kmedoid(combined_matrix)\n",
    "float_precision = np.float16\n",
    "triplet_coefficient = m.TripletCoef(coef_variables_matrix=0.4, coef_embeddings_matrix=0.6, coef_count_matrix=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------2--------------------------------------------------\n",
      "duration 0.85s (0.42s/log line)\n",
      "-------------------------------------------------3--------------------------------------------------\n",
      "duration 1.30s (0.43s/log line)\n",
      "-------------------------------------------------10-------------------------------------------------\n",
      "duration 4.50s (0.45s/log line)\n",
      "------------------------------------------------148-------------------------------------------------\n",
      "duration 75.97s (0.51s/log line)\n"
     ]
    }
   ],
   "source": [
    "for size, [log_name, logs] in splits_samples.items():\n",
    "    print(f\"{size:-^100}\")\n",
    "    start = time.perf_counter()\n",
    "    m.execute_full_pipeline(\n",
    "        logs,\n",
    "        triplet_coefficient,\n",
    "        parser,\n",
    "        embedder,\n",
    "        embedding_distance_fn,\n",
    "        line_distance_fn,\n",
    "        clustering_fn,\n",
    "        float_precision,\n",
    "    )\n",
    "    end = time.perf_counter()\n",
    "    print(f\"duration {end-start:.2f}s ({(end-start)/len(logs):.2f}s/log line)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['dup_id', 'event_id', 'group_id', 'line_num', 'log_name', 'planid',\n",
      "       'raw', 'template', 'text', 'variables', 'name', 'build_log'],\n",
      "      dtype='object')\n",
      "              count\n",
      "count  36599.000000\n",
      "mean      18.587065\n",
      "std       19.757756\n",
      "min        1.000000\n",
      "25%       10.000000\n",
      "50%       15.000000\n",
      "75%       21.000000\n",
      "max      148.000000\n",
      "n_lines=680268.0\n",
      "4 days, 0:22:16.680000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_json(\"../data/stats_dataset.json\")\n",
    "counts_by_log_plan = df.groupby(\"name\").size().reset_index(name='count')\n",
    "print(df.columns)\n",
    "print(counts_by_log_plan.describe())\n",
    "n_lines = float(counts_by_log_plan['count'].sum())\n",
    "print(f\"{n_lines=}\")\n",
    "from datetime import timedelta\n",
    "print(timedelta(seconds=n_lines*0.51))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22:11:50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>243488, COREBASE_CTM_ppc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>244856, COREBASE_CTM_ppc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>242656, COREBASE69_SP3AUX_ppc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241129, COREBASE69_SP2_ppc</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>226431, wr-container-wr-hal-dnx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>245445, ddf_vx_simbc69</td>\n",
       "      <td>145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>240760, ddf_vx_simbc69</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>240761, ddf_vx_simbc69</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>240698, ddf_vx_simbc69</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>240697, ddf_vx_simbc69</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>375 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                name  count\n",
       "0           243488, COREBASE_CTM_ppc      1\n",
       "1           244856, COREBASE_CTM_ppc      1\n",
       "2      242656, COREBASE69_SP3AUX_ppc      1\n",
       "3         241129, COREBASE69_SP2_ppc      1\n",
       "4    226431, wr-container-wr-hal-dnx      1\n",
       "..                               ...    ...\n",
       "370           245445, ddf_vx_simbc69    145\n",
       "371           240760, ddf_vx_simbc69    147\n",
       "372           240761, ddf_vx_simbc69    147\n",
       "373           240698, ddf_vx_simbc69    148\n",
       "374           240697, ddf_vx_simbc69    148\n",
       "\n",
       "[375 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 5\n",
    "# sampled_df = counts_by_log_plan.groupby('count', group_keys=False).apply(lambda group: group.sample(min(n, len(group))))\n",
    "# sampled_df.to_json(\"./sampled_samples.json\",orient=\"records\")\n",
    "sampled_df = pd.read_json(\"./sampled_samples.json\")\n",
    "n_elements = sampled_df['count'].sum()\n",
    "print(timedelta(seconds=n_elements*0.5*10))\n",
    "sampled_df.sort_values(by=['count'],inplace=True)\n",
    "sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1\n",
      "0 0.25 0.75\n",
      "0 0.5 0.5\n",
      "0 0.75 0.25\n",
      "0 1 0\n",
      "0.25 0 0.75\n",
      "0.25 0.25 0.5\n",
      "0.25 0.5 0.25\n",
      "0.25 0.75 0\n",
      "0.3333333333333333 0.3333333333333333 0.3333333333333333\n",
      "0.5 0 0.5\n",
      "0.5 0.25 0.25\n",
      "0.5 0.5 0\n",
      "0.75 0 0.25\n",
      "0.75 0.25 0\n",
      "1 0 0\n"
     ]
    }
   ],
   "source": [
    "import itertools as it\n",
    "l_triplet = []\n",
    "for (a,b,c) in it.product([0,0.25,1/3.,0.5,0.75,1],repeat=3):\n",
    "    if abs(a+b+c-1) < 1e-2:\n",
    "        print(a,b,c)\n",
    "        l_triplet.append(m.TripletCoef(coef_variables_matrix=a, coef_embeddings_matrix=b, coef_count_matrix=c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1404/6000 [1:39:50<12:26:51,  9.75s/it]"
     ]
    }
   ],
   "source": [
    "class Encoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.int32) or isinstance(obj, np.int64):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        if isinstance(obj, list):\n",
    "            return [self.default(e) for e in obj.tolist()]\n",
    "        if isinstance(obj, dict):\n",
    "            return {k:self.default(v) for k,v in obj.items()}\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "\n",
    "L = []\n",
    "with open(\"./results.json\") as fp:\n",
    "    L = json.load(fp)\n",
    "df.loc[:,'line_number'] = df['line_num']\n",
    "\n",
    "i = 0\n",
    "with tqdm.tqdm(total=len(sampled_df['name'].unique())*len(l_triplet)) as pbar:\n",
    "    for build_log in sampled_df['name'].unique():\n",
    "        df_build_log = df.query(f\"build_log == '{build_log}'\")\n",
    "        logs = [d for d in df_build_log[['text','event_id', \"line_number\"]].to_dict(orient=\"records\")]\n",
    "        for triplet_coefficient in l_triplet:\n",
    "            if i == len(L):\n",
    "                clustering_output = m.execute_full_pipeline(\n",
    "                    logs,\n",
    "                    triplet_coefficient,\n",
    "                    parser,\n",
    "                    embedder,\n",
    "                    embedding_distance_fn,\n",
    "                    line_distance_fn,\n",
    "                    clustering_fn,\n",
    "                    float_precision,\n",
    "                )\n",
    "                L.append({\"build_log\": build_log, **clustering_output})\n",
    "            pbar.update(1)\n",
    "            i += 1\n",
    "\n",
    "with open(\"./results.json\", \"w\") as fp:\n",
    "    json.dump(L, fp, cls=Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "severityPrediction",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
