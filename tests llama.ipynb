{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for reading HuggingFace token\n",
    "\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install llama-index-embeddings-huggingface\n",
    "# %pip install llama-index-embeddings-instructor\n",
    "\n",
    "# !pip install llama-index\n",
    "\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# loads BAAI/bge-small-en\n",
    "# embed_model = HuggingFaceEmbedding()\n",
    "\n",
    "#models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "#models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "\n",
    "def get_huggingface_llama():\n",
    "    f = open(\"huggingface_llama.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "# loads BAAI/bge-small-en-v1.5\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"meta-llama/Llama-2-7b-chat-hf\", token=get_huggingface_llama())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import llm as llm_embedding\n",
    "\n",
    "def get_huggingface_llama():\n",
    "    f = open(\"huggingface_llama.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "def embeddings_robin(logs):    \n",
    "    models_names = [\"meta-llama/Llama-2-7b-chat-hf\",\"WhereIsAI/UAE-Large-V1\", \"BAAI/bge-large-en-v1.5\"]\n",
    "    model_name = models_names[2]\n",
    "    init_embedder = llm_embedding.generate_embeddings_llm(model_name=model_name,token=get_huggingface_llama(), use_cpu=True)\n",
    "    ## using pooling by mean\n",
    "    pooling_fn = llm_embedding.get_pooling_function(\"mean\")\n",
    "    #pooling_fn = lambda embedding:embedding\n",
    "    embedder = lambda logs: init_embedder(logs, pooling_fn,limit_tokens=100,precision=np.float16)# type: ignore\n",
    "    return (embedder)\n",
    "\n",
    "logs = [\"teste de hello world\"]\n",
    "print(embeddings_robin(logs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def get_huggingface_llama():\n",
    "    f = open(\"huggingface_llama.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=get_huggingface_llama())\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "print(len(embeddings))\n",
    "print(len(embeddings[0]))\n",
    "print(embeddings)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
