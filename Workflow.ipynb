{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"logs\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "#logName = 'Ciena_error_lines_20220701-20220715.txt' # Name of file to be parsed\n",
    "#logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "/home/vbertalan/Downloads/CSL-1/CSL/logs/\n",
      "Parsing file: /home/vbertalan/Downloads/CSL-1/CSL/logs/Andriod_2k.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 18308.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.301747]\n"
     ]
    }
   ],
   "source": [
    "## First Step - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: /home/vbertalan/Downloads/CSL-1/CSL/logs/Andriod_2k.log\n"
     ]
    }
   ],
   "source": [
    "## Second Step - Embedding Creation Using Transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName)) # Input directory\n",
    "\n",
    "## Code\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using standard MPNet transformer\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        print(\"Iniciando encode\")\n",
    "        vectors = model.encode(raw_content)\n",
    "        pickle.dump(vectors, open(path_to_file, 'wb'))\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_mat = transform(os.path.basename(logName))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "O número de linhas do arquivo transformado é 2000\n",
      "O número de colunas do arquivo transformado é 768\n",
      "-0.04948146\n",
      "-0.0075010364\n"
     ]
    }
   ],
   "source": [
    "print(type(vector_mat))\n",
    "print(\"O número de linhas do arquivo transformado é {}\".format(len(vector_mat)))\n",
    "print(\"O número de colunas do arquivo transformado é {}\".format(len(vector_mat[0])))\n",
    "print(vector_mat[2][1])\n",
    "print(vector_mat[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1839)\n"
     ]
    }
   ],
   "source": [
    "## Third Step - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "#logName = 'Ciena_error_lines_20220701-20220715.txt' # Name of file to be parsed\n",
    "#logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variables list to variable matrix\n",
    "mlb = MultiLabelBinarizer()\n",
    "var_matrix = pd.DataFrame(mlb.fit_transform(var_df),columns=mlb.classes_)\n",
    "print (var_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix de vetores tem 2000 linhas\n",
      "A matrix de vetores tem 768 colunas\n",
      "A matrix de variaveis tem 2000 linhas\n",
      "A matrix de variaveis tem 1839 colunas\n",
      "A matrix concatenada tem 2000 linhas\n",
      "A matrix concatenada tem 2607 colunas\n"
     ]
    }
   ],
   "source": [
    "## Fourth Step - Concatenates Embeddings Matrix with Variables Matrix Using Numpy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_mat = var_matrix.to_numpy()\n",
    "\n",
    "print(\"A matrix de vetores tem {} linhas\".format(len(vector_mat)))\n",
    "print(\"A matrix de vetores tem {} colunas\".format(len(vector_mat[0])))\n",
    "print(\"A matrix de variaveis tem {} linhas\".format(len(var_mat)))\n",
    "print(\"A matrix de variaveis tem {} colunas\".format(len(var_mat[0])))\n",
    "\n",
    "concat_matrix = np.hstack((vector_mat,var_mat))\n",
    "\n",
    "print(\"A matrix concatenada tem {} linhas\".format(len(concat_matrix)))\n",
    "print(\"A matrix concatenada tem {} colunas\".format(len(concat_matrix[0])))\n",
    "\n",
    "#np.savetxt(\"concat_matrix_numpy.csv\", concat_matrix, delimiter=\";\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix de vetores tem formato (2000, 768)\n",
      "A matrix de variaveis tem formato (2000, 1839)\n",
      "A matrix de variaveis tem formato (2000, 2607)\n"
     ]
    }
   ],
   "source": [
    "## Alternate Fourth Step - Concatenates Embeddings Matrix with Variables Matrix Using Pandas\n",
    "\n",
    "vector_df = pd.DataFrame(vector_mat, columns = None)\n",
    "\n",
    "print(\"A matrix de vetores tem formato {}\".format(vector_df.shape))\n",
    "print(\"A matrix de variaveis tem formato {}\".format(var_matrix.shape))\n",
    "\n",
    "concat_matrix = pd.concat([vector_df, var_matrix], axis=1)\n",
    "\n",
    "print(\"A matrix de variaveis tem formato {}\".format(concat_matrix.shape))\n",
    "\n",
    "#concat_matrix.to_csv(\"concat_matrix_pandas.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O numero de clusters e 8\n",
      "Os clusters de cada elemento sao [ 7  7 -1 ...  8  8  8]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Fifth Step - Clustering with HDBScan\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN()\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50,min_samples=1,metric='euclidean',\n",
    "                            allow_single_cluster=False,cluster_selection_method='leaf')\n",
    "clusterer.fit(concat_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento sao {}\".format(cluster_labels))\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Sixth Step - Plotting with TSNE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
