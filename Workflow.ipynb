{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Start - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"logs\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "#logName = 'Ciena_error_lines_20220701-20220715.txt' # Name of file to be parsed\n",
    "#logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\logs\\\n",
      "Parsing file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\logs\\Andriod_2k.log\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 13266.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.582818]\n"
     ]
    }
   ],
   "source": [
    "## First Step - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\logs\\Ciena_error_lines_20220701-20220715.txt\n"
     ]
    }
   ],
   "source": [
    "## Second Step - Embedding Creation Using Transformers\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "#logName = 'Ciena_error_lines_20220701-20220715.txt' # Name of file to be parsed\n",
    "logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName)) # Input directory\n",
    "\n",
    "## Code\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using standard MPNet transformer\n",
    "        model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        print(\"Iniciando encode\")\n",
    "        vectors = model.encode(raw_content)\n",
    "        pickle.dump(vectors, open(path_to_file, 'wb'))\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "O número de linhas do arquivo transformado é 15710\n",
      "O número de colunas do arquivo transformado é 768\n",
      "0.0185639\n",
      "0.005103809\n"
     ]
    }
   ],
   "source": [
    "print(type(vector_df))\n",
    "print(\"O número de linhas do arquivo transformado é {}\".format(len(vector_df)))\n",
    "print(\"O número de colunas do arquivo transformado é {}\".format(len(vector_df[0])))\n",
    "print(vector_df[2][1])\n",
    "print(vector_df[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 1839)\n",
      "-1,\n",
      "-2147483632\n",
      "0, 0),\n",
      "0|com.tencent.mm|4097|null|10112\n",
      "0|com.tencent.mobileqq|121|null|10111\n",
      "1\n",
      "1,\n",
      "10020\n",
      "10027\n",
      "10037,callingPid\n",
      "10057\n"
     ]
    }
   ],
   "source": [
    "## Third Step - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variables list to variable matrix\n",
    "mlb = MultiLabelBinarizer()\n",
    "var_matrix = pd.DataFrame(mlb.fit_transform(var_df),columns=mlb.classes_)\n",
    "print (var_matrix.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
