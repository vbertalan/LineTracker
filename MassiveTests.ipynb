{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################## LIBRARIES ################################## \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "from bertopic.dimensionality import BaseDimensionalityReduction        \n",
    "from bertopic.cluster import BaseCluster\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import DrainMethod\n",
    "import contextlib\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from rouge import Rouge\n",
    "\n",
    "############################## AUXILIARY METHODS ############################## \n",
    "\n",
    "# Code for reading HuggingFace token\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    # if (path.is_file()):\n",
    "    #     vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    # else:\n",
    "    #     Using TFIDF Vectorizer \n",
    "    #     print(\"Starting encode\")\n",
    "    #     tr_idf_model  = TfidfVectorizer()\n",
    "    #     vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "    #     pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    # Using TFIDF Vectorizer \n",
    "    tr_idf_model  = TfidfVectorizer()\n",
    "    vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "    pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "\n",
    "    return vectors_tfidf\n",
    "\n",
    "def creates_lists(clusterer):\n",
    "\n",
    "    ## General Parameters\n",
    "    cluster_idxs = []\n",
    "    cluster_lines = []\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "    # Creates blank lists\n",
    "    for elem in range (clusterer.labels_.max()+1):\n",
    "        cluster_idxs.append([])\n",
    "        cluster_lines.append([])\n",
    "\n",
    "    # Populate the lists with cluster elements\n",
    "    for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "        if elem != -1:\n",
    "            cluster_idxs[elem].append(idx[0])\n",
    "            cluster_lines[elem].append(elem_df[idx[0]])\n",
    "        \n",
    "    return (cluster_idxs, cluster_lines)\n",
    "\n",
    "## Gets number of different templates using Drain\n",
    "def get_template_number():\n",
    "\n",
    "    target_file = \"results/\" + dataset + \"_lines.txt_templates.csv\"\n",
    "    csv = pd.read_csv(target_file)\n",
    "    content = csv[\"EventTemplate\"]\n",
    "    return (len(content))\n",
    "\n",
    "\n",
    "################################# MAIN METHODS ################################ \n",
    "\n",
    "# Code for reading HuggingFace token\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "# Parse logs using Drain\n",
    "def parse_logs(st=0.5, depth=5):\n",
    "    st = st # Drain similarity threshold\n",
    "    depth = depth # Max depth of the parsing tree\n",
    "\n",
    "    ## Code\n",
    "    parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "    parser.parse(log_file)\n",
    "\n",
    "    parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   \n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "# Creates distance matrix, using Euclidean distance\n",
    "def create_distance_matrix(vector_df):\n",
    "    # Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "    tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "    #Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(tfidf_distance)\n",
    "    max_val = np.max(tfidf_distance)\n",
    "    tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "    return (tfidf_distance)\n",
    "\n",
    "# Creates variable matrix, using Jaccard distance\n",
    "def create_variable_matrix():\n",
    "    ## General Parameters\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "    # Breaks the string into lists\n",
    "    for i, line in var_df.items():\n",
    "        var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "    # Transforms variable list to variable sparse matrix\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    var_df = mlb.fit_transform(var_df)\n",
    "    var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "    return (var_distance)\n",
    "\n",
    "def creates_closeness_matrix(tfidf_distance):\n",
    "    # Creates Count Matrix using line numbers from log lines as the counter\n",
    "    count_list = []\n",
    "    n = len(tfidf_distance)\n",
    "    count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "            count_list.append(i)\n",
    "\n",
    "    # Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "    count_array = np.array(count_list)\n",
    "    for x in count_array:\n",
    "        for y in count_array:\n",
    "            count_distance[x,y] = abs(x-y)\n",
    "    # Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(count_distance)\n",
    "    max_val = np.max(count_distance)\n",
    "    count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "    return (count_distance)\n",
    "\n",
    "def saves_matrices(distance_mat, variable_mat, closeness_mat):\n",
    "    np.save(\"tfidf_distance_\" + logName + \".csv\", distance_mat)\n",
    "    np.save(\"var_distance_\" + logName + \".csv\", variable_mat)\n",
    "    np.save(\"count_distance_\" + logName + \".csv\", closeness_mat)\n",
    "\n",
    "def loads_matrices():\n",
    "    tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "    count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "    var_distance = np.load(\"var_distance_\" + logName + \".csv\") \n",
    "    return (tfidf_distance, count_distance, var_distance)\n",
    "\n",
    "def joins_matrices(tfidf_distance, var_distance, count_distance, alpha, beta, gamma):\n",
    "\n",
    "    if alpha+beta+gamma > 1:\n",
    "        raise Exception(\"Values have to sum 1!\")\n",
    "\n",
    "    # New matrices, corrected by the weights\n",
    "    tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "    var_distance_wtd = np.dot(beta, var_distance)\n",
    "    count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "    # Sums remaining matrices\n",
    "    unified_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)\n",
    "    return (unified_matrix)\n",
    "\n",
    "def cluster_kmedoids(unified_matrix, cluster_num):\n",
    "    ## Clusters with cluster_kmedoids\n",
    "\n",
    "    clusterer = KMedoids(n_clusters=cluster_num, method='pam', init='random')\n",
    "    clusterer.fit(unified_matrix)\n",
    "\n",
    "    ## Checks number of outliers\n",
    "    cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "    return (clusterer)\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        empty_reduction_model = BaseDimensionalityReduction()\n",
    "        empty_cluster_model = KMedoids(n_clusters = 1)\n",
    "        \n",
    "        topic_model = BERTopic(hdbscan_model=empty_cluster_model, umap_model=empty_reduction_model, top_n_words=10)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return (summary)\n",
    "\n",
    "def bertopic_previous_clustering(clusterer):\n",
    "    cluster_idxs, cluster_lines = creates_lists(clusterer)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    ## Creates list of boolean values, representing summarized topics\n",
    "    for idx in range(clusterer.labels_.max()):\n",
    "        cluster_topic.append(None)\n",
    "\n",
    "    for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "        ## For each cluster, maps topics, and defines them as the summary\n",
    "        if (cluster_topic[elem-1] == None):\n",
    "            summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "            cluster_topic[elem-1] = summary\n",
    "        \n",
    "        if elem == -1:\n",
    "            topic_summaries.append(\"\")\n",
    "        else:\n",
    "            topic_summaries.append(cluster_topic[elem-1])\n",
    "        \n",
    "        target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "        with open (target_file, \"w\") as f:\n",
    "            for line in topic_summaries:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "def consider_previous_clustering():\n",
    "    ## Tests with BerTopic\n",
    "\n",
    "\n",
    "    target_file = \"ground_truths/\" + dataset + \"_lines.txt_structured.csv\"\n",
    "    csv = pd.read_csv(target_file)\n",
    "    content = csv[\"EventTemplate\"]\n",
    "    num_topics = 10\n",
    "    line_file = []\n",
    "    line_set = []\n",
    "\n",
    "\n",
    "\n",
    "    cluster_model = KMedoids(n_clusters=1)\n",
    "    empty_reduction_model = BaseDimensionalityReduction()\n",
    "    topic_model = BERTopic(hdbscan_model=cluster_model, umap_model=empty_reduction_model)\n",
    "\n",
    "    for idx, line in enumerate(content):\n",
    "\n",
    "        line_set.append(line + '\\n')\n",
    "\n",
    "        if (idx % 20 == 19):\n",
    "    \n",
    "            # print(\"Chegamos ao idx {}\".format(idx))\n",
    "            # print(line_set)\n",
    "\n",
    "            #Applies BertTopic\n",
    "            topics, probs = topic_model.fit_transform(line_set)\n",
    "\n",
    "            #Gets summary of topics\n",
    "            topic_model.get_topic(0)\n",
    "            top_topic = topic_model.get_topic(0)\n",
    "            words = [i[0] for i in top_topic]\n",
    "            summary = ' '.join(words)\n",
    "\n",
    "            #Finds most representative line inside the cluster\n",
    "            best_line = find_best_line(line_set, summary)\n",
    "\n",
    "            for num in range(20):\n",
    "                line_file.append(summary)\n",
    "\n",
    "            line_set = []\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (\"ground_truths/\" + dataset + \"_bert_topics.txt\", \"w\") as f:\n",
    "        for line in line_file:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    return line_file\n",
    "\n",
    "def create_new_bertopic_model(cluster_num=8):\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "        for line in line_file:\n",
    "            lines.append(line)\n",
    "\n",
    "    umap_model = UMAP(init='random')\n",
    "    hdbscan_model = KMedoids(n_clusters=cluster_num, method='pam', init='random')\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model, top_n_words=10)\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(lines)\n",
    "    return (topic_model)\n",
    "\n",
    "def bertopic_new_clustering(cluster_num = 8):\n",
    "\n",
    "    topic_model = create_new_bertopic_model(cluster_num = cluster_num)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    for elem in topic_model.topics_:\n",
    "        \n",
    "        line_topic = topic_model.get_topic(elem)\n",
    "        words = [i[0] for i in line_topic]\n",
    "        summary = ' '.join(words)\n",
    "        topic_summaries.append(summary)\n",
    "\n",
    "\n",
    "    target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (target_file, \"w\") as f:\n",
    "        for line in topic_summaries:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "## Method to find the most representative line inside the cluster\n",
    "## raw_lines = list of lines inside LogSummary's cluster\n",
    "## word_list = list of tokens composed by the LDA/BertTopic\n",
    "def find_best_line(raw_lines, word_list):\n",
    "\n",
    "    tk = WhitespaceTokenizer()\n",
    "\n",
    "    closest_line = 0\n",
    "    similar_tokens = 0\n",
    "    max_similarity = 0\n",
    "    for idx, line in enumerate(raw_lines):\n",
    "        tokenized_line = tk.tokenize(line.lower())\n",
    "        for token in tokenized_line:\n",
    "            if token in word_list:\n",
    "                similar_tokens += 1\n",
    "        #print (\"Line {} has {} identical tokens\".format(idx, similar_tokens))\n",
    "        if similar_tokens > max_similarity:\n",
    "           max_similarity = similar_tokens\n",
    "           closest_line = idx\n",
    "        similar_tokens = 0\n",
    "    return (raw_lines[closest_line])        \n",
    "\n",
    "def calculates_metrics():\n",
    "    \n",
    "    rouge = Rouge()\n",
    "\n",
    "    count_precision = 0\n",
    "    count_recall = 0\n",
    "    count_f1 = 0\n",
    "    total_lines = 2000\n",
    "\n",
    "    target_file = \"_bert_topics_tests.txt\"\n",
    "\n",
    "    # Opens external files with ground truth summaries and created topics\n",
    "    with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "        open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "        for line_summary, line_topic in zip(summaries, topics):\n",
    "            line_summary = line_summary[:-2]\n",
    "            line_summaries = line_summary.split(\";\")\n",
    "\n",
    "            for summary in line_summaries:\n",
    "                current_precision = 0\n",
    "                current_recall = 0\n",
    "                current_f1 = 0\n",
    "                metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']  \n",
    "\n",
    "                ## If the summary improves the f1 score, saves its metrics\n",
    "                if (current_f1 < metrics['f']):\n",
    "                    current_precision = metrics['p']\n",
    "                    current_recall = metrics['r']\n",
    "                    current_f1 = metrics['f']\n",
    "            \n",
    "            count_precision += current_precision\n",
    "            count_recall += current_recall        \n",
    "            count_f1 += current_f1\n",
    "\n",
    "    final_precision = count_precision/total_lines\n",
    "    final_recall = count_recall/total_lines\n",
    "    final_f1 = count_f1/total_lines\n",
    "\n",
    "    # final = \"The precision is {}, the recall is {}, the f1 score is {}\".format(final_precision, final_recall, final_f1)\n",
    "    # print (final)\n",
    "    return (final_f1)\n",
    "\n",
    "################################# TEST SCENARIOS ################################ \n",
    "\n",
    "## Cenário A\n",
    "## Testa usando clusters pré-definidos, e usando BerTopic sem clusterizar\n",
    "def tests_scenario_A(drain_st, drain_depth):\n",
    "\n",
    "    parameters = (\"Testing scenario A using raw data matrix and predefined clustering, with drain st {}, drain depth {}\".\n",
    "          format(drain_st, drain_depth))\n",
    "    print(parameters)\n",
    "    \n",
    "    #parse_logs(drain_st, drain_depth)\n",
    "\n",
    "    consider_previous_clustering()\n",
    "\n",
    "    final = calculates_metrics()\n",
    "    #print(\"F1 score: {}\".format(final))\n",
    "\n",
    "    return (final)\n",
    "\n",
    "## Cenário B\n",
    "## Testando usando BerTopic para clusterizar, sem considerar matriz unificada, transformando os dados brutos\n",
    "def tests_scenario_B(drain_st, drain_depth):\n",
    "    \n",
    "    n_clusters = get_template_number()\n",
    "    \n",
    "    parameters = (\"Testing scenario B using raw data matrix and BerTopic K-Medoids clustering, drain st {}, drain depth {}, cluster number {}\".\n",
    "          format(drain_st, drain_depth, n_clusters))\n",
    "    print(parameters)\n",
    "\n",
    "    # Runs BerTopic\n",
    "    #parse_logs(drain_st, drain_depth)\n",
    "    topic_summaries = bertopic_new_clustering(n_clusters)\n",
    "\n",
    "    final = calculates_metrics()\n",
    "    #print(\"F1 score: {}\".format(final))\n",
    "\n",
    "    return (final)\n",
    "\n",
    "## Cenário C\n",
    "## Testando usando transformação via matriz unificada, depois BerTopic para seleção de tópicos\n",
    "def tests_scenario_C(drain_st, drain_depth, alpha, beta, gamma):\n",
    "\n",
    "    #parse_logs(drain_st, drain_depth)\n",
    "    n_clusters = get_template_number()\n",
    "\n",
    "    parameters = (\"Testing scenario C using joint matrix and BerTopic topic modeling, with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, cluster number {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, n_clusters))\n",
    "    print(parameters)\n",
    "\n",
    "    # Criação matriz unificada\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "      \n",
    "    clusterer = cluster_kmedoids (joint_matrix, n_clusters)\n",
    "\n",
    "    topic_summaries = bertopic_previous_clustering(clusterer)\n",
    "    \n",
    "    final = calculates_metrics()\n",
    "    #print(\"F1 score: {}\".format(final))\n",
    "\n",
    "    return (final)\n",
    "\n",
    "def run_tests(results, dataset, drain_st, drain_depth, num_executions):\n",
    "\n",
    "    #Variable Matrix Parameters\n",
    "    # alpha = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    # beta = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    # gamma = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "    alpha = [0.3]\n",
    "    beta = [0.4, 0.5]\n",
    "    gamma = [0.2, 0.3]\n",
    "\n",
    "    # # Running Single Test for Scenario A\n",
    "    # value = tests_scenario_A(drain_st, drain_depth)\n",
    "    # new_row = [dataset, 'A', drain_st, drain_depth, 0, 0, 0, value]\n",
    "    # results.loc[len(results)] = new_row\n",
    "\n",
    "    # # Running num_executions for Scenario B\n",
    "    # for i in range(num_executions):\n",
    "    #     value = tests_scenario_B(drain_st, drain_depth)\n",
    "    #     new_row = [dataset, 'B', drain_st, drain_depth, 0, 0, 0, value]\n",
    "    #     results.loc[len(results)] = new_row\n",
    "\n",
    "    # Testing Different Hyperparameters for Scenario C\n",
    "    best_f1 = 0\n",
    "    best_alpha = 0\n",
    "    best_beta = 0\n",
    "    best_gamma = 0\n",
    "    for a in alpha:\n",
    "        for b in beta:\n",
    "            for g in gamma:\n",
    "                if (a+b+g != 1):\n",
    "                    pass\n",
    "                else:\n",
    "                    for i in range(num_executions):\n",
    "                        try:\n",
    "                            value = tests_scenario_C(drain_st, drain_depth, a, b, g)\n",
    "                            print(value)\n",
    "                            new_row = [dataset, 'C', drain_st, drain_depth, best_alpha, best_beta, best_gamma, value]\n",
    "                            results.loc[len(results)] = new_row\n",
    "                        except Exception as error:\n",
    "                            value = 0\n",
    "                            print(error)\n",
    "                        if (value > best_f1):\n",
    "                            best_f1 = value\n",
    "                            best_alpha = a\n",
    "                            best_beta = b\n",
    "                            best_gamma = g\n",
    "    print(\"A melhor combinação foi alpha {}, beta {}, gamma {}\".format(best_alpha, best_beta, best_gamma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.4, gamma 0.3, cluster number 53\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.5, gamma 0.2, cluster number 53\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 5, alpha 0.3, beta 0.4, gamma 0.3, cluster number 13\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 5, alpha 0.3, beta 0.5, gamma 0.2, cluster number 13\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.4, drain depth 4, alpha 0.3, beta 0.4, gamma 0.3, cluster number 50\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.4, drain depth 4, alpha 0.3, beta 0.5, gamma 0.2, cluster number 50\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 4, alpha 0.3, beta 0.4, gamma 0.3, cluster number 44\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 4, alpha 0.3, beta 0.5, gamma 0.2, cluster number 44\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.4, gamma 0.3, cluster number 45\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.5, gamma 0.2, cluster number 45\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.4, gamma 0.3, cluster number 57\n",
      "name 'logSlice' is not defined\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 3, alpha 0.3, beta 0.5, gamma 0.2, cluster number 57\n",
      "name 'logSlice' is not defined\n",
      "A melhor combinação foi alpha 0, beta 0, gamma 0\n"
     ]
    }
   ],
   "source": [
    "################################## RUNNING TESTS ################################## \n",
    "\n",
    "## General parameters \n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "results = pd.DataFrame(columns=['Dataset','Scenario', 'Drain St', 'Drain Depth', 'Alpha', 'Beta', 'Gamma', 'F1'])\n",
    "num_executions = 1\n",
    "\n",
    "# Testing BGL\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "drain_st = 0.3\n",
    "drain_depth = 3\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Testing HDFS\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "drain_st = 0.3\n",
    "drain_depth = 5\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Testing HPC\n",
    "dataset = \"hpc\" # The name of the dataset being tested\n",
    "drain_st = 0.4\n",
    "drain_depth = 4\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Testing Proxifier\n",
    "dataset = \"Proxifier\" # The name of the dataset being tested\n",
    "drain_st = 0.3\n",
    "drain_depth = 4\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Testing Spark\n",
    "dataset = \"spark\" # The name of the dataset being tested\n",
    "drain_st = 0.3\n",
    "drain_depth = 3\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Testing Zookeeper\n",
    "dataset = \"Zookeeper\" # The name of the dataset being tested\n",
    "drain_st = 0.3\n",
    "drain_depth = 3\n",
    "#logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "logName = dataset + \"_split.txt\" # Name of the sliced file for training\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "parse_logs(drain_st, drain_depth)\n",
    "run_tests(results, dataset, drain_st, drain_depth, num_executions)\n",
    "\n",
    "# Saves final CSV\n",
    "results.to_csv('testing_results.csv', index=False)  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
