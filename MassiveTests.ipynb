{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbertalan/anaconda3/lib/python3.9/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "\n",
    "################################## LIBRARIES ################################## \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import DrainMethod\n",
    "import contextlib\n",
    "import hdbscan\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "############################## AUXILIARY METHODS ############################## \n",
    "\n",
    "# Code for reading HuggingFace token\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    # if (path.is_file()):\n",
    "    #     vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    # else:\n",
    "    #     Using TFIDF Vectorizer \n",
    "    #     print(\"Starting encode\")\n",
    "    #     tr_idf_model  = TfidfVectorizer()\n",
    "    #     vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "    #     pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    # Using TFIDF Vectorizer \n",
    "    tr_idf_model  = TfidfVectorizer()\n",
    "    vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "    pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "\n",
    "    return vectors_tfidf\n",
    "\n",
    "def creates_lists(clusterer):\n",
    "\n",
    "    ## General Parameters\n",
    "    cluster_idxs = []\n",
    "    cluster_lines = []\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "    # Creates blank lists\n",
    "    for elem in range (clusterer.labels_.max()+1):\n",
    "        cluster_idxs.append([])\n",
    "        cluster_lines.append([])\n",
    "\n",
    "    # Populate the lists with cluster elements\n",
    "    for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "        if elem != -1:\n",
    "            cluster_idxs[elem].append(idx[0])\n",
    "            cluster_lines[elem].append(elem_df[idx[0]])\n",
    "        \n",
    "    return (cluster_idxs, cluster_lines)\n",
    "\n",
    "## Gets number of different templates using Drain\n",
    "def get_template_number():\n",
    "\n",
    "    target_file = \"results/\" + dataset + \"_lines.txt_templates.csv\"\n",
    "    csv = pd.read_csv(target_file)\n",
    "    content = csv[\"EventTemplate\"]\n",
    "    return (len(content))\n",
    "\n",
    "\n",
    "################################# MAIN METHODS ################################ \n",
    "\n",
    "# Code for reading HuggingFace token\n",
    "def get_huggingface_token():\n",
    "    f = open(\"huggingface_token.txt\", \"r\")\n",
    "    return (f.read())\n",
    "\n",
    "# Parse logs using Drain\n",
    "def parse_logs(st=0.5, depth=5):\n",
    "    st = st # Drain similarity threshold\n",
    "    depth = depth # Max depth of the parsing tree\n",
    "\n",
    "    ## Code\n",
    "    parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "    parser.parse(log_file)\n",
    "\n",
    "    parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   \n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "# Creates distance matrix, using Euclidean distance\n",
    "def create_distance_matrix(vector_df):\n",
    "    # Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "    tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "    #Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(tfidf_distance)\n",
    "    max_val = np.max(tfidf_distance)\n",
    "    tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "    return (tfidf_distance)\n",
    "\n",
    "# Creates variable matrix, using Jaccard distance\n",
    "def create_variable_matrix():\n",
    "    ## General Parameters\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "    # Breaks the string into lists\n",
    "    for i, line in var_df.items():\n",
    "        var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "    # Transforms variable list to variable sparse matrix\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    var_df = mlb.fit_transform(var_df)\n",
    "    var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "    return (var_distance)\n",
    "\n",
    "def creates_closeness_matrix(tfidf_distance):\n",
    "    # Creates Count Matrix using line numbers from log lines as the counter\n",
    "    count_list = []\n",
    "    n = len(tfidf_distance)\n",
    "    count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "            count_list.append(i)\n",
    "\n",
    "    # Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "    count_array = np.array(count_list)\n",
    "    for x in count_array:\n",
    "        for y in count_array:\n",
    "            count_distance[x,y] = abs(x-y)\n",
    "    # Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(count_distance)\n",
    "    max_val = np.max(count_distance)\n",
    "    count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "    return (count_distance)\n",
    "\n",
    "def saves_matrices(distance_mat, variable_mat, closeness_mat):\n",
    "    np.save(\"tfidf_distance_\" + logName + \".csv\", distance_mat)\n",
    "    np.save(\"var_distance_\" + logName + \".csv\", variable_mat)\n",
    "    np.save(\"count_distance_\" + logName + \".csv\", closeness_mat)\n",
    "\n",
    "def loads_matrices():\n",
    "    tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "    count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "    var_distance = np.load(\"var_distance_\" + logName + \".csv\") \n",
    "    return (tfidf_distance, count_distance, var_distance)\n",
    "\n",
    "def joins_matrices(tfidf_distance, var_distance, count_distance, alpha, beta, gamma):\n",
    "\n",
    "    if alpha+beta+gamma > 1:\n",
    "        raise Exception(\"Values have to sum 1!\")\n",
    "\n",
    "    # New matrices, corrected by the weights\n",
    "    tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "    var_distance_wtd = np.dot(beta, var_distance)\n",
    "    count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "    # Sums remaining matrices\n",
    "    unified_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)\n",
    "    return (unified_matrix)\n",
    "\n",
    "def cluster_hdbscan(unified_matrix, cluster_size, mn_samples, cluster_selection_epsilon, alpha, leaf_size):\n",
    "    ## Clusters with HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size,min_samples=mn_samples,metric='precomputed',\n",
    "                                cluster_selection_epsilon=cluster_selection_epsilon, alpha=alpha, leaf_size=leaf_size, \n",
    "                                allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                                gen_min_span_tree=True)\n",
    "\n",
    "    clusterer.fit(unified_matrix)\n",
    "\n",
    "    ## Checks number of outliers\n",
    "    cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "    return (clusterer)\n",
    "\n",
    "def cluster_kmedoids(unified_matrix, cluster_num):\n",
    "    ## Clusters with cluster_kmedoids\n",
    "\n",
    "    clusterer = KMedoids(n_clusters=cluster_num)\n",
    "    clusterer.fit(unified_matrix)\n",
    "\n",
    "    ## Checks number of outliers\n",
    "    cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "    return (clusterer)\n",
    "\n",
    "def cluster_hdbscan_raw_data(data, cluster_size, mn_samples, cluster_selection_epsilon, alpha, leaf_size):\n",
    "    ## Clusters with HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size,min_samples=mn_samples,metric='euclidean',\n",
    "                                cluster_selection_epsilon=cluster_selection_epsilon, alpha=alpha, leaf_size=leaf_size, \n",
    "                                allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                                gen_min_span_tree=False)\n",
    "\n",
    "    clusterer.fit(data)\n",
    "\n",
    "    return (clusterer)\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        from bertopic import BERTopic\n",
    "        from bertopic.cluster import BaseCluster\n",
    "        from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "\n",
    "        empty_reduction_model = BaseDimensionalityReduction()\n",
    "        empty_cluster_model = KMedoids(n_clusters = 1)\n",
    "        \n",
    "        #sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\", token=get_huggingface_token())\n",
    "\n",
    "        topic_model = BERTopic(hdbscan_model=empty_cluster_model, umap_model=empty_reduction_model, top_n_words=10)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return (summary)\n",
    "\n",
    "def bertopic_previous_clustering(clusterer):\n",
    "    cluster_idxs, cluster_lines = creates_lists(clusterer)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    ## Creates list of boolean values, representing summarized topics\n",
    "    for idx in range(clusterer.labels_.max()):\n",
    "        cluster_topic.append(None)\n",
    "\n",
    "    for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "        ## For each cluster, maps topics, and defines them as the summary\n",
    "        if (cluster_topic[elem-1] == None):\n",
    "            summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "            cluster_topic[elem-1] = summary\n",
    "        \n",
    "        if elem == -1:\n",
    "            topic_summaries.append(\"\")\n",
    "        else:\n",
    "            topic_summaries.append(cluster_topic[elem-1])\n",
    "        \n",
    "        target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "        with open (target_file, \"w\") as f:\n",
    "            for line in topic_summaries:\n",
    "                f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "def consider_previous_clustering():\n",
    "    ## Tests with BerTopic\n",
    "\n",
    "    from sklearn_extra.cluster import KMedoids\n",
    "    from bertopic import BERTopic\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    import os\n",
    "\n",
    "    target_file = \"ground_truths/\" + dataset + \"_lines.txt_structured.csv\"\n",
    "    csv = pd.read_csv(target_file)\n",
    "    content = csv[\"EventTemplate\"]\n",
    "    num_topics = 10\n",
    "    line_file = []\n",
    "    line_set = []\n",
    "\n",
    "    from bertopic.cluster import BaseCluster\n",
    "    from bertopic.dimensionality import BaseDimensionalityReduction\n",
    "\n",
    "    cluster_model = KMedoids(n_clusters=1)\n",
    "    empty_reduction_model = BaseDimensionalityReduction()\n",
    "    topic_model = BERTopic(hdbscan_model=cluster_model, umap_model=empty_reduction_model)\n",
    "\n",
    "    for idx, line in enumerate(content):\n",
    "\n",
    "        line_set.append(line + '\\n')\n",
    "\n",
    "        if (idx % 20 == 19):\n",
    "    \n",
    "            # print(\"Chegamos ao idx {}\".format(idx))\n",
    "            # print(line_set)\n",
    "\n",
    "            #Applies BertTopic\n",
    "            topics, probs = topic_model.fit_transform(line_set)\n",
    "\n",
    "            #Gets summary of topics\n",
    "            topic_model.get_topic(0)\n",
    "            top_topic = topic_model.get_topic(0)\n",
    "            words = [i[0] for i in top_topic]\n",
    "            summary = ' '.join(words)\n",
    "\n",
    "            #Finds most representative line inside the cluster\n",
    "            best_line = find_best_line(line_set, summary)\n",
    "\n",
    "            for num in range(20):\n",
    "                line_file.append(summary)\n",
    "\n",
    "            line_set = []\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (\"ground_truths/\" + dataset + \"_bert_topics.txt\", \"w\") as f:\n",
    "        for line in line_file:\n",
    "            f.write(f\"{line}\\n\")\n",
    "    \n",
    "    return line_file\n",
    "\n",
    "def create_new_bertopic_model(cluster_num=8):\n",
    "    \n",
    "    lines = []\n",
    "\n",
    "    with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "        for line in line_file:\n",
    "            lines.append(line)\n",
    "\n",
    "    umap_model = UMAP(init='random')\n",
    "    hdbscan_model = KMedoids(n_clusters=cluster_num)\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "    topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, vectorizer_model=vectorizer_model, top_n_words=10)\n",
    "    \n",
    "    topics, probs = topic_model.fit_transform(lines)\n",
    "    return (topic_model)\n",
    "\n",
    "def bertopic_new_clustering(cluster_num = 8):\n",
    "\n",
    "    topic_model = create_new_bertopic_model(cluster_num = cluster_num)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    for elem in topic_model.topics_:\n",
    "        \n",
    "        line_topic = topic_model.get_topic(elem)\n",
    "        words = [i[0] for i in line_topic]\n",
    "        summary = ' '.join(words)\n",
    "        topic_summaries.append(summary)\n",
    "\n",
    "\n",
    "    target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (target_file, \"w\") as f:\n",
    "        for line in topic_summaries:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "## Method to find the most representative line inside the cluster\n",
    "## raw_lines = list of lines inside LogSummary's cluster\n",
    "## word_list = list of tokens composed by the LDA/BertTopic\n",
    "def find_best_line(raw_lines, word_list):\n",
    "\n",
    "    from nltk.tokenize import WhitespaceTokenizer\n",
    "    tk = WhitespaceTokenizer()\n",
    "\n",
    "    closest_line = 0\n",
    "    similar_tokens = 0\n",
    "    max_similarity = 0\n",
    "    for idx, line in enumerate(raw_lines):\n",
    "        tokenized_line = tk.tokenize(line.lower())\n",
    "        for token in tokenized_line:\n",
    "            if token in word_list:\n",
    "                similar_tokens += 1\n",
    "        #print (\"Line {} has {} identical tokens\".format(idx, similar_tokens))\n",
    "        if similar_tokens > max_similarity:\n",
    "           max_similarity = similar_tokens\n",
    "           closest_line = idx\n",
    "        similar_tokens = 0\n",
    "    return (raw_lines[closest_line])        \n",
    "\n",
    "def calculates_metrics():\n",
    "    \n",
    "    from rouge import Rouge \n",
    "    rouge = Rouge()\n",
    "\n",
    "    count_precision = 0\n",
    "    count_recall = 0\n",
    "    count_f1 = 0\n",
    "    total_lines = 2000\n",
    "\n",
    "    target_file = \"_bert_topics_tests.txt\"\n",
    "\n",
    "    # Opens external files with ground truth summaries and created topics\n",
    "    with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "        open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "        for line_summary, line_topic in zip(summaries, topics):\n",
    "            line_summary = line_summary[:-2]\n",
    "            line_summaries = line_summary.split(\";\")\n",
    "\n",
    "            for summary in line_summaries:\n",
    "                current_precision = 0\n",
    "                current_recall = 0\n",
    "                current_f1 = 0\n",
    "                metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']  \n",
    "\n",
    "                ## If the summary improves the f1 score, saves its metrics\n",
    "                if (current_f1 < metrics['f']):\n",
    "                    current_precision = metrics['p']\n",
    "                    current_recall = metrics['r']\n",
    "                    current_f1 = metrics['f']\n",
    "            \n",
    "            count_precision += current_precision\n",
    "            count_recall += current_recall        \n",
    "            count_f1 += current_f1\n",
    "\n",
    "    final_precision = count_precision/total_lines\n",
    "    final_recall = count_recall/total_lines\n",
    "    final_f1 = count_f1/total_lines\n",
    "\n",
    "    final = \"The precision is {}, the recall is {}, the f1 score is {}\".format(final_precision, final_recall, final_f1)\n",
    "    print (final)\n",
    "    return (final_f1)\n",
    "\n",
    "################################# TEST SCENARIOS ################################ \n",
    "\n",
    "## Cenário A\n",
    "## Testa usando clusters pré-definidos, e usando BerTopic sem clusterizar\n",
    "def tests_scenario_A(drain_st, drain_depth):\n",
    "\n",
    "    parameters = (\"Testing scenario A using raw data matrix and predefined clustering, with drain st {}, drain depth {}\".\n",
    "          format(drain_st, drain_depth))\n",
    "    print(parameters)\n",
    "    \n",
    "    parse_logs(drain_st, drain_depth)\n",
    "\n",
    "    consider_previous_clustering()\n",
    "\n",
    "    final = calculates_metrics()\n",
    "    print(\"F1 score: {}\".format(final))\n",
    "\n",
    "    return (final)\n",
    "\n",
    "## Cenário B\n",
    "## Testando usando BerTopic para clusterizar, sem considerar matriz unificada, transformando os dados brutos\n",
    "def tests_scenario_B(drain_st, drain_depth):\n",
    "    \n",
    "    n_clusters = get_template_number()\n",
    "    \n",
    "    parameters = (\"Testing scenario B using raw data matrix and BerTopic K-Medoids clustering, drain st {}, drain depth {}, cluster number {}\".\n",
    "          format(drain_st, drain_depth, n_clusters))\n",
    "    print(parameters)\n",
    "\n",
    "    # Runs BerTopic\n",
    "    parse_logs(drain_st, drain_depth)\n",
    "    topic_summaries = bertopic_new_clustering(n_clusters)\n",
    "\n",
    "    final = calculates_metrics()\n",
    "    print(\"F1 score: {}\".format(final))\n",
    "\n",
    "    return (final)\n",
    "\n",
    "## Cenário C\n",
    "## Testando usando transformação via matriz unificada, depois BerTopic para seleção de tópicos\n",
    "def tests_scenario_C(drain_st, drain_depth, alpha, beta, gamma):\n",
    "\n",
    "    parse_logs(drain_st, drain_depth)\n",
    "    n_clusters = get_template_number()\n",
    "\n",
    "    parameters = (\"Testing scenario C using joint matrix and BerTopic topic modeling, with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, cluster number {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, n_clusters))\n",
    "    print(parameters)\n",
    "\n",
    "    # Criação matriz unificada\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "      \n",
    "    clusterer = cluster_kmedoids (joint_matrix, n_clusters)\n",
    "\n",
    "    topic_summaries = bertopic_previous_clustering(clusterer)\n",
    "    \n",
    "    final = calculates_metrics()\n",
    "    print(\"F1 score: {}\".format(final))\n",
    "\n",
    "\n",
    "    return (final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scenario A using raw data matrix and predefined clustering, with drain st 0.5, drain depth 3\n",
      "The precision is 0.0782499999999984, the recall is 0.3629166666666671, the f1 score is 0.12814560237710168\n",
      "F1 score: 0.12814560237710168\n",
      "Testing scenario B using raw data matrix and BerTopic K-Medoids clustering, drain st 0.5, drain depth 3, cluster number 13\n",
      "The precision is 0.07859999999999824, the recall is 0.36966666666666714, the f1 score is 0.12920512616216392\n",
      "F1 score: 0.12920512616216392\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0, gamma 1, cluster number 13\n",
      "The precision is 0.06999999999999831, the recall is 0.33499999999999974, the f1 score is 0.11551281856985618\n",
      "F1 score: 0.11551281856985618\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.1, gamma 0.9, cluster number 13\n",
      "The precision is 0.0681999999999983, the recall is 0.3272499999999997, the f1 score is 0.1126089724451766\n",
      "F1 score: 0.1126089724451766\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.2, gamma 0.8, cluster number 13\n",
      "The precision is 0.06999999999999816, the recall is 0.3358333333333329, the f1 score is 0.11553113351957713\n",
      "F1 score: 0.11553113351957713\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.3, gamma 0.7, cluster number 13\n",
      "The precision is 0.06699999999999842, the recall is 0.31983333333333336, the f1 score is 0.11045421054265406\n",
      "F1 score: 0.11045421054265406\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.4, gamma 0.6, cluster number 13\n",
      "The precision is 0.06799999999999842, the recall is 0.3225, the f1 score is 0.11194139004094446\n",
      "F1 score: 0.11194139004094446\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.5, gamma 0.5, cluster number 13\n",
      "The precision is 0.07051666666666469, the recall is 0.3242499999999989, the f1 score is 0.1152330566662917\n",
      "F1 score: 0.1152330566662917\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.6, gamma 0.4, cluster number 13\n",
      "The precision is 0.06569999999999841, the recall is 0.31683333333333313, the f1 score is 0.10860256222348864\n",
      "F1 score: 0.10860256222348864\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.7, gamma 0.3, cluster number 13\n",
      "The precision is 0.07275714285714148, the recall is 0.32341666666666685, the f1 score is 0.11789865489773098\n",
      "F1 score: 0.11789865489773098\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.8, gamma 0.2, cluster number 13\n",
      "The precision is 0.06704999999999833, the recall is 0.32108333333333317, the f1 score is 0.11061446693485487\n",
      "F1 score: 0.11061446693485487\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 0.9, gamma 0.1, cluster number 13\n",
      "The precision is 0.07004999999999859, the recall is 0.3310833333333343, the f1 score is 0.11522985155023968\n",
      "F1 score: 0.11522985155023968\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0, beta 1, gamma 0, cluster number 13\n",
      "The precision is 0.1625, the recall is 0.31333333333333313, the f1 score is 0.2133333304127013\n",
      "F1 score: 0.2133333304127013\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0, gamma 0.9, cluster number 13\n",
      "The precision is 0.06999999999999845, the recall is 0.3308333333333337, the f1 score is 0.1151465182182951\n",
      "F1 score: 0.1151465182182951\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.1, gamma 0.8, cluster number 13\n",
      "The precision is 0.06889999999999823, the recall is 0.33033333333333287, the f1 score is 0.11369780021679933\n",
      "F1 score: 0.11369780021679933\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.2, gamma 0.7, cluster number 13\n",
      "The precision is 0.08064047619047418, the recall is 0.32516666666666627, the f1 score is 0.12787600520690737\n",
      "F1 score: 0.12787600520690737\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.3, gamma 0.6, cluster number 13\n",
      "The precision is 0.06799999999999881, the recall is 0.31333333333333313, the f1 score is 0.11103632288372442\n",
      "F1 score: 0.11103632288372442\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.4, gamma 0.5, cluster number 13\n",
      "The precision is 0.06776666666666523, the recall is 0.31583333333333313, the f1 score is 0.111072800271591\n",
      "F1 score: 0.111072800271591\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.5, gamma 0.4, cluster number 13\n",
      "The precision is 0.07091666666666517, the recall is 0.3244166666666663, the f1 score is 0.11573626176484456\n",
      "F1 score: 0.11573626176484456\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.6, gamma 0.3, cluster number 13\n",
      "The precision is 0.06884999999999812, the recall is 0.32258333333333317, the f1 score is 0.11298946690464647\n",
      "F1 score: 0.11298946690464647\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.7, gamma 0.2, cluster number 13\n",
      "The precision is 0.07621666666666521, the recall is 0.32524999999999954, the f1 score is 0.12232157918946675\n",
      "F1 score: 0.12232157918946675\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.8, gamma 0.1, cluster number 13\n",
      "The precision is 0.08019166666666584, the recall is 0.25416666666666626, the f1 score is 0.11824358788216516\n",
      "F1 score: 0.11824358788216516\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.1, beta 0.9, gamma 0, cluster number 13\n",
      "The precision is 0.08070476190476042, the recall is 0.32208333333333317, the f1 score is 0.12776724451083546\n",
      "F1 score: 0.12776724451083546\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0, gamma 0.8, cluster number 13\n",
      "The precision is 0.0714499999999981, the recall is 0.3412499999999994, the f1 score is 0.11780677453549572\n",
      "F1 score: 0.11780677453549572\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.1, gamma 0.7, cluster number 13\n",
      "The precision is 0.06799999999999828, the recall is 0.326666666666666, the f1 score is 0.11230769039250568\n",
      "F1 score: 0.11230769039250568\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.2, gamma 0.6, cluster number 13\n",
      "The precision is 0.06869999999999823, the recall is 0.3301666666666663, the f1 score is 0.1134743570397279\n",
      "F1 score: 0.1134743570397279\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.3, gamma 0.5, cluster number 13\n",
      "The precision is 0.0777071428571415, the recall is 0.33725000000000055, the f1 score is 0.12553418603619312\n",
      "F1 score: 0.12553418603619312\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.4, gamma 0.4, cluster number 13\n",
      "The precision is 0.07268333333333188, the recall is 0.32008333333333316, the f1 score is 0.11742322753988302\n",
      "F1 score: 0.11742322753988302\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.5, gamma 0.3, cluster number 13\n",
      "The precision is 0.07510833333333289, the recall is 0.31658333333333316, the f1 score is 0.11999937638728757\n",
      "F1 score: 0.11999937638728757\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.6, gamma 0.2, cluster number 13\n",
      "The precision is 0.07557499999999798, the recall is 0.31933333333333314, the f1 score is 0.12122280014577369\n",
      "F1 score: 0.12122280014577369\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.2, beta 0.8, gamma 0, cluster number 13\n",
      "The precision is 0.08251666666666492, the recall is 0.33858333333333285, the f1 score is 0.13132279999853577\n",
      "F1 score: 0.13132279999853577\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0, gamma 0.7, cluster number 13\n",
      "The precision is 0.07636666666666421, the recall is 0.31583333333333313, the f1 score is 0.12167322749738198\n",
      "F1 score: 0.12167322749738198\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.1, gamma 0.6, cluster number 13\n",
      "The precision is 0.07617619047618897, the recall is 0.35291666666666743, the f1 score is 0.12469016890591025\n",
      "F1 score: 0.12469016890591025\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.2, gamma 0.5, cluster number 13\n",
      "The precision is 0.07743333333333305, the recall is 0.3241666666666669, the f1 score is 0.12341254372409415\n",
      "F1 score: 0.12341254372409415\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.3, gamma 0.4, cluster number 13\n",
      "The precision is 0.0671666666666649, the recall is 0.31583333333333313, the f1 score is 0.11017322763154905\n",
      "F1 score: 0.11017322763154905\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.4, gamma 0.3, cluster number 13\n",
      "The precision is 0.07433333333333254, the recall is 0.32266666666666627, the f1 score is 0.11944587710567187\n",
      "F1 score: 0.11944587710567187\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.5, gamma 0.2, cluster number 13\n",
      "The precision is 0.06893333333333143, the recall is 0.31533333333333313, the f1 score is 0.11251922884696076\n",
      "F1 score: 0.11251922884696076\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.3, beta 0.7, gamma 0, cluster number 13\n",
      "The precision is 0.08490476190476028, the recall is 0.32270833333333315, the f1 score is 0.1331680380970648\n",
      "F1 score: 0.1331680380970648\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0, gamma 0.6, cluster number 13\n",
      "The precision is 0.07576190476190318, the recall is 0.31658333333333316, the f1 score is 0.12131645095612595\n",
      "F1 score: 0.12131645095612595\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.1, gamma 0.5, cluster number 13\n",
      "The precision is 0.07712619047618903, the recall is 0.3561666666666674, the f1 score is 0.1261187402924154\n",
      "F1 score: 0.1261187402924154\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.2, gamma 0.4, cluster number 13\n",
      "The precision is 0.06011666666666543, the recall is 0.2779166666666657, the f1 score is 0.09836126208055519\n",
      "F1 score: 0.09836126208055519\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.3, gamma 0.3, cluster number 13\n",
      "The precision is 0.0715833333333313, the recall is 0.32341666666666685, the f1 score is 0.11648626175776043\n",
      "F1 score: 0.11648626175776043\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.4, gamma 0.2, cluster number 13\n",
      "The precision is 0.07482857142856988, the recall is 0.34291666666666776, the f1 score is 0.12229609078632453\n",
      "F1 score: 0.12229609078632453\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.5, gamma 0.1, cluster number 13\n",
      "The precision is 0.08889523809523676, the recall is 0.33441666666666797, the f1 score is 0.13840827011612783\n",
      "F1 score: 0.13840827011612783\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.4, beta 0.6, gamma 0, cluster number 13\n",
      "The precision is 0.06636666666666509, the recall is 0.31558333333333316, the f1 score is 0.10930494315177383\n",
      "F1 score: 0.10930494315177383\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0, gamma 0.5, cluster number 13\n",
      "The precision is 0.0827738095238081, the recall is 0.32291666666666685, the f1 score is 0.13023519321510538\n",
      "F1 score: 0.13023519321510538\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0.1, gamma 0.4, cluster number 13\n",
      "The precision is 0.0756666666666659, the recall is 0.32183333333333314, the f1 score is 0.12099908219417431\n",
      "F1 score: 0.12099908219417431\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0.2, gamma 0.3, cluster number 13\n",
      "The precision is 0.07356666666666557, the recall is 0.31583333333333313, the f1 score is 0.11817322753821681\n",
      "F1 score: 0.11817322753821681\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0.3, gamma 0.2, cluster number 13\n",
      "The precision is 0.06693571428571272, the recall is 0.31733333333333313, the f1 score is 0.11024145109160606\n",
      "F1 score: 0.11024145109160606\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0.4, gamma 0.1, cluster number 13\n",
      "The precision is 0.07547857142857017, the recall is 0.31841666666666574, the f1 score is 0.12105769030278413\n",
      "F1 score: 0.12105769030278413\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.5, beta 0.5, gamma 0, cluster number 13\n",
      "The precision is 0.07510714285714129, the recall is 0.3139583333333331, the f1 score is 0.12052518112746281\n",
      "F1 score: 0.12052518112746281\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.6, beta 0, gamma 0.4, cluster number 13\n",
      "The precision is 0.08205952380952297, the recall is 0.3489999999999989, the f1 score is 0.13113797096411206\n",
      "F1 score: 0.13113797096411206\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.6, beta 0.1, gamma 0.3, cluster number 13\n",
      "The precision is 0.07362380952380826, the recall is 0.32133333333333314, the f1 score is 0.11843284290496546\n",
      "F1 score: 0.11843284290496546\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.6, beta 0.2, gamma 0.2, cluster number 13\n",
      "The precision is 0.07703333333333194, the recall is 0.31733333333333286, the f1 score is 0.12265277572054607\n",
      "F1 score: 0.12265277572054607\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.6, beta 0.4, gamma 0, cluster number 13\n",
      "The precision is 0.06026666666666568, the recall is 0.25195833333333256, the f1 score is 0.09601983961843176\n",
      "F1 score: 0.09601983961843176\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.7, beta 0, gamma 0.3, cluster number 13\n",
      "The precision is 0.07765238095237963, the recall is 0.33908333333333257, the f1 score is 0.12538675002040875\n",
      "F1 score: 0.12538675002040875\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.7, beta 0.1, gamma 0.2, cluster number 13\n",
      "The precision is 0.06393809523809423, the recall is 0.2889166666666669, the f1 score is 0.10415918634683298\n",
      "F1 score: 0.10415918634683298\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.7, beta 0.3, gamma 0, cluster number 13\n",
      "The precision is 0.07359999999999854, the recall is 0.31720833333333315, the f1 score is 0.11813853999248491\n",
      "F1 score: 0.11813853999248491\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.8, beta 0, gamma 0.2, cluster number 13\n",
      "The precision is 0.08084285714285484, the recall is 0.3370833333333336, the f1 score is 0.129369808655667\n",
      "F1 score: 0.129369808655667\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.8, beta 0.1, gamma 0.1, cluster number 13\n",
      "The precision is 0.0795722222222214, the recall is 0.32433333333333303, the f1 score is 0.12718643642282967\n",
      "F1 score: 0.12718643642282967\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.8, beta 0.2, gamma 0, cluster number 13\n",
      "The precision is 0.09137380952380807, the recall is 0.31383333333333313, the f1 score is 0.1409999977234546\n",
      "F1 score: 0.1409999977234546\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.9, beta 0, gamma 0.1, cluster number 13\n",
      "The precision is 0.0740999999999984, the recall is 0.32483333333333314, the f1 score is 0.11869245828060322\n",
      "F1 score: 0.11869245828060322\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 0.9, beta 0.1, gamma 0, cluster number 13\n",
      "The precision is 0.09047857142856987, the recall is 0.32058333333333316, the f1 score is 0.14030555327800684\n",
      "F1 score: 0.14030555327800684\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.5, drain depth 3, alpha 1, beta 0, gamma 0, cluster number 13\n",
      "The precision is 0.08976666666666505, the recall is 0.33308333333333284, the f1 score is 0.13994444215825358\n",
      "F1 score: 0.13994444215825358\n",
      "A melhor combinação foi alpha 0, beta 1, gamma 0\n"
     ]
    }
   ],
   "source": [
    "## General parameters \n",
    "\n",
    "import pandas as pd\n",
    "num_executions = 10\n",
    "dataset = \"hdfs\" \n",
    "\n",
    "# Parsing Parameters\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "# Drain Parameters\n",
    "drain_st = 0.5\n",
    "drain_depth = 3\n",
    "# Variable Matrix Parameters\n",
    "alpha = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "beta = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "gamma = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "# Creating Results Dataframe\n",
    "results = pd.DataFrame(columns=['Scenario', 'Drain St', 'Drain Depth', 'Alpha', 'Beta', 'Gamma', 'F1'])\n",
    "\n",
    "# Running Tests for Scenarios A and B\n",
    "for i in range(num_executions):\n",
    "    value = tests_scenario_A(drain_st, drain_depth)\n",
    "    new_row = ['A', drain_st, drain_depth, 0, 0, 0, value]\n",
    "    results.loc[len(results)] = new_row\n",
    "for i in range(num_executions):\n",
    "    value = tests_scenario_B(drain_st, drain_depth)\n",
    "    new_row = ['B', drain_st, drain_depth, 0, 0, 0, value]\n",
    "    results.loc[len(results)] = new_row\n",
    "\n",
    "# Testing different hyperparameters for Scenario C\n",
    "# best_f1 = 0\n",
    "# best_alpha = 0\n",
    "# best_beta = 0\n",
    "# best_gamma = 0\n",
    "# for a in alpha:\n",
    "#     for b in beta:\n",
    "#         for g in gamma:\n",
    "#             if (a+b+g != 1):\n",
    "#                 pass\n",
    "#             else:\n",
    "#                 try:\n",
    "#                     new_f1 = tests_scenario_C(drain_st, drain_depth, a, b, g)\n",
    "#                 except:\n",
    "#                     new_f1 = 0\n",
    "#                 if (new_f1 > best_f1):\n",
    "#                     best_f1 = new_f1\n",
    "#                     best_alpha = a\n",
    "#                     best_beta = b\n",
    "#                     best_gamma = g\n",
    "# print(\"A melhor combinação foi alpha {}, beta {}, gamma {}\".format(best_alpha, best_beta, best_gamma))\n",
    "\n",
    "# Now running num_executions for each scenario, getting all scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Scenario  Drain St  Drain Depth  Alpha  Beta  Gamma    F1\n",
      "0       S4      0.65           13   0.82  0.78   0.68  0.89\n"
     ]
    }
   ],
   "source": [
    "results = pd.DataFrame(columns=['Scenario', 'Drain St', 'Drain Depth', 'Alpha', 'Beta', 'Gamma', 'F1'])\n",
    "# New row data\n",
    "new_row = ['S4', 0.65, 13, 0.82, 0.78, 0.68, 0.89]\n",
    "\n",
    "# Insert the new row\n",
    "results.loc[len(results)] = new_row\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/nanops.py:719\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    718\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(the_sum)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/numpy/core/_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mumr_sum\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# create plot\u001b[39;00m\n\u001b[1;32m     16\u001b[0m sns\u001b[38;5;241m.\u001b[39mbarplot(x \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m'\u001b[39m, hue \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m, data \u001b[38;5;241m=\u001b[39m titanic,\n\u001b[1;32m     17\u001b[0m             palette \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhls\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     18\u001b[0m             order \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmale\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfemale\u001b[39m\u001b[38;5;124m'\u001b[39m],  \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m             ci \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msd\u001b[39m\u001b[38;5;124m'\u001b[39m   \n\u001b[1;32m     23\u001b[0m             )\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtitanic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msex\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(titanic\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mstd()[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msurvived\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     28\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_py_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# create plot\n",
    "sns.barplot(x = 'sex', y = 'survived', hue = 'class', data = titanic,\n",
    "            palette = 'hls',\n",
    "            order = ['male', 'female'],  \n",
    "            capsize = 0.05,             \n",
    "            saturation = 8,             \n",
    "            errcolor = 'gray', errwidth = 2,  \n",
    "            ci = 'sd'   \n",
    "            )\n",
    "\n",
    "print(titanic.groupby(['sex', 'class']).mean()['survived'])\n",
    "print(titanic.groupby(['sex', 'class']).std()['survived'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 5, alpha 0.3, beta 0.3, gamma 0.3, cluster number 13\n",
      "The precision is 0.06899999999999865, the recall is 0.3258333333333343, the f1 score is 0.1134798515794063\n",
      "F1 score: 0.1134798515794063\n",
      "Testing scenario C using joint matrix and BerTopic topic modeling, with drain st 0.3, drain depth 5, alpha 0.3, beta 0.3, gamma 0.3, cluster number 13\n",
      "The precision is 0.06899999999999865, the recall is 0.3258333333333343, the f1 score is 0.1134798515794063\n",
      "F1 score: 0.1134798515794063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1134798515794063"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "tests_scenario_C(0.3, 5, 0.3, 0.3, 0.3)\n",
    "tests_scenario_C(0.3, 5, 0.3, 0.3, 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculating Elbow\n",
    "\n",
    "def clusters_elbow (unified_matrix, dataset):\n",
    "    ## Clusters with cluster_kmedoids and silhouette score\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib\n",
    "\n",
    "    # Finding the optimal number of clusters using Elbow Method\n",
    "    wcss = []\n",
    "    max_clusters = 30\n",
    "    for i in range(1, max_clusters):\n",
    "        kmedoids = KMedoids(n_clusters=i, random_state=42)\n",
    "        kmedoids.fit(unified_matrix)\n",
    "        wcss.append(kmedoids.inertia_)\n",
    "    plt.gca().get_xaxis().get_major_formatter().set_useOffset(False)\n",
    "    plt.plot(range(1, max_clusters), wcss)\n",
    "    plt.title('Elbow Method for {}'.format(dataset))\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()    \n",
    "\n",
    "def cluster_kmedoids_silhouette(unified_matrix, cluster_num):\n",
    "    ## Clusters with cluster_kmedoids and silhouette score\n",
    "    from sklearn.metrics import silhouette_score\n",
    "\n",
    "    clusterer = KMedoids(n_clusters=cluster_num, random_state=42)\n",
    "    #clusterer.fit(unified_matrix)\n",
    "    #print(\"Para {} clusters, o silhouette score é {}\".format(cluster_num,silhouette_score(unified_matrix, clusterer(unified_matrix))))\n",
    "    print(\"Para {} clusters, o silhouette score é {}\".format(cluster_num, silhouette_score(unified_matrix, clusterer.fit_predict(unified_matrix))))\n",
    "\n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "alpha = 0.3\n",
    "beta = 0.4\n",
    "gamma = 0.3\n",
    "vector_df = transform(os.path.basename(logName))\n",
    "distance_matrix = create_distance_matrix(vector_df)\n",
    "variable_matrix = create_variable_matrix()\n",
    "closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "clusters_elbow(joint_matrix, dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster_kmedoids_silhouette(joint_matrix, 20)\n",
    "\n",
    "for i in range (2,30):\n",
    "    cluster_kmedoids_silhouette(joint_matrix, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"Proxifier\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.0\n",
    "alpha_clustering = 1.0\n",
    "leaf_size = 40\n",
    "\n",
    "tests_predefined_rawdata(drain_st, drain_depth, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "tests_generated_rawdata(drain_st, drain_depth, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "tests_generated_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tests_hdbscan(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size):\n",
    "    parameters = (\"Testing generated clustering and precomputed matrix with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, min cluster size {}, min samples {}, cluster selection epsilon {}, alpha {}, and leaf size {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size,min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size))\n",
    "    print(parameters)\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                                alpha, beta, gamma)\n",
    "    clustering = cluster_hdbscan(joint_matrix, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "    topic_summaries = bertopic_new_clustering()\n",
    "    final = calculates_metrics()\n",
    "    return (final)\n",
    "\n",
    "def tests_kmedoids(drain_st, drain_depth, alpha, beta, gamma, cluster_num):\n",
    "    parameters = (\"Testing generated clustering and precomputed matrix with drain st {}, drain depth {}, alpha {}, beta {}, gamma {}, cluster_num {}\".\n",
    "          format(drain_st, drain_depth, alpha, beta, gamma, cluster_num))\n",
    "    print(parameters)\n",
    "    vector_df = transform(os.path.basename(logName))\n",
    "    distance_matrix = create_distance_matrix(vector_df)\n",
    "    variable_matrix = create_variable_matrix()\n",
    "    closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "    joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix,alpha, beta, gamma)\n",
    "    clustering = cluster_kmedoids(joint_matrix, cluster_num)\n",
    "    topic_summaries = bertopic_new_clustering()\n",
    "    final = calculates_metrics()\n",
    "    return (final)\n",
    "\n",
    "## General parameters \n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.0\n",
    "alpha_clustering = 1.0\n",
    "leaf_size = 40\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.3\n",
    "beta = 0.4\n",
    "gamma = 0.3\n",
    "cluster_num = 2\n",
    "\n",
    "#tests_hdbscan(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_kmedoids(drain_st, drain_depth, alpha, beta, gamma, 8)\n",
    "tests_kmedoids(drain_st, drain_depth, alpha, beta, gamma, 20)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth,  0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n",
    "# tests_hdbscan(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_kmedoids(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# print(\"***\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_predefined_rawdata(drain_st, drain_depth, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "print(\"*****\")\n",
    "\n",
    "tests_generated_rawdata(drain_st, drain_depth, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.0\n",
    "alpha_clustering = 1.0\n",
    "leaf_size = 40\n",
    "\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "# tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, 0.9, leaf_size)\n",
    "# tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, 0.8, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, 0.75, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, 0.5, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"spark\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.2\n",
    "beta = 0.1\n",
    "gamma = 0.7\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0\n",
    "alpha_clustering = 1.0\n",
    "leaf_size = 40\n",
    "\n",
    "tests_predefined_rawdata(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_predefined_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_rawdata(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, 2, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, 10, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, 20, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_samples, 2, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_samples, 10, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_samples, 20, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, 0.5, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, 0.25, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, 1, alpha_clustering, leaf_size)\n",
    "\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.1, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.5, leaf_size)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.75, leaf_size)\n",
    "\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.1, 2)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.5, 10)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.75, 20)\n",
    "\n",
    "tests_generated_precomputed(0.2, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(0.7, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(0.1, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "alpha_clustering = 0.25\n",
    "leaf_size = 5\n",
    "\n",
    "print (\"teste com generated, raw data\")\n",
    "tests_generated_rawdata(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_rawdata(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, 20)\n",
    "tests_generated_rawdata(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, 40)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.75, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.5, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.1, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.75, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 20)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 40)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 40)\n",
    "tests_generated_precomputed(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, 0.25, 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_generated_precomputed(drain_st, 2, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, 7, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "tests_generated_precomputed(drain_st, 10, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "alpha_clustering = 1.0\n",
    "leaf_size = 10\n",
    "\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5, alpha_clustering, leaf_size)\n",
    "\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon, alpha_clustering, leaf_size)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5, alpha_clustering, leaf_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.3, 0.4, 0.3, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)\n",
    "\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## General parameters \n",
    "\n",
    "dataset = \"Zookeeper\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.3, 0.4, 0.3, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "previous_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)\n",
    "\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.3, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(0.1, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 3, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, 5, alpha, beta, gamma, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 1, 0, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0, 1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.2, 0.1, 0.7, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.9, 0.1, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.5, 0.5, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, 0.1, 0.9, 0, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, 10, min_samples, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, 10, cluster_selection_epsilon)\n",
    "new_clustering(drain_st, drain_depth, alpha, beta, gamma, min_cluster_size, min_samples, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "lista_f = []\n",
    "\n",
    "with open('ground_truths/bgl_lines.txt', 'r') as fd:\n",
    "    for row in fd:\n",
    "        lista_f.append(row)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, top_n_words=5)\n",
    "topics, probabilities = topic_model.fit_transform(lista_f)\n",
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "lista_f = []\n",
    "\n",
    "with open('ground_truths/bgl_lines.txt', 'r') as fd:\n",
    "    for row in fd:\n",
    "        lista_f.append(row)\n",
    "\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model, top_n_words=5)\n",
    "topics, probabilities = topic_model.fit_transform(lista_f)\n",
    "topic_model.get_topic_info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
