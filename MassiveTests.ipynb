{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0 - Parameters and Libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import DrainMethod\n",
    "import contextlib\n",
    "import hdbscan\n",
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "log_file = os.path.basename(logName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Auxiliary methods\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    return vectors_tfidf\n",
    "\n",
    "def creates_lists(clusterer):\n",
    "    ## General Parameters\n",
    "\n",
    "    cluster_idxs = []\n",
    "    cluster_lines = []\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "    # Creates blank lists\n",
    "    for elem in range (clusterer.labels_.max()+1):\n",
    "        cluster_idxs.append([])\n",
    "        cluster_lines.append([])\n",
    "\n",
    "    # Populate the lists with cluster elements\n",
    "    for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "        if elem != -1:\n",
    "            cluster_idxs[elem].append(idx[0])\n",
    "            cluster_lines[elem].append(elem_df[idx[0]])\n",
    "        \n",
    "    return (cluster_idxs, cluster_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Main methods\n",
    "\n",
    "# Parse logs using Drain\n",
    "\n",
    "def parse_logs(st=0.5, depth=5):\n",
    "    st = st # Drain similarity threshold\n",
    "    depth = depth # Max depth of the parsing tree\n",
    "\n",
    "    ## Code\n",
    "    parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "    parser.parse(log_file)\n",
    "\n",
    "    parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   \n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "# Creates distance matrix, using Euclidean distance\n",
    "def create_distance_matrix(vector_df):\n",
    "    # Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "    tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "    #Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(tfidf_distance)\n",
    "    max_val = np.max(tfidf_distance)\n",
    "    tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "    return (tfidf_distance)\n",
    "\n",
    "# Creates variable matrix, using Jaccard distance\n",
    "def create_variable_matrix():\n",
    "    ## General Parameters\n",
    "    output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "    output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "    ## Code\n",
    "    # Reads parameters list\n",
    "    full_df = pd.read_csv(output_csv)\n",
    "    var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "    # Breaks the string into lists\n",
    "    for i, line in var_df.items():\n",
    "        var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "    # Transforms variable list to variable sparse matrix\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    var_df = mlb.fit_transform(var_df)\n",
    "    var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "    return (var_distance)\n",
    "\n",
    "def creates_closeness_matrix(tfidf_distance):\n",
    "    # Creates Count Matrix using line numbers from log lines as the counter\n",
    "    count_list = []\n",
    "    n = len(tfidf_distance)\n",
    "    count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "            count_list.append(i)\n",
    "\n",
    "    # Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "    count_array = np.array(count_list)\n",
    "    for x in count_array:\n",
    "        for y in count_array:\n",
    "            count_distance[x,y] = abs(x-y)\n",
    "    # Normalizes Distance Matrix with Min-Max\n",
    "    min_val = np.min(count_distance)\n",
    "    max_val = np.max(count_distance)\n",
    "    count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "    return (count_distance)\n",
    "\n",
    "def saves_matrices(distance_mat, variable_mat, closeness_mat):\n",
    "    np.save(\"tfidf_distance_\" + logName + \".csv\", distance_mat)\n",
    "    np.save(\"var_distance_\" + logName + \".csv\", variable_mat)\n",
    "    np.save(\"count_distance_\" + logName + \".csv\", closeness_mat)\n",
    "\n",
    "def loads_matrices():\n",
    "    tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "    count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "    var_distance = np.load(\"var_distance_\" + logName + \".csv\") \n",
    "    return (tfidf_distance, count_distance, var_distance)\n",
    "\n",
    "def joins_matrices(tfidf_distance, var_distance, count_distance, alpha, beta, gamma):\n",
    "\n",
    "    if alpha+beta+gamma > 1:\n",
    "        raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "    # New matrices, corrected by the weights\n",
    "    tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "    var_distance_wtd = np.dot(beta, var_distance)\n",
    "    count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "    # Sums remaining matrices\n",
    "    unified_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)\n",
    "    return (unified_matrix)\n",
    "\n",
    "def cluster_hdbscan(unified_matrix, cluster_size, mn_samples, cluster_selection_epsilon):\n",
    "    ## Clusters with HDBSCAN\n",
    "    clusterer = hdbscan.HDBSCAN(min_cluster_size=cluster_size,min_samples=mn_samples,metric='precomputed',\n",
    "                                cluster_selection_epsilon=cluster_selection_epsilon, alpha=1.0, leaf_size=40, \n",
    "                                allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                                gen_min_span_tree=True)\n",
    "\n",
    "    clusterer.fit(unified_matrix)\n",
    "\n",
    "    ## Checks number of outliers\n",
    "    cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "\n",
    "    #print(\"O número de outliers é {}\".format(cont))\n",
    "    #print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))\n",
    "    return (clusterer)\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        umap_model = UMAP(init='random')\n",
    "        cluster_model = KMedoids(n_clusters = 1)\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "        topic_model = BERTopic(embedding_model=\"all-MiniLM-L6-v2\", hdbscan_model=cluster_model, \n",
    "                               vectorizer_model=vectorizer_model, umap_model=umap_model, \n",
    "                               top_n_words=10,verbose=True)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return (summary)\n",
    "\n",
    "def bertopic_previous_clustering(clusterer):\n",
    "    cluster_idxs, cluster_lines = creates_lists(clusterer)\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    ## Creates list of boolean values, representing summarized topics\n",
    "    for idx in range(clusterer.labels_.max()):\n",
    "        cluster_topic.append(None)\n",
    "\n",
    "    for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "        ## For each cluster, maps topics, and defines them as the summary\n",
    "        if (cluster_topic[elem-1] == None):\n",
    "            summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "            cluster_topic[elem-1] = summary\n",
    "        \n",
    "        if elem == -1:\n",
    "            topic_summaries.append(\"\")\n",
    "        else:\n",
    "            topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "    return topic_summaries\n",
    "\n",
    "def create_new_bertopic_model():\n",
    "    lines = []\n",
    "    with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "        for line in line_file:\n",
    "            lines.append(line)\n",
    "\n",
    "    umap_model = UMAP(init='random')\n",
    "    vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "    embedding_model = \"all-mpnet-base-v2\"\n",
    "    topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, \n",
    "                        umap_model=umap_model, top_n_words=10, verbose=True)\n",
    "    topics, probs = topic_model.fit_transform(lines)\n",
    "    return (topic_model)\n",
    "\n",
    "# def creates_lists_bertopic(topic_model):\n",
    "\n",
    "#     ## General Parameters\n",
    "#     cluster_idxs = []\n",
    "#     cluster_lines = []\n",
    "#     output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "#     output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "#     ## Code\n",
    "\n",
    "#     # Reads parameters list\n",
    "#     full_df = pd.read_csv(output_csv)\n",
    "#     elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "#     # Creates blank lists\n",
    "#     #for elem in range (clusterer.labels_.max()+1):\n",
    "#     for elem in range (max(topic_model.topics_)+1):\n",
    "#         cluster_idxs.append([])\n",
    "#         cluster_lines.append([])\n",
    "\n",
    "#     # Populate the lists with cluster elements\n",
    "#     for idx, elem in np.ndenumerate(topic_model.topics_):\n",
    "#         if elem != -1:\n",
    "#             cluster_idxs[elem].append(idx[0])\n",
    "#             cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "#     #print(cluster_lines[10][9])\n",
    "\n",
    "def bertopic_new_clustering():\n",
    "\n",
    "    topic_model = create_new_bertopic_model()\n",
    "    cluster_topic = []\n",
    "    topic_summaries = []\n",
    "\n",
    "    # ## Creates list of boolean values, representing summarized topics\n",
    "    # for idx in range(clusterer.labels_.max()):\n",
    "    #     cluster_topic.append(None)\n",
    "\n",
    "    for elem in topic_model.topics_:\n",
    "        \n",
    "        line_topic = topic_model.get_topic(elem)\n",
    "        words = [i[0] for i in line_topic]\n",
    "        summary = ' '.join(words)\n",
    "        topic_summaries.append(summary)\n",
    "\n",
    "    ## Writes external file with created topics\n",
    "    with open (\"ground_truths/\" + dataset + \"_bert_topics_global.txt\", \"w\") as f:\n",
    "        for line in topic_summaries:\n",
    "            f.write(f\"{line}\\n\")\n",
    "\n",
    "def calculates_metrics(target_file):\n",
    "    \n",
    "    from rouge import Rouge \n",
    "    rouge = Rouge()\n",
    "\n",
    "    count_precision = 0\n",
    "    count_recall = 0\n",
    "    count_f1 = 0\n",
    "    total_lines = 2000\n",
    "\n",
    "    # Opens external files with ground truth summaries and created topics\n",
    "    with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "        open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "        for line_summary, line_topic in zip(summaries, topics):\n",
    "            line_summary = line_summary[:-2]\n",
    "            line_summaries = line_summary.split(\";\")\n",
    "\n",
    "            for summary in line_summaries:\n",
    "                current_precision = 0\n",
    "                current_recall = 0\n",
    "                current_f1 = 0\n",
    "                metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']    \n",
    "                ## If the summary improves the f1 score, saves its metrics\n",
    "                if (current_f1 < metrics['f']):\n",
    "                    current_precision = metrics['p']\n",
    "                    current_recall = metrics['r']\n",
    "                    current_f1 = metrics['f']\n",
    "            \n",
    "            count_precision += current_precision\n",
    "            count_recall += current_recall        \n",
    "            count_f1 += current_f1\n",
    "\n",
    "    final_precision = count_precision/total_lines\n",
    "    final_recall = count_recall/total_lines\n",
    "    final_f1 = count_f1/total_lines\n",
    "\n",
    "    print(\"The precision is {}\".format(final_precision))\n",
    "    print(\"The recall is {}\".format(final_recall))\n",
    "    print(\"The f1 score is {}\".format(final_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing logs...\n",
      "Parsing file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker-OLD\\LineTracker\\ground_truths\\bgl_lines.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 16993.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.586319]\n",
      "Transforming data using TFIDF...\n",
      "Creating distance matrix...\n",
      "Creating variable matrix...\n",
      "Creating closeness matrix...\n",
      "Joining matrices...\n",
      "Clustering using HDBSCAN...\n",
      "<class 'int'>\n",
      "<class 'int'>\n",
      "0.75\n",
      "Creating validation file, using BerTopic on previous clustering...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c2b9864a59b425b8140de463d9729db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:32,674 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:36,590 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:36,603 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57a3d27763c8470c98bcfccaebda904d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:38,274 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:41,071 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:41,075 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ffba100c094f0ba46207c6c022d1ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:44,749 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:46,633 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:46,644 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a280da897fdf4b9dbd7b7b5cc6ee3306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:47,022 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:48,654 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:48,654 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f827bbc5f0941a68f59c083d6eae71f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:49,389 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:51,199 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:51,202 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b706a28100ad47bc98f272bcb127b325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:51,940 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:54,167 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:54,175 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa40bd5619c94ea38d4a5cf23988abe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:54,579 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:56,264 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:56,267 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "804f1427fa4d4b4f87617a102074c438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:57,078 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:49:58,790 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:49:58,793 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cc8ab6a167745fda646c005b1376498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:49:59,192 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:00,824 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:00,824 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c770b42f9ba47d692d792b04a95fa02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:01,268 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:03,192 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:03,195 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f303e8d8949c4798aa6a6ee7f203deaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:03,495 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:05,009 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:05,009 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ce7c45f289f42e382c3a691bb807bda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:05,424 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:06,993 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:06,995 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185b305433c1417f9e46c5912fd19b30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:07,395 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:09,001 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:09,011 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c1dc3e79367487f8f5e657afa79b0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:09,809 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:11,897 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:11,897 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb24db67556f4ff1af5e7dc94d0c945f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:13,535 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:15,829 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:15,835 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73ed9ebe0db241d5ade7ff9287610228",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:16,204 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:17,928 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:17,928 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd451c4142a843d2bfc1450d444e9565",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:18,570 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:20,865 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:20,870 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "137570519f524b74840236f50eb6ad17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 18:50:22,174 - BERTopic - Transformed documents to Embeddings\n",
      "2024-06-06 18:50:24,924 - BERTopic - Reduced dimensionality\n",
      "2024-06-06 18:50:24,924 - BERTopic - Clustered reduced embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing external file with topic summaries...\n",
      "Calculating metrics...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ground_truths/bglground_truths/bgl_bert_topics_tests.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 39\u001b[0m\n\u001b[0;32m     37\u001b[0m             f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mline\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalculating metrics...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 39\u001b[0m calculates_metrics(target_file)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDone!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[13], line 233\u001b[0m, in \u001b[0;36mcalculates_metrics\u001b[1;34m(target_file)\u001b[0m\n\u001b[0;32m    229\u001b[0m total_lines \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2000\u001b[39m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Opens external files with ground truth summaries and created topics\u001b[39;00m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truths/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_summaries.txt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m summaries, \\\n\u001b[1;32m--> 233\u001b[0m     \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mground_truths/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m dataset \u001b[38;5;241m+\u001b[39m target_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m topics:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line_summary, line_topic \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(summaries, topics):\n\u001b[0;32m    235\u001b[0m         line_summary \u001b[38;5;241m=\u001b[39m line_summary[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ground_truths/bglground_truths/bgl_bert_topics_tests.txt'"
     ]
    }
   ],
   "source": [
    "## Pipeline of methods\n",
    "\n",
    "drain_st = 0.5\n",
    "drain_depth = 5\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "min_cluster_size = 5\n",
    "min_samples = 5\n",
    "cluster_selection_epsilon = 0.75\n",
    "\n",
    "print(\"Parsing logs...\")\n",
    "parse_logs(drain_st, drain_depth)\n",
    "print(\"Transforming data using TFIDF...\")\n",
    "vector_df = transform(os.path.basename(logName))\n",
    "print(\"Creating distance matrix...\")\n",
    "distance_matrix = create_distance_matrix(vector_df)\n",
    "print(\"Creating variable matrix...\")\n",
    "variable_matrix = create_variable_matrix()\n",
    "print(\"Creating closeness matrix...\")\n",
    "closeness_matrix = creates_closeness_matrix(distance_matrix)\n",
    "#print(\"Saving matrices...\")\n",
    "#saves_matrices(distance_matrix, variable_matrix, closeness_matrix)\n",
    "#print(\"Loading matrices...\")\n",
    "#distance_matrix, variable_matrix, closeness_matrix = loads_matrices()\n",
    "print(\"Joining matrices...\")\n",
    "joint_matrix = joins_matrices(distance_matrix, variable_matrix, closeness_matrix, \n",
    "                              alpha, beta, gamma)\n",
    "print(\"Clustering using HDBSCAN...\")\n",
    "clustering = cluster_hdbscan(joint_matrix, min_cluster_size, min_samples, cluster_selection_epsilon)\n",
    "print(\"Creating validation file, using BerTopic on previous clustering...\")\n",
    "topic_summaries = bertopic_previous_clustering(clustering)\n",
    "print(\"Writing external file with topic summaries...\")\n",
    "target_file = \"ground_truths/\" + dataset + \"_bert_topics_tests.txt\"\n",
    "with open (target_file, \"w\") as f:\n",
    "        for line in topic_summaries:\n",
    "            f.write(f\"{line}\\n\")\n",
    "print(\"Calculating metrics...\")\n",
    "calculates_metrics(target_file)\n",
    "print(\"Done!\")\n",
    "#print(\"Creating validation file, using BerTopic on new clustering...\")\n",
    "#bertopic_new_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluates Accuracy\n",
    "\n",
    "## Step 12 - Calculates average recall, precision and f1\n",
    "\n",
    "## Initial tests with rouge\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "count_precision = 0\n",
    "count_recall = 0\n",
    "count_f1 = 0\n",
    "total_lines = 2000\n",
    "#target_file = \"_luhn.txt\"\n",
    "#target_file = \"_lsa.txt\"\n",
    "#target_file = \"_local_topics.txt\"\n",
    "#target_file = \"_global_topics.txt\"\n",
    "#target_file = \"_lda_topics.txt\"\n",
    "#target_file = \"_lexrank.txt\"\n",
    "#target_file = \"_textrank.txt\"\n",
    "#target_file = \"_bert_topics_local.txt\"\n",
    "target_file = \"_bert_topics_global.txt\"\n",
    "\n",
    "# Opens external files with ground truth summaries and created topics\n",
    "with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "     open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "    for line_summary, line_topic in zip(summaries, topics):\n",
    "        line_summary = line_summary[:-2]\n",
    "        line_summaries = line_summary.split(\";\")\n",
    "\n",
    "        for summary in line_summaries:\n",
    "            current_precision = 0\n",
    "            current_recall = 0\n",
    "            current_f1 = 0\n",
    "            metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']    \n",
    "            ## If the summary improves the f1 score, saves its metrics\n",
    "            if (current_f1 < metrics['f']):\n",
    "                current_precision = metrics['p']\n",
    "                current_recall = metrics['r']\n",
    "                current_f1 = metrics['f']\n",
    "        \n",
    "        count_precision += current_precision\n",
    "        count_recall += current_recall        \n",
    "        count_f1 += current_f1\n",
    "\n",
    "final_precision = count_precision/total_lines\n",
    "final_recall = count_recall/total_lines\n",
    "final_f1 = count_f1/total_lines\n",
    "\n",
    "print(final_precision)\n",
    "print(final_recall)\n",
    "print(final_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
