{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0 - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "/home/vbertalan/Downloads/Projetos/LineTracker/LineTracker/ground_truths/\n",
      "Parsing file: /home/vbertalan/Downloads/Projetos/LineTracker/LineTracker/ground_truths/bgl_lines.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 18349.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.239320]\n"
     ]
    }
   ],
   "source": [
    "## Step 1 - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: /home/vbertalan/Downloads/Projetos/LineTracker/LineTracker/ground_truths/bgl_lines.txt\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 2 - Vector Creation Using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    print(type(vectors_tfidf))\n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix parseada de variaveis tem o formato (2000, 1395)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 3 - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variable list to variable sparse matrix\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "var_df = mlb.fit_transform(var_df)\n",
    "print (\"A matrix parseada de variaveis tem o formato {}\".format(var_df.shape))\n",
    "print(type(var_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de embeddings são (2000, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbertalan/.local/lib/python3.10/site-packages/sklearn/metrics/pairwise.py:2317: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de variáveis são (2000, 2000)\n",
      "As dimensões da matriz de contadores são (2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Creates distance matrix \n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "import numpy as np\n",
    "\n",
    "# Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "#Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(tfidf_distance)\n",
    "max_val = np.max(tfidf_distance)\n",
    "tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de embeddings são {}\".format(tfidf_distance.shape))\n",
    "\n",
    "# Using Jaccard Distance between the rows of the Variable Matrix\n",
    "var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "print(\"As dimensões da matriz de variáveis são {}\".format(var_distance.shape))\n",
    "\n",
    "# Creates Count Matrix using line numbers from log lines as the counter\n",
    "count_list = []\n",
    "n = len(tfidf_distance)\n",
    "count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "for i in range(n):\n",
    "        count_list.append(i)\n",
    "\n",
    "# Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "count_array = np.array(count_list)\n",
    "for x in count_array:\n",
    "  for y in count_array:\n",
    "    count_distance[x,y] = abs(x-y)\n",
    "# Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(count_distance)\n",
    "max_val = np.max(count_distance)\n",
    "count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de contadores são {}\".format(count_distance.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving matrices\n",
    "\n",
    "print(type(tfidf_distance))\n",
    "np.save(\"tfidf_distance_\" + logName + \".csv\", tfidf_distance)\n",
    "print(type(var_distance))\n",
    "np.save(\"var_distance_\" + logName + \".csv\", var_distance)\n",
    "print(type(count_distance))\n",
    "np.save(\"count_distance_\" + logName + \".csv\", count_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads matrices\n",
    "\n",
    "tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "var_distance = np.load(\"var_distance_\" + logName + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Using alpha to define the weight of the TFIDF Matrix,  \n",
    "# Beta to define the weight of the Variable Matrix,\n",
    "# and Gamma to define the weight of the Count Matrix\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "\n",
    "if alpha+beta+gamma > 1:\n",
    "   raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "# New matrices, corrected by the weights\n",
    "tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "var_distance_wtd = np.dot(beta, var_distance)\n",
    "count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "# Sums remaining matrices\n",
    "distance_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O numero de clusters e 18\n",
      "Os clusters de cada elemento são [16 16 16 ... 16 16 16]\n",
      "O número de outliers é 9\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 6 - Clustering with HDBScan Using Pre-defined Distance Matrix\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "\n",
    "# min_cluster_size:int, optional (default=5)\n",
    "# The minimum size of clusters; single linkage splits that contain fewer points than this will \n",
    "# be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "# min_samples:int, optional (default=None)\n",
    "# The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "\n",
    "# p :int, optional (default=None)\n",
    "# p value to use if using the minkowski metric.\n",
    "\n",
    "# alpha :float, optional (default=1.0)\n",
    "# A distance scaling parameter as used in robust single linkage. See [3] for more information.\n",
    "\n",
    "# cluster_selection_epsilon: float, optional (default=0.0)\n",
    "# A distance threshold. Clusters below this value will be merged.\n",
    "# See [5] for more information.\n",
    "\n",
    "# algorithm :string, optional (default=’best’)\n",
    "# Exactly which algorithm to use; hdbscan has variants specialised for different characteristics \n",
    "# of the data. By default this is set to best which chooses the “best” algorithm given the nature \n",
    "# of the data. You can force other options if you believe you know better. Options are: 'best',\n",
    "# 'generic', 'prims_kdtree', 'prims_balltree', 'boruvka_kdtree' and 'boruvka_balltree'\n",
    "\n",
    "# leaf_size: int, optional (default=40)\n",
    "# If using a space tree algorithm (kdtree, or balltree) the number of points ina leaf node of the tree. \n",
    "# This does not alter the resulting clustering, but may have an effect on the runtime of the algorithm.\n",
    "\n",
    "# cluster_selection_method :string, optional (default=’eom’)\n",
    "# The method used to select clusters from the condensed tree. The standard approach for HDBSCAN is \n",
    "# to use an Excess of Mass algorithm to find the most persistent clusters. Alternatively you can \n",
    "# instead select the clusters at the leaves of the tree – this provides the most fine grained and \n",
    "# homogeneous clusters. Options are: 'eom' and 'leaf'\n",
    "\n",
    "# allow_single_cluster :bool, optional (default=False)\n",
    "# By default HDBSCAN will not produce a single cluster, setting this to True will override this \n",
    "# and allow single cluster results in the case that you feel this is a valid result for your dataset.\n",
    "\n",
    "## Clusters with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=None,metric='precomputed',\n",
    "                            cluster_selection_epsilon=0.75, alpha=1.0, leaf_size=40, \n",
    "                            allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                            gen_min_span_tree=True)\n",
    "\n",
    "\n",
    "clusterer.fit(distance_matrix)\n",
    "\n",
    "print (\"O numero de clusters e {}\".format(clusterer.labels_.max()))\n",
    "print (\"Os clusters de cada elemento são {}\".format(clusterer.labels_))\n",
    "\n",
    "## Checks number of outliers\n",
    "cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6.1 - Uses K-Means for clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(n_clusters=50, random_state=0, n_init=10)\n",
    "clusterer.fit(distance_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento sao {}\".format(cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6.2 - Uses K-Medoids for clustering\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "clusterer = KMedoids(n_clusters=50, random_state=0, metric='precomputed')\n",
    "clusterer.fit(distance_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento são {}\".format(cluster_labels))\n",
    "print (\"Os centros dos clusters são {} \".format(clusterer.cluster_centers_))\n",
    "print (\"A inércia dos clusters é {}\".format(clusterer.inertia_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número de outliers é 9\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 7 - Checks number of outliers\n",
    "\n",
    "cont = 0\n",
    "\n",
    "for elem in clusterer.labels_:\n",
    "   if (elem == -1):\n",
    "      cont += 1\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do cluster 0 é 60\n",
      "O tamanho do cluster 1 é 8\n",
      "O tamanho do cluster 2 é 120\n",
      "O tamanho do cluster 3 é 36\n",
      "O tamanho do cluster 4 é 87\n",
      "O tamanho do cluster 5 é 40\n",
      "O tamanho do cluster 6 é 29\n",
      "O tamanho do cluster 7 é 10\n",
      "O tamanho do cluster 8 é 20\n",
      "O tamanho do cluster 9 é 61\n",
      "O tamanho do cluster 10 é 20\n",
      "O tamanho do cluster 11 é 30\n",
      "O tamanho do cluster 12 é 66\n",
      "O tamanho do cluster 13 é 40\n",
      "O tamanho do cluster 14 é 72\n",
      "O tamanho do cluster 15 é 6\n",
      "O tamanho do cluster 16 é 934\n",
      "O tamanho do cluster 17 é 332\n",
      "O tamanho do cluster 18 é 20\n"
     ]
    }
   ],
   "source": [
    "## Step 8 - Creates a list of lists representing the clusters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "# Creates blank lists\n",
    "for elem in range (clusterer.labels_.max()+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "for i in range(len(cluster_idxs)):\n",
    "   print(\"O tamanho do cluster {} é {}\".format(i,len(cluster_idxs[i])))\n",
    "\n",
    "#print(cluster_lines[10][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_idxs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9 - Eliminates stopwords on each cluster\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "\n",
    "# Parameters\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['teste'])\n",
    "\n",
    "# Converts sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Removes stopwords from each sentence\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10 - Finds topics of a given cluster using LDA\n",
    "\n",
    "# Finds topic of a given cluster, defining the number of topics\n",
    "def find_topics(cluster_list, cluster_number, num_topics):\n",
    "    # Converts to words\n",
    "    data_words = list(sent_to_words(cluster_list[cluster_number]))\n",
    "    # Removes stop words\n",
    "    #data_words = remove_stopwords(data_words)\n",
    "    # Creates dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Creates corpora\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    # Builds LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "    return lda_model\n",
    "\n",
    "# topics = find_topics(cluster_lines, 9, 1)\n",
    "\n",
    "# # Gets word topics\n",
    "# x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "# topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# #Below Code Prints Only Words \n",
    "# for topic,words in topics_words:\n",
    "#     a =  \" \".join(words)\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vbertalan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "## Step 10-A - Find topic on a given cluster using BerTopic\n",
    "\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        umap_model = UMAP(init='random')\n",
    "        cluster_model = KMedoids(n_clusters = 1)\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "        embedding_model = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\")      \n",
    "        topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=cluster_model, \n",
    "                               vectorizer_model=vectorizer_model, umap_model=umap_model, top_n_words=10)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 11 - Builds new file with topic modeling summaries - LDA\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "## Creates list of boolean values, representing summarized topics\n",
    "for idx in range(clusterer.labels_.max()):\n",
    "    cluster_topic.append(None)\n",
    "\n",
    "for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "    ## For each cluster, maps topics, and defines them as the summary\n",
    "    if (cluster_topic[elem-1] == None):\n",
    "        topics = find_topics(cluster_lines, elem-1, 1)\n",
    "        x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "        topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "        for topic,words in topics_words:\n",
    "            summary = \" \".join(words)\n",
    "        cluster_topic[elem-1] = summary\n",
    "    \n",
    "    if elem == -1:\n",
    "        topic_summaries.append(\"\")\n",
    "    else:\n",
    "        topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_global_topics.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name sentence-transformers/all-MiniLM-L6-v2. Creating a new one with mean pooling.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "sentence-transformers/all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:286\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/lib/python3/dist-packages/requests/models.py:943\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m--> 943\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:385\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 385\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1368\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1367\u001b[0m     \u001b[38;5;66;03m# Repo not found => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1368\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1369\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1370\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1238\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout, endpoint)\u001b[0m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1238\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1239\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1244\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlibrary_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlibrary_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1245\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1246\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Cache the non-existence of the file and raise\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1631\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent)\u001b[0m\n\u001b[1;32m   1630\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1631\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1633\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1635\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1637\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1639\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1640\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:385\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 385\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    387\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    388\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    392\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:409\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    408\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 409\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:323\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    315\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    316\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    317\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 323\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-665e2da1-0ff383e1402a8ed1740d00c9;82dd6297-4f81-41a8-b40d-b6b3e6f09a4c)\n\nRepository Not Found for url: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nUser Access Token \"llama\" is expired",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clusterer\u001b[38;5;241m.\u001b[39mlabels_):\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m## For each cluster, maps topics, and defines them as the summary\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (cluster_topic[elem\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 14\u001b[0m         summary \u001b[38;5;241m=\u001b[39m \u001b[43mfind_topics_bertopic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_lines\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43melem\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m         cluster_topic[elem\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m summary\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m, in \u001b[0;36mfind_topics_bertopic\u001b[0;34m(cluster_list, cluster_number, num_topics)\u001b[0m\n\u001b[1;32m     12\u001b[0m cluster_model \u001b[38;5;241m=\u001b[39m KMedoids(n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m vectorizer_model \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall-MiniLM-L6-v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m        \n\u001b[1;32m     15\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(embedding_model\u001b[38;5;241m=\u001b[39membedding_model, hdbscan_model\u001b[38;5;241m=\u001b[39mcluster_model, \n\u001b[1;32m     16\u001b[0m                        vectorizer_model\u001b[38;5;241m=\u001b[39mvectorizer_model, umap_model\u001b[38;5;241m=\u001b[39mumap_model, top_n_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#Applies BertTopic\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:298\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    286\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(\n\u001b[1;32m    287\u001b[0m             model_name_or_path,\n\u001b[1;32m    288\u001b[0m             token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    295\u001b[0m             config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    296\u001b[0m         )\n\u001b[1;32m    297\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m         modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_auto_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modules \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modules, OrderedDict):\n\u001b[1;32m    311\u001b[0m     modules \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;28mstr\u001b[39m(idx), module) \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(modules)])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1312\u001b[0m, in \u001b[0;36mSentenceTransformer._load_auto_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1309\u001b[0m tokenizer_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m tokenizer_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mtokenizer_kwargs}\n\u001b[1;32m   1310\u001b[0m config_kwargs \u001b[38;5;241m=\u001b[39m shared_kwargs \u001b[38;5;28;01mif\u001b[39;00m config_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mshared_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_kwargs}\n\u001b[0;32m-> 1312\u001b[0m transformer_model \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1319\u001b[0m pooling_model \u001b[38;5;241m=\u001b[39m Pooling(transformer_model\u001b[38;5;241m.\u001b[39mget_word_embedding_dimension(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_card_data\u001b[38;5;241m.\u001b[39mset_base_model(model_name_or_path, revision\u001b[38;5;241m=\u001b[39mrevision)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:52\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_args \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     50\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 52\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mconfig_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1100\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1097\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1098\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1100\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1102\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:634\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    633\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    636\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME)\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:406\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    404\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    407\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    409\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    410\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    411\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    412\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RevisionNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    414\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    415\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    416\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    417\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: sentence-transformers/all-MiniLM-L6-v2 is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "## Step 11-A - Builds new file with topic modeling summaries - Bertopic\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "## Creates list of boolean values, representing summarized topics\n",
    "for idx in range(clusterer.labels_.max()):\n",
    "    cluster_topic.append(None)\n",
    "\n",
    "for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "    ## For each cluster, maps topics, and defines them as the summary\n",
    "    if (cluster_topic[elem-1] == None):\n",
    "        summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "        cluster_topic[elem-1] = summary\n",
    "    \n",
    "    if elem == -1:\n",
    "        topic_summaries.append(\"\")\n",
    "    else:\n",
    "        topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics_local.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 - Builds file with clustering from BerTopic Local (Using pre-defined clustering)\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "    for line in line_file:\n",
    "        lines.append(line)\n",
    "\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "umap_model = UMAP(init='random')\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")        \n",
    "topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, \n",
    "                       umap_model=umap_model, top_n_words=10)\n",
    "topics, probs = topic_model.fit_transform(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 65, 65, 27, 65, 65, 27, 65, 65, 65, -1, 65, -1, -1, 65, 27, 65, 27, 65, 27, 65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 21, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 21, 66, 21, 66, 35, 21, 35, 21, 35, 21, 35, 21, 35, 21, 21, 35, 21, 35, 35, 21, 35, 21, 35, 21, 4, 13, 4, 13, 4, 13, 4, 13, 4, 13, 4, 13, 13, 4, 13, 4, 13, 4, 13, 4, 63, 13, 63, 63, 13, 63, 13, 63, 13, 13, 63, 13, 63, 63, 13, 63, 13, 63, 13, 63, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 12, 17, 59, 12, 17, 19, 19, 32, 32, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 56, 2, 56, 56, 56, 56, 56, 56, 56, 2, 56, 56, 56, 56, 56, 56, 56, 56, 56, 12, 17, 17, 17, 19, 59, 59, 19, 19, 12, 19, 59, 19, 19, 17, 12, 17, 19, 12, 19, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 18, 22, 22, 14, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 28, 2, 28, 28, 28, 2, 28, 2, 28, 2, 2, 2, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 33, 33, 33, 14, 14, 33, 33, 14, 33, 33, 33, 14, 33, 33, 14, 14, 14, 14, 14, 33, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 23, 23, 23, 23, 23, 23, 23, 23, 23, 57, 57, 3, 3, 3, 3, 3, 3, 3, 57, 3, 57, 57, 3, 3, 3, 3, 3, 3, 3, 57, 3, 3, 3, 3, 3, 57, 57, 57, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 29, 22, 29, 18, 18, 18, 22, 18, 22, 22, 29, 29, 22, 29, 29, 18, 18, 18, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 12, 12, 17, 12, 17, 12, 12, 12, 12, 17, 17, 12, 17, 17, 12, 12, 12, 12, 12, 17, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 55, 63, 55, 55, 55, 34, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 61, 61, 61, 62, 62, 62, 62, 61, 62, 62, 62, 62, 62, 62, 62, 61, 62, 61, 61, 62, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 8, 58, 20, 58, 20, 22, 20, 19, 22, 20, 19, 60, 20, 19, 12, 17, 22, 20, 58, 63, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 17, 17, 12, 12, 12, 12, 12, 12, 12, 17, 12, 17, 17, 17, 12, 12, 17, 17, 17, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 57, 57, 57, 57, 57, 57, 57, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 64, 64, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 17, 17, 17, 12, 12, 17, 17, 19, 19, 59, 12, 12, 17, 17, 19, 12, 17, 19, 12, 17, 22, 20, 60, 20, 60, 20, 58, 20, 25, 20, 60, 60, 20, 20, 0, 58, 20, 25, 20, 60, 26, 26, 26, 26, 26, 26, 66, 60, 20, 60, 60, 20, 20, 25, 20, 22, 20, 58, 20, 22, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 30, 30, 30, 19, 19, 19, 19, 19, 12, 12, 19, 19, 19, 19, 19, 19, 12, 19, 19, 19, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 25, 20, 60, 20, 25, 20, 25, 20, 25, 20, 60, 20, 60, 20, 60, 20, 60, 20, 60, 20, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 22, 29, 18, 18, 18, 18, 22, 22, 22, 29, 22, 29, 29, 29, 18, 18, 18, 18, 18, 18, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 61, 16, 61, 16, 15, 16, 15, 16, 61, 61, 15, 15, 16, 15, 16, 15, 15, 16, 15, 7, 7, 31, 24, 7, 31, 31, 24, 24, 24, 7, 31, 31, 7, 24, 7, 7, 7, 7, 24, 7, 7, 24, 24, 24, 24, 31, 24, 7, 24, 24, 7, 7, 7, 7, 31, 31, 7, 24, 7, 7, 31, 24, 7, 7, 7, 31, 24, 7, 7, 24, 7, 7, 7, 7, 7, 7, 31, 31, 31, 7, 24, 31, 7, 7, 31, 24, 7, 7, 24, 31, 7, 7, 24, 31, 7, 7, 24, 31, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 35, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 61, 16, 61, 16, 15, 16, 15, 16, 61, 61, 15, 15, 16, 15, 16, 15, 15, 16, 15, 7, 7, 31, 24, 7, 31, 31, 24, 24, 24, 7, 31, 31, 7, 24, 7, 7, 7, 7, 24]\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 121, 1: 80, 2: 78, 3: 64, 4: 60, 5: 60, 6: 59, 7: 49, 8: 41, 12: 40, 10: 40, 11: 40, 9: 40, 13: 39, 14: 38, 16: 36, 15: 36, 17: 33, 18: 32, 19: 31, 20: 31, 21: 30, 22: 30, 23: 29, 24: 28, 25: 27, 26: 26, 27: 25, 29: 24, 28: 24, -1: 23, 30: 23, 31: 23, 32: 22, 35: 21, 33: 21, 34: 21, 37: 20, 50: 20, 51: 20, 47: 20, 45: 20, 48: 20, 49: 20, 46: 20, 52: 20, 53: 20, 43: 20, 36: 20, 38: 20, 39: 20, 40: 20, 41: 20, 42: 20, 44: 20, 54: 19, 56: 18, 55: 18, 59: 16, 58: 16, 57: 16, 61: 15, 60: 15, 63: 13, 64: 13, 62: 13, 65: 12, 66: 11})\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topic_sizes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: '-1_0x02f6_fp_cr_update', 0: '0_parity_cache_instruction_corrected', 1: '1_tlb_data_interrupt_fatal', 2: '2_1146800_hummer_double_exceptions', 3: '3_source_assert_idotransportmgr_cpp', 4: '4_illegal_instruction_interrupt_program', 5: '5_format_allreduce_exec_scaletest', 6: '6_dear_iar_0x00544ea8_0x00544eb8', 7: '7_microseconds_calls_correctable_single', 8: '8_lustre_gb1_point_mount', 9: '9_18a_src_glosli_g90', 10: '10_demo_spasm_mpi_germann2_128', 11: '11_tested_copro_tomandjeff_permission_denied', 12: '12_sym_ce_mask_0x10', 13: '13_enable_debug_pt_floating', 14: '14_vpd_match_card_check', 15: '15_link_severed_reading_116', 16: '16_received_errno_code_15', 17: '17_ddr_total_detected_corrected', 18: '18_fully_functional_warning_discovery', 19: '19_symbol_rank_errors_bit', 20: '20_suppressing_type_interrupts_info', 21: '21_caused_store_icbi_data', 22: '22_information_assembly_severe_discovery', 23: '23_cop_fftw_opt_urgent_new', 24: '24_interrupts_critical_input_total', 25: '25_vnm_32k_urgent_hpcc_ibm', 26: '26_load_message_reading_link_severed', 27: '27_10_status_tue_aug', 28: '28_1311300_hummer_double_exceptions', 29: '29_active_temperature_asserted_ok', 30: '30_gunnels_vnm64_vnm_urgent', 31: '31_interrupts_critical_input_total', 32: '32_06_20_chdir_login', 33: '33_ecid_0000000000000000000000000000_slot_processor', 34: '34_1004_reason_terminated_rts', 35: '35_usr_problem_ordering_byte', 36: '36__dev2__hyperclaw__jag_work__pasci2', 37: '37_map_bad_xyzt_pakin1', 38: '38_broadcast_yates_directory_file', 39: '39_raptor_254_noprint_followup', 40: '40_17_ddcmd1_glosli_ddcmdbglv', 41: '41_stability_work_mdcask_inferno', 42: '42_pollcontroldescriptors_died_debugger_detected', 43: '43_retrying_slept_nfs_seconds', 44: '44_g24_spasm_static_spasm_mpi_germann2', 45: '45_register_syndrome_0x08000000_exception', 46: '46_64mps_temporarily_sequential_resource', 47: '47_gathering_disable_store_fatal', 48: '48_floating_operation_point_info', 49: '49_unimplemented_operation_interrupt_program', 50: '50_storage_data_interrupt_fatal', 51: '51_space_address_instruction_fatal', 52: '52_unable_filesystem_mount_fatal', 53: '53_swl_sppm_chkpt_prep_ibm', 54: '54_hardware_linkcard_severe_power', 55: '55_1001_reason_terminated_rts', 56: '56_1969920_hummer_double_exceptions', 57: '57_source_assert_idotransportmgr_cpp', 58: '58_control_stream_read_0x0b', 59: '59_receiver_pipe_dcr_0x02ee', 60: '60_tree_wire_unit_0x0b', 61: '61_login_message_link_severed_reading', 62: '62_spasm_static_login_chdir_germann2', 63: '63_stopping_panic_execution_rts', 64: '64_vpd_j19_card_check', 65: '65_4294967295_51_treeaddr_sent', 66: '66_enable_critical_input_interrupt'}\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topic_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity cache instruction corrected error info kernel ras  \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "possible_topic = topic_model.get_topic(0)\n",
    "words = [i[0] for i in possible_topic]\n",
    "summary = ' '.join(words)\n",
    "print(summary)\n",
    "print(type(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "O tamanho do cluster 0 é 121\n",
      "O tamanho do cluster 1 é 80\n",
      "O tamanho do cluster 2 é 78\n",
      "O tamanho do cluster 3 é 64\n",
      "O tamanho do cluster 4 é 60\n",
      "O tamanho do cluster 5 é 60\n",
      "O tamanho do cluster 6 é 59\n",
      "O tamanho do cluster 7 é 49\n",
      "O tamanho do cluster 8 é 41\n",
      "O tamanho do cluster 9 é 40\n",
      "O tamanho do cluster 10 é 40\n",
      "O tamanho do cluster 11 é 40\n",
      "O tamanho do cluster 12 é 40\n",
      "O tamanho do cluster 13 é 39\n",
      "O tamanho do cluster 14 é 38\n",
      "O tamanho do cluster 15 é 36\n",
      "O tamanho do cluster 16 é 36\n",
      "O tamanho do cluster 17 é 33\n",
      "O tamanho do cluster 18 é 32\n",
      "O tamanho do cluster 19 é 31\n",
      "O tamanho do cluster 20 é 31\n",
      "O tamanho do cluster 21 é 30\n",
      "O tamanho do cluster 22 é 30\n",
      "O tamanho do cluster 23 é 29\n",
      "O tamanho do cluster 24 é 28\n",
      "O tamanho do cluster 25 é 27\n",
      "O tamanho do cluster 26 é 26\n",
      "O tamanho do cluster 27 é 25\n",
      "O tamanho do cluster 28 é 24\n",
      "O tamanho do cluster 29 é 24\n",
      "O tamanho do cluster 30 é 23\n",
      "O tamanho do cluster 31 é 23\n",
      "O tamanho do cluster 32 é 22\n",
      "O tamanho do cluster 33 é 21\n",
      "O tamanho do cluster 34 é 21\n",
      "O tamanho do cluster 35 é 21\n",
      "O tamanho do cluster 36 é 20\n",
      "O tamanho do cluster 37 é 20\n",
      "O tamanho do cluster 38 é 20\n",
      "O tamanho do cluster 39 é 20\n",
      "O tamanho do cluster 40 é 20\n",
      "O tamanho do cluster 41 é 20\n",
      "O tamanho do cluster 42 é 20\n",
      "O tamanho do cluster 43 é 20\n",
      "O tamanho do cluster 44 é 20\n",
      "O tamanho do cluster 45 é 20\n",
      "O tamanho do cluster 46 é 20\n",
      "O tamanho do cluster 47 é 20\n",
      "O tamanho do cluster 48 é 20\n",
      "O tamanho do cluster 49 é 20\n",
      "O tamanho do cluster 50 é 20\n",
      "O tamanho do cluster 51 é 20\n",
      "O tamanho do cluster 52 é 20\n",
      "O tamanho do cluster 53 é 20\n",
      "O tamanho do cluster 54 é 19\n",
      "O tamanho do cluster 55 é 18\n",
      "O tamanho do cluster 56 é 18\n",
      "O tamanho do cluster 57 é 16\n",
      "O tamanho do cluster 58 é 16\n",
      "O tamanho do cluster 59 é 16\n",
      "O tamanho do cluster 60 é 15\n",
      "O tamanho do cluster 61 é 15\n",
      "O tamanho do cluster 62 é 13\n",
      "O tamanho do cluster 63 é 13\n",
      "O tamanho do cluster 64 é 13\n",
      "O tamanho do cluster 65 é 12\n",
      "O tamanho do cluster 66 é 11\n"
     ]
    }
   ],
   "source": [
    "# Step 13 - Creates lists of lists using BerTopic clustering\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "# Creates blank lists\n",
    "#for elem in range (clusterer.labels_.max()+1):\n",
    "for elem in range (max(topic_model.topics_)+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(topic_model.topics_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "for i in range(len(cluster_idxs)):\n",
    "   print(\"O tamanho do cluster {} é {}\".format(i,len(cluster_idxs[i])))\n",
    "\n",
    "#print(cluster_lines[10][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 14 - Builds new file with topic modeling summaries - Bertopic Global (Using BerTopic Clustering)\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "# ## Creates list of boolean values, representing summarized topics\n",
    "# for idx in range(clusterer.labels_.max()):\n",
    "#     cluster_topic.append(None)\n",
    "\n",
    "for elem in topic_model.topics_:\n",
    "    \n",
    "    line_topic = topic_model.get_topic(elem)\n",
    "    words = [i[0] for i in line_topic]\n",
    "    summary = ' '.join(words)\n",
    "    topic_summaries.append(summary)\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics_global.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21961190476190648\n",
      "0.5396357142857172\n",
      "0.30495196617323017\n"
     ]
    }
   ],
   "source": [
    "## Step 12 - Calculates average recall, precision and f1\n",
    "\n",
    "## Initial tests with rouge\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "count_precision = 0\n",
    "count_recall = 0\n",
    "count_f1 = 0\n",
    "total_lines = 2000\n",
    "dataset = \"bgl\"\n",
    "#target_file = \"_luhn.txt\"\n",
    "#target_file = \"_lsa.txt\"\n",
    "#target_file = \"_local_topics.txt\"\n",
    "#target_file = \"_global_topics.txt\"\n",
    "#target_file = \"_lda_topics.txt\"\n",
    "#target_file = \"_lexrank.txt\"\n",
    "#target_file = \"_textrank.txt\"\n",
    "#target_file = \"_bert_topics_local.txt\"\n",
    "target_file = \"_bert_topics_global.txt\"\n",
    "\n",
    "# Opens external files with ground truth summaries and created topics\n",
    "with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "     open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "    for line_summary, line_topic in zip(summaries, topics):\n",
    "        line_summary = line_summary[:-2]\n",
    "        line_summaries = line_summary.split(\";\")\n",
    "\n",
    "        for summary in line_summaries:\n",
    "            current_precision = 0\n",
    "            current_recall = 0\n",
    "            current_f1 = 0\n",
    "            #print(\"Agora estamos comparando '{}' e '{}'\".format(line_topic, summary))\n",
    "            metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']    \n",
    "            ## If the summary improves the f1 score, saves its metrics\n",
    "            if (current_f1 < metrics['f']):\n",
    "                current_precision = metrics['p']\n",
    "                current_recall = metrics['r']\n",
    "                current_f1 = metrics['f']\n",
    "        \n",
    "        count_precision += current_precision\n",
    "        count_recall += current_recall        \n",
    "        count_f1 += current_f1\n",
    "\n",
    "final_precision = count_precision/total_lines\n",
    "final_recall = count_recall/total_lines\n",
    "final_f1 = count_f1/total_lines\n",
    "\n",
    "print(final_precision)\n",
    "print(final_recall)\n",
    "print(final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0.75, 'p': 0.75, 'f': 0.749999995}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "line_topic = \"make compile completed successfully\"\n",
    "summary = \"the software make and compile was completed successfully\"\n",
    "summary2 = \"run make compile completed\"\n",
    "\n",
    "metrics = rouge.get_scores(summary2, line_topic)[0]['rouge-1'] \n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
