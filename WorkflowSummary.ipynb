{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0 - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"logs\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "logName = 'Ciena_error_lines_20220701-20220715.txt' # Name of file to be parsed\n",
    "#logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "#logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1 - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2.1 - Vector Creation Using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    print(type(vectors_tfidf))\n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3 - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variable list to variable sparse matrix\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "var_df = mlb.fit_transform(var_df)\n",
    "print (\"A matrix parseada de variaveis tem o formato {}\".format(var_df.shape))\n",
    "print(type(var_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Creates distance matrix \n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "import numpy as np\n",
    "\n",
    "# Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "#Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(tfidf_distance)\n",
    "max_val = np.max(tfidf_distance)\n",
    "tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de embeddings são {}\".format(tfidf_distance.shape))\n",
    "\n",
    "# Using Jaccard Distance between the rows of the Variable Matrix\n",
    "var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "print(\"As dimensões da matriz de variáveis são {}\".format(var_distance.shape))\n",
    "\n",
    "# Creates Count Matrix using line numbers from log lines as the counter\n",
    "count_list = []\n",
    "n = len(tfidf_distance)\n",
    "count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "for i in range(n):\n",
    "        count_list.append(i)\n",
    "\n",
    "# Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "count_array = np.array(count_list)\n",
    "for x in count_array:\n",
    "  for y in count_array:\n",
    "    count_distance[x,y] = abs(x-y)\n",
    "# Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(count_distance)\n",
    "max_val = np.max(count_distance)\n",
    "count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de contadores são {}\".format(count_distance.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving matrices\n",
    "\n",
    "print(type(tfidf_distance))\n",
    "np.save(\"tfidf_distance_\" + logName + \".csv\", tfidf_distance)\n",
    "print(type(var_distance))\n",
    "np.save(\"var_distance_\" + logName + \".csv\", var_distance)\n",
    "print(type(count_distance))\n",
    "np.save(\"count_distance_\" + logName + \".csv\", count_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads matrices\n",
    "\n",
    "tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "var_distance = np.load(\"var_distance_\" + logName + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using alpha to define the weight of the TFIDF Matrix,  Beta to define the weight of the Variable Matrix,\n",
    "# and Gamma to define the weight of the Count Matrix\n",
    "alpha = 0.3\n",
    "beta = 0.5\n",
    "gamma = 0.2\n",
    "\n",
    "if alpha+beta+gamma > 1:\n",
    "   raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "# New matrices, corrected by the weights\n",
    "tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "var_distance_wtd = np.dot(beta, var_distance)\n",
    "count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "# Sums remaining matrices\n",
    "distance_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 5 - Clustering with HDBScan Using Pre-defined Distance Matrix\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "\n",
    "# min_cluster_size:int, optional (default=5)\n",
    "# The minimum size of clusters; single linkage splits that contain fewer points than this will \n",
    "# be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "# min_samples:int, optional (default=None)\n",
    "# The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "\n",
    "# p :int, optional (default=None)\n",
    "# p value to use if using the minkowski metric.\n",
    "\n",
    "# alpha :float, optional (default=1.0)\n",
    "# A distance scaling parameter as used in robust single linkage. See [3] for more information.\n",
    "\n",
    "# cluster_selection_epsilon: float, optional (default=0.0)\n",
    "# A distance threshold. Clusters below this value will be merged.\n",
    "# See [5] for more information.\n",
    "\n",
    "# algorithm :string, optional (default=’best’)\n",
    "# Exactly which algorithm to use; hdbscan has variants specialised for different characteristics \n",
    "# of the data. By default this is set to best which chooses the “best” algorithm given the nature \n",
    "# of the data. You can force other options if you believe you know better. Options are: 'best',\n",
    "# 'generic', 'prims_kdtree', 'prims_balltree', 'boruvka_kdtree' and 'boruvka_balltree'\n",
    "\n",
    "# leaf_size: int, optional (default=40)\n",
    "# If using a space tree algorithm (kdtree, or balltree) the number of points ina leaf node of the tree. \n",
    "# This does not alter the resulting clustering, but may have an effect on the runtime of the algorithm.\n",
    "\n",
    "# cluster_selection_method :string, optional (default=’eom’)\n",
    "# The method used to select clusters from the condensed tree. The standard approach for HDBSCAN is \n",
    "# to use an Excess of Mass algorithm to find the most persistent clusters. Alternatively you can \n",
    "# instead select the clusters at the leaves of the tree – this provides the most fine grained and \n",
    "# homogeneous clusters. Options are: 'eom' and 'leaf'\n",
    "\n",
    "# allow_single_cluster :bool, optional (default=False)\n",
    "# By default HDBSCAN will not produce a single cluster, setting this to True will override this \n",
    "# and allow single cluster results in the case that you feel this is a valid result for your dataset.\n",
    "\n",
    "## Clusters with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=None,metric='precomputed',\n",
    "                            cluster_selection_epsilon=0.8, alpha=1.0, leaf_size=40, \n",
    "                            allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                            gen_min_span_tree=True)\n",
    "\n",
    "\n",
    "clusterer.fit(distance_matrix)\n",
    "\n",
    "print (\"O numero de clusters e {}\".format(clusterer.labels_.max()))\n",
    "print (\"Os clusters de cada elemento são {}\".format(clusterer.labels_))\n",
    "\n",
    "## Checks number of outliers\n",
    "cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 7 - Checks number of outliers\n",
    "\n",
    "cont = 0\n",
    "\n",
    "for elem in clusterer.labels_:\n",
    "   if (elem == -1):\n",
    "      cont += 1\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 11 - Creates a list of lists representing the clusters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"Content\"]\n",
    "\n",
    "# Creates blank lists\n",
    "for elem in range (clusterer.labels_.max()+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "#for i in range(len(cluster_idxs)):\n",
    "#   print(len(cluster_idxs[i]))\n",
    "\n",
    "#print(cluster_lines[10][9])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 12 - Eliminates stopwords on each cluster\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "import nltk\n",
    "\n",
    "# Parameters\n",
    "#nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "# Converts sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Removes stopwords from each sentence\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "\n",
    "# Finds topics of a given cluster\n",
    "def find_topics(cluster_list, cluster_number, num_topics):\n",
    "    # Converts to words\n",
    "    data_words = list(sent_to_words(cluster_list[cluster_number]))\n",
    "    # Removes stop words - OPTIONAL\n",
    "    #data_words = remove_stopwords(data_words)\n",
    "    # Creates dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Creates corpora\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    # Builds LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "    # Prints topics\n",
    "    pprint(lda_model.print_topics())\n",
    "    pprint(lda_model.get_topic_terms(0))\n",
    "    return lda_model\n",
    "\n",
    "topics = find_topics(cluster_lines, 0, 1)\n",
    "\n",
    "# Gets word topics\n",
    "x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# Prints only the topics\n",
    "for topic,words in topics_words:\n",
    "    print(words)\n",
    "\n",
    "for topic,words in topics_words:\n",
    "    print(str(topic)+ \"::\"+ str(words))\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print (\" \".join(words))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
