{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0 - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "dataset = \"bgl\" # The name of the dataset being tested\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker-OLD\\LineTracker\\ground_truths\\\n",
      "Parsing file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker-OLD\\LineTracker\\ground_truths\\bgl_lines.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 7611.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.782367]\n"
     ]
    }
   ],
   "source": [
    "## Step 1 - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker-OLD\\LineTracker\\ground_truths\\bgl_lines.txt\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 2 - Vector Creation Using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    print(type(vectors_tfidf))\n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix parseada de variaveis tem o formato (2000, 1395)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 3 - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variable list to variable sparse matrix\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "var_df = mlb.fit_transform(var_df)\n",
    "print (\"A matrix parseada de variaveis tem o formato {}\".format(var_df.shape))\n",
    "print(type(var_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de embeddings são (2000, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2181: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de variáveis são (2000, 2000)\n",
      "As dimensões da matriz de contadores são (2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Creates distance matrix \n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "import numpy as np\n",
    "\n",
    "# Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "#Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(tfidf_distance)\n",
    "max_val = np.max(tfidf_distance)\n",
    "tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de embeddings são {}\".format(tfidf_distance.shape))\n",
    "\n",
    "# Using Jaccard Distance between the rows of the Variable Matrix\n",
    "var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "print(\"As dimensões da matriz de variáveis são {}\".format(var_distance.shape))\n",
    "\n",
    "# Creates Count Matrix using line numbers from log lines as the counter\n",
    "count_list = []\n",
    "n = len(tfidf_distance)\n",
    "count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "for i in range(n):\n",
    "        count_list.append(i)\n",
    "\n",
    "# Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "count_array = np.array(count_list)\n",
    "for x in count_array:\n",
    "  for y in count_array:\n",
    "    count_distance[x,y] = abs(x-y)\n",
    "# Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(count_distance)\n",
    "max_val = np.max(count_distance)\n",
    "count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de contadores são {}\".format(count_distance.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving matrices\n",
    "\n",
    "print(type(tfidf_distance))\n",
    "np.save(\"tfidf_distance_\" + logName + \".csv\", tfidf_distance)\n",
    "print(type(var_distance))\n",
    "np.save(\"var_distance_\" + logName + \".csv\", var_distance)\n",
    "print(type(count_distance))\n",
    "np.save(\"count_distance_\" + logName + \".csv\", count_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads matrices\n",
    "\n",
    "tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "var_distance = np.load(\"var_distance_\" + logName + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Using alpha to define the weight of the TFIDF Matrix,  \n",
    "# Beta to define the weight of the Variable Matrix,\n",
    "# and Gamma to define the weight of the Count Matrix\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "\n",
    "if alpha+beta+gamma > 1:\n",
    "   raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "# New matrices, corrected by the weights\n",
    "tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "var_distance_wtd = np.dot(beta, var_distance)\n",
    "count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "# Sums remaining matrices\n",
    "distance_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O numero de clusters e 18\n",
      "Os clusters de cada elemento são [16 16 16 ... 16 16 16]\n",
      "O número de outliers é 9\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 6 - Clustering with HDBScan Using Pre-defined Distance Matrix\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "\n",
    "# min_cluster_size:int, optional (default=5)\n",
    "# The minimum size of clusters; single linkage splits that contain fewer points than this will \n",
    "# be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "# min_samples:int, optional (default=None)\n",
    "# The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "\n",
    "# p :int, optional (default=None)\n",
    "# p value to use if using the minkowski metric.\n",
    "\n",
    "# alpha :float, optional (default=1.0)\n",
    "# A distance scaling parameter as used in robust single linkage. See [3] for more information.\n",
    "\n",
    "# cluster_selection_epsilon: float, optional (default=0.0)\n",
    "# A distance threshold. Clusters below this value will be merged.\n",
    "# See [5] for more information.\n",
    "\n",
    "# algorithm :string, optional (default=’best’)\n",
    "# Exactly which algorithm to use; hdbscan has variants specialised for different characteristics \n",
    "# of the data. By default this is set to best which chooses the “best” algorithm given the nature \n",
    "# of the data. You can force other options if you believe you know better. Options are: 'best',\n",
    "# 'generic', 'prims_kdtree', 'prims_balltree', 'boruvka_kdtree' and 'boruvka_balltree'\n",
    "\n",
    "# leaf_size: int, optional (default=40)\n",
    "# If using a space tree algorithm (kdtree, or balltree) the number of points ina leaf node of the tree. \n",
    "# This does not alter the resulting clustering, but may have an effect on the runtime of the algorithm.\n",
    "\n",
    "# cluster_selection_method :string, optional (default=’eom’)\n",
    "# The method used to select clusters from the condensed tree. The standard approach for HDBSCAN is \n",
    "# to use an Excess of Mass algorithm to find the most persistent clusters. Alternatively you can \n",
    "# instead select the clusters at the leaves of the tree – this provides the most fine grained and \n",
    "# homogeneous clusters. Options are: 'eom' and 'leaf'\n",
    "\n",
    "# allow_single_cluster :bool, optional (default=False)\n",
    "# By default HDBSCAN will not produce a single cluster, setting this to True will override this \n",
    "# and allow single cluster results in the case that you feel this is a valid result for your dataset.\n",
    "\n",
    "## Clusters with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=None,metric='precomputed',\n",
    "                            cluster_selection_epsilon=0.75, alpha=1.0, leaf_size=40, \n",
    "                            allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                            gen_min_span_tree=True)\n",
    "\n",
    "\n",
    "clusterer.fit(distance_matrix)\n",
    "\n",
    "print (\"O numero de clusters e {}\".format(clusterer.labels_.max()))\n",
    "print (\"Os clusters de cada elemento são {}\".format(clusterer.labels_))\n",
    "\n",
    "## Checks number of outliers\n",
    "cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6.1 - Uses K-Means for clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(n_clusters=50, random_state=0, n_init=10)\n",
    "clusterer.fit(distance_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento sao {}\".format(cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 6.2 - Uses K-Medoids for clustering\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "\n",
    "clusterer = KMedoids(n_clusters=50, random_state=0, metric='precomputed')\n",
    "clusterer.fit(distance_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento são {}\".format(cluster_labels))\n",
    "print (\"Os centros dos clusters são {} \".format(clusterer.cluster_centers_))\n",
    "print (\"A inércia dos clusters é {}\".format(clusterer.inertia_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número de outliers é 9\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 7 - Checks number of outliers\n",
    "\n",
    "cont = 0\n",
    "\n",
    "for elem in clusterer.labels_:\n",
    "   if (elem == -1):\n",
    "      cont += 1\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do cluster 0 é 60\n",
      "O tamanho do cluster 1 é 8\n",
      "O tamanho do cluster 2 é 120\n",
      "O tamanho do cluster 3 é 36\n",
      "O tamanho do cluster 4 é 87\n",
      "O tamanho do cluster 5 é 40\n",
      "O tamanho do cluster 6 é 29\n",
      "O tamanho do cluster 7 é 10\n",
      "O tamanho do cluster 8 é 20\n",
      "O tamanho do cluster 9 é 61\n",
      "O tamanho do cluster 10 é 20\n",
      "O tamanho do cluster 11 é 30\n",
      "O tamanho do cluster 12 é 66\n",
      "O tamanho do cluster 13 é 40\n",
      "O tamanho do cluster 14 é 72\n",
      "O tamanho do cluster 15 é 6\n",
      "O tamanho do cluster 16 é 934\n",
      "O tamanho do cluster 17 é 332\n",
      "O tamanho do cluster 18 é 20\n"
     ]
    }
   ],
   "source": [
    "## Step 8 - Creates a list of lists representing the clusters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "# Creates blank lists\n",
    "for elem in range (clusterer.labels_.max()+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "for i in range(len(cluster_idxs)):\n",
    "   print(\"O tamanho do cluster {} é {}\".format(i,len(cluster_idxs[i])))\n",
    "\n",
    "#print(cluster_lines[10][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[960, 961, 962, 963, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 976, 977, 978, 979, 1500, 1501, 1502, 1503, 1504, 1505, 1506, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519]\n"
     ]
    }
   ],
   "source": [
    "print(cluster_idxs[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9 - Eliminates stopwords on each cluster\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "\n",
    "# Parameters\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['teste'])\n",
    "\n",
    "# Converts sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Removes stopwords from each sentence\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10 - Finds topics of a given cluster using LDA\n",
    "\n",
    "# Finds topic of a given cluster, defining the number of topics\n",
    "def find_topics(cluster_list, cluster_number, num_topics):\n",
    "    # Converts to words\n",
    "    data_words = list(sent_to_words(cluster_list[cluster_number]))\n",
    "    # Removes stop words\n",
    "    #data_words = remove_stopwords(data_words)\n",
    "    # Creates dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Creates corpora\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    # Builds LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "    return lda_model\n",
    "\n",
    "# topics = find_topics(cluster_lines, 9, 1)\n",
    "\n",
    "# # Gets word topics\n",
    "# x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "# topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "# #Below Code Prints Only Words \n",
    "# for topic,words in topics_words:\n",
    "#     a =  \" \".join(words)\n",
    "#     print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(clusterer.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 10-A - Find topic on a given cluster using BerTopic\n",
    "\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def find_topics_bertopic(cluster_list, cluster_number, num_topics):\n",
    "        \n",
    "        umap_model = UMAP(init='random')\n",
    "        cluster_model = KMedoids(n_clusters = 1)\n",
    "        vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "        embedding_model = \"all-MiniLM-L6-v2\"\n",
    "        topic_model = BERTopic(embedding_model=embedding_model, hdbscan_model=cluster_model, \n",
    "                               vectorizer_model=vectorizer_model, umap_model=umap_model, top_n_words=10)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(cluster_list[cluster_number])\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 11 - Builds new file with topic modeling summaries - LDA\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "## Creates list of boolean values, representing summarized topics\n",
    "for idx in range(clusterer.labels_.max()):\n",
    "    cluster_topic.append(None)\n",
    "\n",
    "for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "    ## For each cluster, maps topics, and defines them as the summary\n",
    "    if (cluster_topic[elem-1] == None):\n",
    "        topics = find_topics(cluster_lines, elem-1, 1)\n",
    "        x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "        topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "        for topic,words in topics_words:\n",
    "            summary = \" \".join(words)\n",
    "        cluster_topic[elem-1] = summary\n",
    "    \n",
    "    if elem == -1:\n",
    "        topic_summaries.append(\"\")\n",
    "    else:\n",
    "        topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_global_topics.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RepositoryNotFoundError",
     "evalue": "401 Client Error. (Request ID: Root=1-665e3055-3fb2d4ec4791dd172860e9a6)\n\nRepository Not Found for url: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nUser Access Token \"llama\" is expired",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:259\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 259\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, elem \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(clusterer\u001b[38;5;241m.\u001b[39mlabels_):\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m## For each cluster, maps topics, and defines them as the summary\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (cluster_topic[elem\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m---> 14\u001b[0m         summary \u001b[38;5;241m=\u001b[39m find_topics_bertopic(cluster_lines, elem\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     15\u001b[0m         cluster_topic[elem\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m summary\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "Cell \u001b[1;32mIn[9], line 14\u001b[0m, in \u001b[0;36mfind_topics_bertopic\u001b[1;34m(cluster_list, cluster_number, num_topics)\u001b[0m\n\u001b[0;32m     12\u001b[0m cluster_model \u001b[38;5;241m=\u001b[39m KMedoids(n_clusters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     13\u001b[0m vectorizer_model \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 14\u001b[0m embedding_model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence-transformers/all-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m)      \n\u001b[0;32m     15\u001b[0m topic_model \u001b[38;5;241m=\u001b[39m BERTopic(embedding_model\u001b[38;5;241m=\u001b[39membedding_model, hdbscan_model\u001b[38;5;241m=\u001b[39mcluster_model, \n\u001b[0;32m     16\u001b[0m                        vectorizer_model\u001b[38;5;241m=\u001b[39mvectorizer_model, umap_model\u001b[38;5;241m=\u001b[39mumap_model, top_n_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#Applies BertTopic\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:87\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[1;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[0;32m     83\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cache_folder, model_name_or_path\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[0;32m     86\u001b[0m         \u001b[38;5;66;03m# Download from hub with caching\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m         snapshot_download(model_name_or_path,\n\u001b[0;32m     88\u001b[0m                             cache_dir\u001b[38;5;241m=\u001b[39mcache_folder,\n\u001b[0;32m     89\u001b[0m                             library_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence-transformers\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     90\u001b[0m                             library_version\u001b[38;5;241m=\u001b[39m__version__,\n\u001b[0;32m     91\u001b[0m                             ignore_files\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflax_model.msgpack\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrust_model.ot\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtf_model.h5\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     92\u001b[0m                             use_auth_token\u001b[38;5;241m=\u001b[39muse_auth_token)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(model_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodules.json\u001b[39m\u001b[38;5;124m'\u001b[39m)):    \u001b[38;5;66;03m#Load as SentenceTransformer model\u001b[39;00m\n\u001b[0;32m     95\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_sbert_model(model_path)\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\sentence_transformers\\util.py:442\u001b[0m, in \u001b[0;36msnapshot_download\u001b[1;34m(repo_id, revision, cache_dir, library_name, library_version, user_agent, ignore_files, use_auth_token)\u001b[0m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_auth_token:\n\u001b[0;32m    440\u001b[0m     token \u001b[38;5;241m=\u001b[39m HfFolder\u001b[38;5;241m.\u001b[39mget_token()\n\u001b[1;32m--> 442\u001b[0m model_info \u001b[38;5;241m=\u001b[39m _api\u001b[38;5;241m.\u001b[39mmodel_info(repo_id\u001b[38;5;241m=\u001b[39mrepo_id, revision\u001b[38;5;241m=\u001b[39mrevision, token\u001b[38;5;241m=\u001b[39mtoken)\n\u001b[0;32m    444\u001b[0m storage_folder \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m    445\u001b[0m     cache_dir, repo_id\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    446\u001b[0m )\n\u001b[0;32m    448\u001b[0m all_files \u001b[38;5;241m=\u001b[39m model_info\u001b[38;5;241m.\u001b[39msiblings\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\huggingface_hub\\hf_api.py:1676\u001b[0m, in \u001b[0;36mHfApi.model_info\u001b[1;34m(self, repo_id, revision, timeout, securityStatus, files_metadata, token)\u001b[0m\n\u001b[0;32m   1674\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblobs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   1675\u001b[0m r \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mget(path, headers\u001b[38;5;241m=\u001b[39mheaders, timeout\u001b[38;5;241m=\u001b[39mtimeout, params\u001b[38;5;241m=\u001b[39mparams)\n\u001b[1;32m-> 1676\u001b[0m hf_raise_for_status(r)\n\u001b[0;32m   1677\u001b[0m d \u001b[38;5;241m=\u001b[39m r\u001b[38;5;241m.\u001b[39mjson()\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ModelInfo(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39md)\n",
      "File \u001b[1;32mc:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_errors.py:291\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepoNotFound\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m:\n\u001b[0;32m    278\u001b[0m     \u001b[38;5;66;03m# 401 is misleading as it is returned for:\u001b[39;00m\n\u001b[0;32m    279\u001b[0m     \u001b[38;5;66;03m#    - private and gated repos if user is not authenticated\u001b[39;00m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;66;03m#    - missing repos\u001b[39;00m\n\u001b[0;32m    281\u001b[0m     \u001b[38;5;66;03m# => for now, we process them as `RepoNotFound` anyway.\u001b[39;00m\n\u001b[0;32m    282\u001b[0m     \u001b[38;5;66;03m# See https://gist.github.com/Wauplin/46c27ad266b15998ce56a6603796f0b9\u001b[39;00m\n\u001b[0;32m    283\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m     )\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m RepositoryNotFoundError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n\u001b[0;32m    294\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    295\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m endpoint_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBad request:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    296\u001b[0m     )\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-665e3055-3fb2d4ec4791dd172860e9a6)\n\nRepository Not Found for url: https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.\nUser Access Token \"llama\" is expired"
     ]
    }
   ],
   "source": [
    "## Step 11-A - Builds new file with topic modeling summaries - Bertopic\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "## Creates list of boolean values, representing summarized topics\n",
    "for idx in range(clusterer.labels_.max()):\n",
    "    cluster_topic.append(None)\n",
    "\n",
    "for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "    ## For each cluster, maps topics, and defines them as the summary\n",
    "    if (cluster_topic[elem-1] == None):\n",
    "        summary = find_topics_bertopic(cluster_lines, elem-1, 1)\n",
    "        cluster_topic[elem-1] = summary\n",
    "    \n",
    "    if elem == -1:\n",
    "        topic_summaries.append(\"\")\n",
    "    else:\n",
    "        topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics_local.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 - Builds file with clustering from BerTopic Local (Using pre-defined clustering)\n",
    "\n",
    "lines = []\n",
    "\n",
    "with open('ground_truths/' + dataset + '_lines.txt', 'r') as line_file:\n",
    "    for line in line_file:\n",
    "        lines.append(line)\n",
    "\n",
    "from umap import UMAP\n",
    "from bertopic import BERTopic\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "umap_model = UMAP(init='random')\n",
    "vectorizer_model = CountVectorizer(stop_words=\"english\")\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")        \n",
    "topic_model = BERTopic(embedding_model=embedding_model, vectorizer_model=vectorizer_model, \n",
    "                       umap_model=umap_model, top_n_words=10)\n",
    "topics, probs = topic_model.fit_transform(lines)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 65, 65, 27, 65, 65, 27, 65, 65, 65, -1, 65, -1, -1, 65, 27, 65, 27, 65, 27, 65, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 50, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 4, 21, 21, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, 35, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, -1, 4, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 66, 21, 21, 66, 21, 66, 35, 21, 35, 21, 35, 21, 35, 21, 35, 21, 21, 35, 21, 35, 35, 21, 35, 21, 35, 21, 4, 13, 4, 13, 4, 13, 4, 13, 4, 13, 4, 13, 13, 4, 13, 4, 13, 4, 13, 4, 63, 13, 63, 63, 13, 63, 13, 63, 13, 13, 63, 13, 63, 63, 13, 63, 13, 63, 13, 63, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 59, 12, 17, 59, 12, 17, 19, 19, 32, 32, 58, 58, 58, 58, 58, 58, 58, 58, 58, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 56, 56, 2, 56, 56, 56, 56, 56, 56, 56, 2, 56, 56, 56, 56, 56, 56, 56, 56, 56, 12, 17, 17, 17, 19, 59, 59, 19, 19, 12, 19, 59, 19, 19, 17, 12, 17, 19, 12, 19, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 48, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 18, 22, 22, 14, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 54, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 28, 2, 28, 28, 28, 2, 28, 2, 28, 2, 2, 2, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 46, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 14, 33, 33, 14, 33, 33, 33, 14, 14, 33, 33, 14, 33, 33, 33, 14, 33, 33, 14, 14, 14, 14, 14, 33, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 23, 23, 23, 23, 23, 23, 23, 23, 23, 57, 57, 3, 3, 3, 3, 3, 3, 3, 57, 3, 57, 57, 3, 3, 3, 3, 3, 3, 3, 57, 3, 3, 3, 3, 3, 57, 57, 57, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 22, 29, 18, 29, 22, 29, 18, 18, 18, 22, 18, 22, 22, 29, 29, 22, 29, 29, 18, 18, 18, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 27, 12, 12, 17, 12, 17, 12, 12, 12, 12, 17, 17, 12, 17, 17, 12, 12, 12, 12, 12, 17, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 55, 63, 55, 55, 55, 34, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 55, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 53, 61, 61, 61, 62, 62, 62, 62, 61, 62, 62, 62, 62, 62, 62, 62, 61, 62, 61, 61, 62, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 43, 8, 58, 20, 58, 20, 22, 20, 19, 22, 20, 19, 60, 20, 19, 12, 17, 22, 20, 58, 63, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 17, 17, 12, 12, 12, 12, 12, 12, 12, 17, 12, 17, 17, 17, 12, 12, 17, 17, 17, 12, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 57, 57, 57, 57, 57, 57, 57, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 64, 64, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 32, 17, 17, 17, 12, 12, 17, 17, 19, 19, 59, 12, 12, 17, 17, 19, 12, 17, 19, 12, 17, 22, 20, 60, 20, 60, 20, 58, 20, 25, 20, 60, 60, 20, 20, 0, 58, 20, 25, 20, 60, 26, 26, 26, 26, 26, 26, 66, 60, 20, 60, 60, 20, 20, 25, 20, 22, 20, 58, 20, 22, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 30, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 30, 30, 30, 19, 19, 19, 19, 19, 12, 12, 19, 19, 19, 19, 19, 19, 12, 19, 19, 19, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 39, 25, 20, 60, 20, 25, 20, 25, 20, 25, 20, 60, 20, 60, 20, 60, 20, 60, 20, 60, 20, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 34, 22, 29, 18, 18, 18, 18, 22, 22, 22, 29, 22, 29, 29, 29, 18, 18, 18, 18, 18, 18, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 59, -1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 28, 28, 28, 28, 28, 28, 2, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 61, 16, 61, 16, 15, 16, 15, 16, 61, 61, 15, 15, 16, 15, 16, 15, 15, 16, 15, 7, 7, 31, 24, 7, 31, 31, 24, 24, 24, 7, 31, 31, 7, 24, 7, 7, 7, 7, 24, 7, 7, 24, 24, 24, 24, 31, 24, 7, 24, 24, 7, 7, 7, 7, 31, 31, 7, 24, 7, 7, 31, 24, 7, 7, 7, 31, 24, 7, 7, 24, 7, 7, 7, 7, 7, 7, 31, 31, 31, 7, 24, 31, 7, 7, 31, 24, 7, 7, 24, 31, 7, 7, 24, 31, 7, 7, 24, 31, 7, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 35, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 40, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 44, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 16, 61, 16, 61, 16, 15, 16, 15, 16, 61, 61, 15, 15, 16, 15, 16, 15, 15, 16, 15, 7, 7, 31, 24, 7, 31, 31, 24, 24, 24, 7, 31, 31, 7, 24, 7, 7, 7, 7, 24]\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topics_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 121, 1: 80, 2: 78, 3: 64, 4: 60, 5: 60, 6: 59, 7: 49, 8: 41, 12: 40, 10: 40, 11: 40, 9: 40, 13: 39, 14: 38, 16: 36, 15: 36, 17: 33, 18: 32, 19: 31, 20: 31, 21: 30, 22: 30, 23: 29, 24: 28, 25: 27, 26: 26, 27: 25, 29: 24, 28: 24, -1: 23, 30: 23, 31: 23, 32: 22, 35: 21, 33: 21, 34: 21, 37: 20, 50: 20, 51: 20, 47: 20, 45: 20, 48: 20, 49: 20, 46: 20, 52: 20, 53: 20, 43: 20, 36: 20, 38: 20, 39: 20, 40: 20, 41: 20, 42: 20, 44: 20, 54: 19, 56: 18, 55: 18, 59: 16, 58: 16, 57: 16, 61: 15, 60: 15, 63: 13, 64: 13, 62: 13, 65: 12, 66: 11})\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topic_sizes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{-1: '-1_0x02f6_fp_cr_update', 0: '0_parity_cache_instruction_corrected', 1: '1_tlb_data_interrupt_fatal', 2: '2_1146800_hummer_double_exceptions', 3: '3_source_assert_idotransportmgr_cpp', 4: '4_illegal_instruction_interrupt_program', 5: '5_format_allreduce_exec_scaletest', 6: '6_dear_iar_0x00544ea8_0x00544eb8', 7: '7_microseconds_calls_correctable_single', 8: '8_lustre_gb1_point_mount', 9: '9_18a_src_glosli_g90', 10: '10_demo_spasm_mpi_germann2_128', 11: '11_tested_copro_tomandjeff_permission_denied', 12: '12_sym_ce_mask_0x10', 13: '13_enable_debug_pt_floating', 14: '14_vpd_match_card_check', 15: '15_link_severed_reading_116', 16: '16_received_errno_code_15', 17: '17_ddr_total_detected_corrected', 18: '18_fully_functional_warning_discovery', 19: '19_symbol_rank_errors_bit', 20: '20_suppressing_type_interrupts_info', 21: '21_caused_store_icbi_data', 22: '22_information_assembly_severe_discovery', 23: '23_cop_fftw_opt_urgent_new', 24: '24_interrupts_critical_input_total', 25: '25_vnm_32k_urgent_hpcc_ibm', 26: '26_load_message_reading_link_severed', 27: '27_10_status_tue_aug', 28: '28_1311300_hummer_double_exceptions', 29: '29_active_temperature_asserted_ok', 30: '30_gunnels_vnm64_vnm_urgent', 31: '31_interrupts_critical_input_total', 32: '32_06_20_chdir_login', 33: '33_ecid_0000000000000000000000000000_slot_processor', 34: '34_1004_reason_terminated_rts', 35: '35_usr_problem_ordering_byte', 36: '36__dev2__hyperclaw__jag_work__pasci2', 37: '37_map_bad_xyzt_pakin1', 38: '38_broadcast_yates_directory_file', 39: '39_raptor_254_noprint_followup', 40: '40_17_ddcmd1_glosli_ddcmdbglv', 41: '41_stability_work_mdcask_inferno', 42: '42_pollcontroldescriptors_died_debugger_detected', 43: '43_retrying_slept_nfs_seconds', 44: '44_g24_spasm_static_spasm_mpi_germann2', 45: '45_register_syndrome_0x08000000_exception', 46: '46_64mps_temporarily_sequential_resource', 47: '47_gathering_disable_store_fatal', 48: '48_floating_operation_point_info', 49: '49_unimplemented_operation_interrupt_program', 50: '50_storage_data_interrupt_fatal', 51: '51_space_address_instruction_fatal', 52: '52_unable_filesystem_mount_fatal', 53: '53_swl_sppm_chkpt_prep_ibm', 54: '54_hardware_linkcard_severe_power', 55: '55_1001_reason_terminated_rts', 56: '56_1969920_hummer_double_exceptions', 57: '57_source_assert_idotransportmgr_cpp', 58: '58_control_stream_read_0x0b', 59: '59_receiver_pipe_dcr_0x02ee', 60: '60_tree_wire_unit_0x0b', 61: '61_login_message_link_severed_reading', 62: '62_spasm_static_login_chdir_germann2', 63: '63_stopping_panic_execution_rts', 64: '64_vpd_j19_card_check', 65: '65_4294967295_51_treeaddr_sent', 66: '66_enable_critical_input_interrupt'}\n"
     ]
    }
   ],
   "source": [
    "print(topic_model.topic_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parity cache instruction corrected error info kernel ras  \n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "possible_topic = topic_model.get_topic(0)\n",
    "words = [i[0] for i in possible_topic]\n",
    "summary = ' '.join(words)\n",
    "print(summary)\n",
    "print(type(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "O tamanho do cluster 0 é 121\n",
      "O tamanho do cluster 1 é 80\n",
      "O tamanho do cluster 2 é 78\n",
      "O tamanho do cluster 3 é 64\n",
      "O tamanho do cluster 4 é 60\n",
      "O tamanho do cluster 5 é 60\n",
      "O tamanho do cluster 6 é 59\n",
      "O tamanho do cluster 7 é 49\n",
      "O tamanho do cluster 8 é 41\n",
      "O tamanho do cluster 9 é 40\n",
      "O tamanho do cluster 10 é 40\n",
      "O tamanho do cluster 11 é 40\n",
      "O tamanho do cluster 12 é 40\n",
      "O tamanho do cluster 13 é 39\n",
      "O tamanho do cluster 14 é 38\n",
      "O tamanho do cluster 15 é 36\n",
      "O tamanho do cluster 16 é 36\n",
      "O tamanho do cluster 17 é 33\n",
      "O tamanho do cluster 18 é 32\n",
      "O tamanho do cluster 19 é 31\n",
      "O tamanho do cluster 20 é 31\n",
      "O tamanho do cluster 21 é 30\n",
      "O tamanho do cluster 22 é 30\n",
      "O tamanho do cluster 23 é 29\n",
      "O tamanho do cluster 24 é 28\n",
      "O tamanho do cluster 25 é 27\n",
      "O tamanho do cluster 26 é 26\n",
      "O tamanho do cluster 27 é 25\n",
      "O tamanho do cluster 28 é 24\n",
      "O tamanho do cluster 29 é 24\n",
      "O tamanho do cluster 30 é 23\n",
      "O tamanho do cluster 31 é 23\n",
      "O tamanho do cluster 32 é 22\n",
      "O tamanho do cluster 33 é 21\n",
      "O tamanho do cluster 34 é 21\n",
      "O tamanho do cluster 35 é 21\n",
      "O tamanho do cluster 36 é 20\n",
      "O tamanho do cluster 37 é 20\n",
      "O tamanho do cluster 38 é 20\n",
      "O tamanho do cluster 39 é 20\n",
      "O tamanho do cluster 40 é 20\n",
      "O tamanho do cluster 41 é 20\n",
      "O tamanho do cluster 42 é 20\n",
      "O tamanho do cluster 43 é 20\n",
      "O tamanho do cluster 44 é 20\n",
      "O tamanho do cluster 45 é 20\n",
      "O tamanho do cluster 46 é 20\n",
      "O tamanho do cluster 47 é 20\n",
      "O tamanho do cluster 48 é 20\n",
      "O tamanho do cluster 49 é 20\n",
      "O tamanho do cluster 50 é 20\n",
      "O tamanho do cluster 51 é 20\n",
      "O tamanho do cluster 52 é 20\n",
      "O tamanho do cluster 53 é 20\n",
      "O tamanho do cluster 54 é 19\n",
      "O tamanho do cluster 55 é 18\n",
      "O tamanho do cluster 56 é 18\n",
      "O tamanho do cluster 57 é 16\n",
      "O tamanho do cluster 58 é 16\n",
      "O tamanho do cluster 59 é 16\n",
      "O tamanho do cluster 60 é 15\n",
      "O tamanho do cluster 61 é 15\n",
      "O tamanho do cluster 62 é 13\n",
      "O tamanho do cluster 63 é 13\n",
      "O tamanho do cluster 64 é 13\n",
      "O tamanho do cluster 65 é 12\n",
      "O tamanho do cluster 66 é 11\n"
     ]
    }
   ],
   "source": [
    "# Step 13 - Creates lists of lists using BerTopic clustering\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "# Creates blank lists\n",
    "#for elem in range (clusterer.labels_.max()+1):\n",
    "for elem in range (max(topic_model.topics_)+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(topic_model.topics_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "for i in range(len(cluster_idxs)):\n",
    "   print(\"O tamanho do cluster {} é {}\".format(i,len(cluster_idxs[i])))\n",
    "\n",
    "#print(cluster_lines[10][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 14 - Builds new file with topic modeling summaries - Bertopic Global (Using BerTopic Clustering)\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "# ## Creates list of boolean values, representing summarized topics\n",
    "# for idx in range(clusterer.labels_.max()):\n",
    "#     cluster_topic.append(None)\n",
    "\n",
    "for elem in topic_model.topics_:\n",
    "    \n",
    "    line_topic = topic_model.get_topic(elem)\n",
    "    words = [i[0] for i in line_topic]\n",
    "    summary = ' '.join(words)\n",
    "    topic_summaries.append(summary)\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics_global.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21961190476190648\n",
      "0.5396357142857172\n",
      "0.30495196617323017\n"
     ]
    }
   ],
   "source": [
    "## Step 12 - Calculates average recall, precision and f1\n",
    "\n",
    "## Initial tests with rouge\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "count_precision = 0\n",
    "count_recall = 0\n",
    "count_f1 = 0\n",
    "total_lines = 2000\n",
    "dataset = \"bgl\"\n",
    "#target_file = \"_luhn.txt\"\n",
    "#target_file = \"_lsa.txt\"\n",
    "#target_file = \"_local_topics.txt\"\n",
    "#target_file = \"_global_topics.txt\"\n",
    "#target_file = \"_lda_topics.txt\"\n",
    "#target_file = \"_lexrank.txt\"\n",
    "#target_file = \"_textrank.txt\"\n",
    "#target_file = \"_bert_topics_local.txt\"\n",
    "target_file = \"_bert_topics_global.txt\"\n",
    "\n",
    "# Opens external files with ground truth summaries and created topics\n",
    "with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "     open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "    for line_summary, line_topic in zip(summaries, topics):\n",
    "        line_summary = line_summary[:-2]\n",
    "        line_summaries = line_summary.split(\";\")\n",
    "\n",
    "        for summary in line_summaries:\n",
    "            current_precision = 0\n",
    "            current_recall = 0\n",
    "            current_f1 = 0\n",
    "            #print(\"Agora estamos comparando '{}' e '{}'\".format(line_topic, summary))\n",
    "            metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']    \n",
    "            ## If the summary improves the f1 score, saves its metrics\n",
    "            if (current_f1 < metrics['f']):\n",
    "                current_precision = metrics['p']\n",
    "                current_recall = metrics['r']\n",
    "                current_f1 = metrics['f']\n",
    "        \n",
    "        count_precision += current_precision\n",
    "        count_recall += current_recall        \n",
    "        count_f1 += current_f1\n",
    "\n",
    "final_precision = count_precision/total_lines\n",
    "final_recall = count_recall/total_lines\n",
    "final_f1 = count_f1/total_lines\n",
    "\n",
    "print(final_precision)\n",
    "print(final_recall)\n",
    "print(final_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'r': 0.75, 'p': 0.75, 'f': 0.749999995}\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "rouge = Rouge()\n",
    "line_topic = \"make compile completed successfully\"\n",
    "summary = \"the software make and compile was completed successfully\"\n",
    "summary2 = \"run make compile completed\"\n",
    "\n",
    "metrics = rouge.get_scores(summary2, line_topic)[0]['rouge-1'] \n",
    "print(metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
