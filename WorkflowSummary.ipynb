{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 0 - Parameters and Libraries\n",
    "\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "dataset = \"hdfs\" # The name of the dataset being tested\n",
    "logName = dataset + '_lines.txt' # Name of file to be parsed\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\ground_truths\\\n",
      "Parsing file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\ground_truths\\hdfs_lines.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 2000/2000 [00:00<00:00, 17139.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:00.364999]\n"
     ]
    }
   ],
   "source": [
    "## Step 1 - Log Parsing Using Drain\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\CSL\\CSL-1\\ground_truths\\hdfs_lines.txt\n",
      "Iniciando encode\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 2 - Vector Creation Using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    if (path.is_file()):\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Iniciando encode\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    print(type(vectors_tfidf))\n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A matrix parseada de variaveis tem o formato (2000, 2447)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "## Step 3 - Creates matrix of parsed items\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variable list to variable sparse matrix\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "var_df = mlb.fit_transform(var_df)\n",
    "print (\"A matrix parseada de variaveis tem o formato {}\".format(var_df.shape))\n",
    "print(type(var_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de embeddings são (2000, 2000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vbert\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:2025: DataConversionWarning: Data was converted to boolean for metric jaccard\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As dimensões da matriz de variáveis são (2000, 2000)\n",
      "As dimensões da matriz de contadores são (2000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# Step 4 - Creates distance matrix \n",
    "\n",
    "from sklearn.metrics.pairwise import pairwise_distances \n",
    "import numpy as np\n",
    "\n",
    "# Using Euclidean Distance between the rows of the TFIDF Matrix\n",
    "tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "#Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(tfidf_distance)\n",
    "max_val = np.max(tfidf_distance)\n",
    "tfidf_distance = (tfidf_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de embeddings são {}\".format(tfidf_distance.shape))\n",
    "\n",
    "# Using Jaccard Distance between the rows of the Variable Matrix\n",
    "var_distance = pairwise_distances(np.asarray(var_df.todense()), metric=\"jaccard\", n_jobs=-1)\n",
    "print(\"As dimensões da matriz de variáveis são {}\".format(var_distance.shape))\n",
    "\n",
    "# Creates Count Matrix using line numbers from log lines as the counter\n",
    "count_list = []\n",
    "n = len(tfidf_distance)\n",
    "count_distance = np.zeros(shape=(n, n), dtype=int)\n",
    "for i in range(n):\n",
    "        count_list.append(i)\n",
    "\n",
    "# Using a Subtraction Distance using the line numbers as a Count Matrix\n",
    "count_array = np.array(count_list)\n",
    "for x in count_array:\n",
    "  for y in count_array:\n",
    "    count_distance[x,y] = abs(x-y)\n",
    "# Normalizes Distance Matrix with Min-Max\n",
    "min_val = np.min(count_distance)\n",
    "max_val = np.max(count_distance)\n",
    "count_distance = (count_distance - min_val) / (max_val - min_val)\n",
    "print(\"As dimensões da matriz de contadores são {}\".format(count_distance.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "## Saving matrices\n",
    "\n",
    "print(type(tfidf_distance))\n",
    "np.save(\"tfidf_distance_\" + logName + \".csv\", tfidf_distance)\n",
    "print(type(var_distance))\n",
    "np.save(\"var_distance_\" + logName + \".csv\", var_distance)\n",
    "print(type(count_distance))\n",
    "np.save(\"count_distance_\" + logName + \".csv\", count_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loads matrices\n",
    "\n",
    "tfidf_distance = np.load(\"tfidf_distance_\" + logName + \".csv\")\n",
    "count_distance = np.load(\"count_distance_\" + logName + \".csv\")\n",
    "var_distance = np.load(\"var_distance_\" + logName + \".csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5 - Using alpha to define the weight of the TFIDF Matrix,  \n",
    "# Beta to define the weight of the Variable Matrix,\n",
    "# and Gamma to define the weight of the Count Matrix\n",
    "alpha = 0.7\n",
    "beta = 0.2\n",
    "gamma = 0.1\n",
    "\n",
    "if alpha+beta+gamma > 1:\n",
    "   raise Exception(\"Valores devem somar 1!\")\n",
    "\n",
    "# New matrices, corrected by the weights\n",
    "tfidf_distance_wtd = np.dot(alpha,tfidf_distance)\n",
    "var_distance_wtd = np.dot(beta, var_distance)\n",
    "count_distance_wtd = np.dot(gamma, count_distance)\n",
    "\n",
    "# Sums remaining matrices\n",
    "distance_matrix = np.asarray(tfidf_distance_wtd + var_distance_wtd + count_distance_wtd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O numero de clusters e 6\n",
      "Os clusters de cada elemento são [ 5  5  5 ... -1 -1 -1]\n",
      "O número de outliers é 129\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 6 - Clustering with HDBScan Using Pre-defined Distance Matrix\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import hdbscan\n",
    "\n",
    "# min_cluster_size:int, optional (default=5)\n",
    "# The minimum size of clusters; single linkage splits that contain fewer points than this will \n",
    "# be considered points “falling out” of a cluster rather than a cluster splitting into two new clusters.\n",
    "\n",
    "# min_samples:int, optional (default=None)\n",
    "# The number of samples in a neighbourhood for a point to be considered a core point.\n",
    "\n",
    "# p :int, optional (default=None)\n",
    "# p value to use if using the minkowski metric.\n",
    "\n",
    "# alpha :float, optional (default=1.0)\n",
    "# A distance scaling parameter as used in robust single linkage. See [3] for more information.\n",
    "\n",
    "# cluster_selection_epsilon: float, optional (default=0.0)\n",
    "# A distance threshold. Clusters below this value will be merged.\n",
    "# See [5] for more information.\n",
    "\n",
    "# algorithm :string, optional (default=’best’)\n",
    "# Exactly which algorithm to use; hdbscan has variants specialised for different characteristics \n",
    "# of the data. By default this is set to best which chooses the “best” algorithm given the nature \n",
    "# of the data. You can force other options if you believe you know better. Options are: 'best',\n",
    "# 'generic', 'prims_kdtree', 'prims_balltree', 'boruvka_kdtree' and 'boruvka_balltree'\n",
    "\n",
    "# leaf_size: int, optional (default=40)\n",
    "# If using a space tree algorithm (kdtree, or balltree) the number of points ina leaf node of the tree. \n",
    "# This does not alter the resulting clustering, but may have an effect on the runtime of the algorithm.\n",
    "\n",
    "# cluster_selection_method :string, optional (default=’eom’)\n",
    "# The method used to select clusters from the condensed tree. The standard approach for HDBSCAN is \n",
    "# to use an Excess of Mass algorithm to find the most persistent clusters. Alternatively you can \n",
    "# instead select the clusters at the leaves of the tree – this provides the most fine grained and \n",
    "# homogeneous clusters. Options are: 'eom' and 'leaf'\n",
    "\n",
    "# allow_single_cluster :bool, optional (default=False)\n",
    "# By default HDBSCAN will not produce a single cluster, setting this to True will override this \n",
    "# and allow single cluster results in the case that you feel this is a valid result for your dataset.\n",
    "\n",
    "## Clusters with HDBSCAN\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=5,min_samples=None,metric='precomputed',\n",
    "                            cluster_selection_epsilon=0.75, alpha=1.0, leaf_size=40, \n",
    "                            allow_single_cluster=False,cluster_selection_method='eom',\n",
    "                            gen_min_span_tree=True)\n",
    "\n",
    "\n",
    "clusterer.fit(distance_matrix)\n",
    "\n",
    "print (\"O numero de clusters e {}\".format(clusterer.labels_.max()))\n",
    "print (\"Os clusters de cada elemento são {}\".format(clusterer.labels_))\n",
    "\n",
    "## Checks number of outliers\n",
    "cont = np.count_nonzero(clusterer.labels_ == -1)\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O numero de clusters e 49\n",
      "Os clusters de cada elemento sao [ 7  7  7 ... 16 16  0]\n"
     ]
    }
   ],
   "source": [
    "## A TESTAR\n",
    "## TESTAR TAMBÉM COM K-MEDOIDS\n",
    "\n",
    "## Step 6.1 - Uses K-Means for clustering\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "clusterer = KMeans(n_clusters=50, random_state=0, n_init=10)\n",
    "clusterer.fit(distance_matrix)\n",
    "cluster_num = clusterer.labels_.max()\n",
    "print (\"O numero de clusters e {}\".format(cluster_num))\n",
    "cluster_labels = clusterer.labels_\n",
    "print (\"Os clusters de cada elemento sao {}\".format(cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O número de outliers é 0\n",
      "O número de total de elementos é 2000\n"
     ]
    }
   ],
   "source": [
    "## Step 7 - Checks number of outliers\n",
    "\n",
    "cont = 0\n",
    "\n",
    "for elem in clusterer.labels_:\n",
    "   if (elem == -1):\n",
    "      cont += 1\n",
    "\n",
    "print(\"O número de outliers é {}\".format(cont))\n",
    "print(\"O número de total de elementos é {}\".format(len(clusterer.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O tamanho do cluster 0 é 20\n",
      "O tamanho do cluster 1 é 52\n",
      "O tamanho do cluster 2 é 77\n",
      "O tamanho do cluster 3 é 7\n",
      "O tamanho do cluster 4 é 7\n",
      "O tamanho do cluster 5 é 1697\n",
      "O tamanho do cluster 6 é 11\n"
     ]
    }
   ],
   "source": [
    "## Step 8 - Creates a list of lists representing the clusters\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "## General Parameters\n",
    "\n",
    "cluster_idxs = []\n",
    "cluster_lines = []\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "## Code\n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "elem_df = full_df[\"EventTemplate\"]\n",
    "\n",
    "# Creates blank lists\n",
    "for elem in range (clusterer.labels_.max()+1):\n",
    "    cluster_idxs.append([])\n",
    "    cluster_lines.append([])\n",
    "\n",
    "# Populate the lists with cluster elements\n",
    "for idx, elem in np.ndenumerate(clusterer.labels_):\n",
    "  if elem != -1:\n",
    "    cluster_idxs[elem].append(idx[0])\n",
    "    cluster_lines[elem].append(elem_df[idx[0]])\n",
    "\n",
    "# Check sizes of each cluster\n",
    "for i in range(len(cluster_idxs)):\n",
    "   print(\"O tamanho do cluster {} é {}\".format(i,len(cluster_idxs[i])))\n",
    "\n",
    "#print(cluster_lines[10][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 9 - Eliminates stopwords on each cluster\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk.corpus import stopwords\n",
    "import gensim.corpora as corpora\n",
    "from pprint import pprint\n",
    "import gensim\n",
    "\n",
    "# Parameters\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['teste'])\n",
    "\n",
    "# Converts sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "# Removes stopwords from each sentence\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 19\u001b[0m\n\u001b[0;32m     14\u001b[0m     lda_model \u001b[38;5;241m=\u001b[39m gensim\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mLdaMulticore(corpus\u001b[38;5;241m=\u001b[39mcorpus,\n\u001b[0;32m     15\u001b[0m                                        id2word\u001b[38;5;241m=\u001b[39mid2word,\n\u001b[0;32m     16\u001b[0m                                        num_topics\u001b[38;5;241m=\u001b[39mnum_topics)\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lda_model\n\u001b[1;32m---> 19\u001b[0m topics \u001b[38;5;241m=\u001b[39m find_topics(cluster_lines, \u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Gets word topics\u001b[39;00m\n\u001b[0;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m topics\u001b[38;5;241m.\u001b[39mshow_topics(num_topics\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,formatted\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[10], line 6\u001b[0m, in \u001b[0;36mfind_topics\u001b[1;34m(cluster_list, cluster_number, num_topics)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_topics\u001b[39m(cluster_list, cluster_number, num_topics):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Converts to words\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     data_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(sent_to_words(cluster_list[cluster_number]))\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Removes stop words\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#data_words = remove_stopwords(data_words)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;66;03m# Creates dictionary\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     id2word \u001b[38;5;241m=\u001b[39m corpora\u001b[38;5;241m.\u001b[39mDictionary(data_words)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## Step 10 - Finds topics of a given cluster\n",
    "\n",
    "# Finds topic of a given cluster, defining the number of topics\n",
    "def find_topics(cluster_list, cluster_number, num_topics):\n",
    "    # Converts to words\n",
    "    data_words = list(sent_to_words(cluster_list[cluster_number]))\n",
    "    # Removes stop words\n",
    "    #data_words = remove_stopwords(data_words)\n",
    "    # Creates dictionary\n",
    "    id2word = corpora.Dictionary(data_words)\n",
    "    # Creates corpora\n",
    "    corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "    # Builds LDA model\n",
    "    lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "    return lda_model\n",
    "\n",
    "topics = find_topics(cluster_lines, 9, 1)\n",
    "\n",
    "# Gets word topics\n",
    "x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    a =  \" \".join(words)\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 11 - Builds new file with topic modeling summaries\n",
    "\n",
    "cluster_topic = []\n",
    "topic_summaries = []\n",
    "\n",
    "## Creates list of boolean values, representing summarized topics\n",
    "for idx in range(clusterer.labels_.max()):\n",
    "    cluster_topic.append(None)\n",
    "\n",
    "for i, elem in enumerate(clusterer.labels_):\n",
    "\n",
    "    ## For each cluster, maps topics, and defines them as the summary\n",
    "    if (cluster_topic[elem-1] == None):\n",
    "        topics = find_topics(cluster_lines, elem-1, 1)\n",
    "        x = topics.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "        topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "        for topic,words in topics_words:\n",
    "            summary = \" \".join(words)\n",
    "        cluster_topic[elem-1] = summary\n",
    "    \n",
    "    if elem == -1:\n",
    "        topic_summaries.append(\"\")\n",
    "    else:\n",
    "        topic_summaries.append(cluster_topic[elem-1])\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_global_topics.txt\", \"w\") as f:\n",
    "     for line in topic_summaries:\n",
    "          f.write(f\"{line}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07132659263455057\n",
      "0.5953333333333353\n",
      "0.12319436951631704\n"
     ]
    }
   ],
   "source": [
    "## Step 12 - Calculates average recall, precision and f1\n",
    "\n",
    "## Initial tests with rouge\n",
    "\n",
    "from rouge import Rouge \n",
    "rouge = Rouge()\n",
    "\n",
    "count_precision = 0\n",
    "count_recall = 0\n",
    "count_f1 = 0\n",
    "total_lines = 2000\n",
    "dataset = \"Zookeeper\"\n",
    "#target_file = \"_luhn.txt\"\n",
    "target_file = \"_lsa.txt\"\n",
    "#target_file = \"_local_topics.txt\"\n",
    "#target_file = \"_global_topics.txt\"\n",
    "#target_file = \"_lda_topics.txt\"\n",
    "#target_file = \"_lexrank.txt\"\n",
    "#target_file = \"_textrank.txt\"\n",
    "\n",
    "# Opens external files with ground truth summaries and created topics\n",
    "with open('ground_truths/' + dataset + '_summaries.txt', 'r') as summaries, \\\n",
    "     open('ground_truths/' + dataset + target_file, 'r') as topics:\n",
    "    for line_summary, line_topic in zip(summaries, topics):\n",
    "        line_summary = line_summary[:-2]\n",
    "        line_summaries = line_summary.split(\";\")\n",
    "\n",
    "        for summary in line_summaries:\n",
    "            current_precision = 0\n",
    "            current_recall = 0\n",
    "            current_f1 = 0\n",
    "            #print(\"Agora estamos comparando '{}' e '{}'\".format(line_topic, summary))\n",
    "            metrics = rouge.get_scores(line_topic, summary)[0]['rouge-1']    \n",
    "            ## If the summary improves the f1 score, saves its metrics\n",
    "            if (current_f1 < metrics['f']):\n",
    "                current_precision = metrics['p']\n",
    "                current_recall = metrics['r']\n",
    "                current_f1 = metrics['f']\n",
    "        \n",
    "        count_precision += current_precision\n",
    "        count_recall += current_recall        \n",
    "        count_f1 += current_f1\n",
    "\n",
    "final_precision = count_precision/total_lines\n",
    "final_recall = count_recall/total_lines\n",
    "final_f1 = count_f1/total_lines\n",
    "\n",
    "print(final_precision)\n",
    "print(final_recall)\n",
    "print(final_f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
