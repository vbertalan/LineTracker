from pathlib import Path
from typing import *  # type: ignore
import json
import re
import datetime
import gc
import os
from itertools import product
from textwrap import wrap
import warnings
import argparse
import abc
import shutil
import logging
import subprocess
from typing import Any, Dict, List, Optional, Tuple, Union
import psutil
import multiprocessing as mp
import functools
import random
import fire
import torch
from torch import nn
import itertools as it
import sklearn.preprocessing as skPrepro
import sklearn.metrics as skMetrics
from sklearn.feature_extraction.text import TfidfVectorizer
import uuid
import pandas as pd
import contextlib
import hashlib
from sklearn.cluster import DBSCAN
import ctypes
import functools as ft
from sklearn.metrics import adjusted_rand_score, silhouette_score, jaccard_score
from sklearn.metrics.cluster import rand_score
from sklearn.metrics.pairwise import pairwise_distances
from contextlib import redirect_stdout
from io import StringIO

try:
    import DrainMethod
    import drainmethod_functionnal as drain_func
except ImportError:
    import src.DrainMethod as DrainMethod
    import src.drainmethod_functionnal as drain_func

"""File of code: disclaimer functions comes from the repository https://github.com/AndressaStefany/severityPrediction"""
# tye hints
LlamaTokenizer = Union["trf.LlamaTokenizer", "trf.LlamaTokenizerFast"]
LlamaModel = "trf.LlamaForCausalLM"
PoolingOperationCode = Literal["mean", "sum"]
PoolingFn = Callable[["torch.Tensor"], "torch.Tensor"]
ModelName = Literal["meta-llama/Llama-2-13b-chat-hf", "meta-llama/Llama-2-7b-chat-hf"]
DatasetName = Literal["eclipse_72k", "mozilla_200k"]
BugId: int
ParserTypes = Literal["drain"]
EmbedderType = Literal["llama-13b", "tfidf"]
EmbeddingDistanceType = Literal["cosine", "euclidean"]
ClusteringType = Literal["kmedoid", "dbscan"]
# typehint imports
import transformers as trf
import torch
import torch.nn as nn
import torch.utils.data as dt
import huggingface_hub
import pandas as pd
import numpy as np
import peft
import trl
import matplotlib.pyplot as plt
import matplotlib
import sklearn.metrics as skMetr
import sklearn.model_selection as skMsel
import tqdm
import datasets
import h5py
import bitsandbytes as bnb
import evaluate  # type: ignore
import optuna
import accelerate



class LogData(TypedDict):
    """
    - event_id: str, Unique string per bug_id
    - text: str, Text of the error
    - line_num: str, Plan id of the log: with log_name constitute the build_log
    """

    event_id: str
    text: str
    line_num: str


class TripletMatrix(TypedDict):
    """
    - variables_matrix: np.ndarray, matrix distances
    - embeddings_matrix: np.ndarray, embeddings distances
    - count_matrix: np.ndarray, count of line distances
    """

    variables_matrix: np.ndarray
    embeddings_matrix: np.ndarray
    count_matrix: np.ndarray


class TripletCoef(TypedDict):
    """
    - coef_variables_matrix: coefficient for the matrix distances
    - coef_embeddings_matrix: coefficient for the embeddings distances
    - coef_count_matrix: coefficient for the count of line distances
    """

    coef_variables_matrix: float
    coef_embeddings_matrix: float
    coef_count_matrix: float


class LineTrackerException(Exception):
    """Base class for the LineTracker exceptions"""


class EmptyLog(LineTrackerException):
    """Exception when no logs is found"""

    def __init__(self, msg, logs: List[Dict[str, Any]]):
        super().__init__(msg)
        self.logs = logs


class NoVariable(LineTrackerException):
    """Exception when no variables are inside log lines"""

    def __init__(self, msg, logs: List[Dict[str, Any]]):
        super().__init__(msg)
        self.logs = logs


class ParsingOutput(TypedDict):
    """
    - event_id: str, Unique string per bug_id
    - template: str, the template used in this event
    - variables: List[str], the variables in this event
    """

    event_id: str
    template: str
    variables: List[str]


class LogEmbeddingData(TypedDict):
    """
    - event_id: str, Unique string per bug_id
    - text: str, the source text provided to the model to make the embedding
    - embedding: np.ndarray, the embedding generated by the model
    """

    event_id: str
    text: str
    embedding: np.ndarray

class ClusteringAlgorithmOutput(TypedDict):
    type: str
    clustering: Dict[int, int]
    hyperparameters: Dict[str, Any]
    
class ClusteringAlgorithmOutputKMedoid(ClusteringAlgorithmOutput):
    type: str
    clustering: Dict[int, int]
    hyperparameters: Dict[str, Any]
    score: float

def get_parser_fn(
    parser_type: ParserTypes,
) -> Callable[[List[LogData]], List[ParsingOutput]]:
    """Get the function for the parser"""
    if parser_type == "drain":
        return lambda events: get_parsing_drainparser(
            events,
            depth=5,
            similarity_threshold=0.4,
            max_children=3,
        )
    else:
        raise ValueError(f"Expecting parser type to be among {','.join(get_args(ParserTypes))}")

def get_embedder(embedder_type: EmbedderType) -> Callable[[List[LogData]], Generator[LogEmbeddingData, None, None]]:
    """Get the function to generate the embeddings"""
    if embedder_type == "llama-13b":
        return get_embedder_fn(embedder_type, pooling_code='mean', model_name='meta-llama/Llama-2-13b-chat-hf',limit_tokens=1000)
    elif embedder_type == "tfidf":
        return generate_tfidf_embeddings
    else:
        raise ValueError(f"Expecting embedder_type to be among {','.join(get_args(EmbedderType))}")

def get_emb_dist_fn(embedding_distance: EmbeddingDistanceType) -> Callable[[np.ndarray], np.ndarray]:
    """Get the function to generate the normalized (0-1) embeddings distances from the embeddings"""
    if embedding_distance == 'cosine':
        # in case of cosine distance that can take values between 0 and 2 we divide the result by 2 to come back between 0 and 1
        return lambda data: get_distance_matrix(data, metric=embedding_distance)/2
    elif embedding_distance == 'euclidean':
        # in case of euclidean distance that can take values unbouded we do a standard 0-1 normalization
        def fn_euclidean(data):
            d = get_distance_matrix(data, metric=embedding_distance)
            return (d-np.min(d))/(np.max(d)-np.min(d))
        return fn_euclidean
    else:
        raise ValueError(f"Expecting embedding_distance to be among {','.join(get_args(EmbeddingDistanceType))}")
        
def get_clustering_fn(clustering_type: ClusteringType, must_link: Optional[List[Tuple[int,int]]] = None, cannot_link: Optional[List[Tuple[int,int]]] = None, epsilon: Optional[float] = None) -> Callable[[np.ndarray], ClusteringAlgorithmOutput]:
    """Get the function to generate the clustering from the combined distance matrix"""
    if clustering_type == 'kmedoid':
        assert epsilon is None, "epsilon is not used for kmedoid"
        return lambda combined_matrix: best_clustering_kmedoid(combined_matrix, must_link=must_link, cannot_link=cannot_link)
    elif clustering_type == 'dbscan':
        assert (must_link is None and cannot_link is None), f"For dbscan must_link and cannot_link must be None as this option is not supported ({must_link=}, {cannot_link=})"
        assert epsilon is not None
        return lambda combined_matrix: clustering_dbscan(combined_matrix, epsilon=epsilon)
    else:
        raise ValueError(f"Expecting embedding_distance to be among {','.join(get_args(ClusteringType))}")


def execute_full_pipeline(
    logs: List[LogData],
    triplet_coefficient: TripletCoef,
    parser: Callable[[List[LogData]], List[ParsingOutput]],
    embedder: Callable[[List[LogData]], Generator[LogEmbeddingData, None, None]],
    emb_dist_fn: Callable[[np.ndarray], np.ndarray],
    clustering_fn: Callable[[np.ndarray], ClusteringAlgorithmOutput],
    float_precision: type = np.float32,
) -> ClusteringAlgorithmOutput:
    """Cluster logs provided in argument into groups of related log lines
    # Arguments
    - logs: List[LogData], the log lines
    - triplet_coefficient: TripletCoef, the three coefficients to use to ponderate the matrices
    - parser: Callable[[List[LogData]], List[ParsingOutput]], a function that from the list of logs lines can generate for each line
    - embedder: Callable[[List[LogData]], Generator[LogEmbeddingData, None, None]], the function that can generate embeddings from logs
    - emb_dist_fn: Callable[[np.ndarray], np.ndarray], given all embeddings of each log lines of the same log file, generate the normalized (between 0 and 1) distances between all embeddings
    - clustering_fn:  Callable[[np.ndarray], ClusteringAlgorithmOutput], taking the combined matrix with the coefficients provided, clusters the logs
    - float_precision: type = np.float32, the precision to use for all floating point matrices
    """
    # 1. parse the logs
    parsed_logs = parser(logs)
    # 2. build the variable matrix (alreay normalized matrix as it has values between 0 and 1)
    variables_matrix = get_variable_matrix(parsed_logs, variable_field="variables")
    variables_distance_matrix: np.ndarray = get_distance_matrix(  #
        variables_matrix,
        metric="jaccard",
    ).astype(float_precision)
    del variables_matrix
    # 3. build the embeddings
    embeddings: np.ndarray = np.array(
        [embedding["embedding"] for embedding in embedder(logs)]
    ).astype(float_precision)
    # 4. build the distance matrix
    embeddings_distance_matrix = emb_dist_fn(embeddings).astype(float_precision)
    max_v, min_v = np.max(embeddings_distance_matrix), np.min(
        embeddings_distance_matrix
    )
    assert (
        max_v <= 1 and min_v >= 0
    ), f"Expecting the matrix to be normalized with values in the interval [0,1] but found values of embeddings distance between [{min_v},{max_v}]. check your emb_dist_fn"
    del embeddings
    # 5. build the count matrix
    count_matrix = get_count_distance_matrix(logs, count_matrix_mode="absolute").astype(
        float_precision
    )
    # 6. merge the matrices with triplet coefficient
    combined_matrix = combine_matrices(
        TripletMatrix(
            variables_matrix=variables_distance_matrix,
            embeddings_matrix=embeddings_distance_matrix,
            count_matrix=count_matrix,
        ),
        triplet_coef=triplet_coefficient,
    ).astype(float_precision)
    # note: values will be between 0 and 3 (addition of 3 matrices normalized between 0 and 3)
    del variables_distance_matrix
    del embeddings_distance_matrix
    # 7. run the clustering algorithm with the constraints
    clustering_output = clustering_fn(combined_matrix)
    # 8. return the result
    return clustering_output
