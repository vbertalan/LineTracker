{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "################################## LIBRARIES ##################################\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from ast import literal_eval\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import DrainMethod\n",
    "import contextlib\n",
    "import pickle\n",
    "import os\n",
    "from rouge import Rouge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "############################## AUXILIARY METHODS ##############################\n",
    "\n",
    "# Code for reading HuggingFace token\n",
    "def get_huggingface_token():\n",
    "    return Path(\"huggingface_token.txt\").read_text().strip()\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx, 'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers, regex = [], ''\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    for i, splitter in enumerate(splitters):\n",
    "        if i % 2 == 0:\n",
    "            regex += re.sub(' +', '\\\\\\s+', splitter)\n",
    "        else:\n",
    "            header = splitter.strip('<>').strip()\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    return headers, re.compile('^' + regex + '$')\n",
    "\n",
    "# Function to transform log file to dataframe\n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin:\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                log_messages.append([match.group(header) for header in headers])\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', range(1, len(logdf) + 1))\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    if Path(path_to_file).is_file():\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        tr_idf_model = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates distance matrix, using Euclidean distance\n",
    "def create_distance_matrix(vector_df):\n",
    "    tfidf_distance = pairwise_distances(vector_df, metric=\"euclidean\", n_jobs=-1)\n",
    "    return (tfidf_distance - tfidf_distance.min()) / (tfidf_distance.max() - tfidf_distance.min())\n",
    "\n",
    "# Creates variable matrix, using Jaccard distance\n",
    "def create_variable_matrix():\n",
    "    output_csv = os.path.join(os.getcwd(), \"results\", log_file + '_structured.csv')\n",
    "    var_df = pd.read_csv(output_csv)[\"ParameterList\"]\n",
    "    var_df = var_df.apply(literal_eval)\n",
    "    mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "    var_distance = pairwise_distances(mlb.fit_transform(var_df).todense(), metric=\"jaccard\", n_jobs=-1)\n",
    "    return var_distance\n",
    "\n",
    "# Creates closeness matrix\n",
    "def creates_closeness_matrix(tfidf_distance):\n",
    "    n = len(tfidf_distance)\n",
    "    count_distance = np.abs(np.subtract.outer(range(n), range(n)))\n",
    "    return (count_distance - count_distance.min()) / (count_distance.max() - count_distance.min())\n",
    "\n",
    "# Saves matrices to files\n",
    "def saves_matrices(distance_mat, variable_mat, closeness_mat):\n",
    "    np.save(f\"tfidf_distance_{logName}.csv\", distance_mat)\n",
    "    np.save(f\"var_distance_{logName}.csv\", variable_mat)\n",
    "    np.save(f\"count_distance_{logName}.csv\", closeness_mat)\n",
    "\n",
    "# Combines matrices using weights\n",
    "def joins_matrices(tfidf_distance, var_distance, count_distance, alpha, beta, gamma):\n",
    "    if alpha + beta + gamma > 1:\n",
    "        raise ValueError(\"Values must sum to 1!\")\n",
    "    return alpha * tfidf_distance + beta * var_distance + gamma * count_distance\n",
    "\n",
    "# Clusters with KMedoids\n",
    "def cluster_kmedoids(unified_matrix, cluster_num):\n",
    "    clusterer = KMedoids(n_clusters=cluster_num, method='pam', init='random').fit(unified_matrix)\n",
    "    return clusterer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
