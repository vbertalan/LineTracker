{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforms files for general topic modeling\n",
    "\n",
    "import re\n",
    "\n",
    "dataset = \"Zookeeper\"\n",
    "file = open(\"ground_truths/\" + dataset + \".txt\", \"r\")\n",
    "LINE_PATTERN = \"#[0-9]+#\"\n",
    "SUMMARY_PATTERN = \"#summary:#\"\n",
    "line_file = []\n",
    "summary_file = []\n",
    "line_match = True\n",
    "summary_match = True\n",
    "line_count = 0\n",
    "\n",
    "for line in file:\n",
    "\n",
    "        if len(line) == 1:                  \n",
    "            summary_match = False\n",
    "            line_match = False\n",
    "            continue\n",
    "        elif re.match(LINE_PATTERN, line):            \n",
    "            line_match = True\n",
    "            continue\n",
    "        elif re.match(SUMMARY_PATTERN, line):\n",
    "            summary_match = True\n",
    "            continue\n",
    "        elif (line_match):\n",
    "            line_file.append(line)\n",
    "            line_count += 1\n",
    "            continue\n",
    "        elif (summary_match):\n",
    "            for elem in range(line_count):                \n",
    "                summary_file.append(line.rstrip() + '\\n')\n",
    "            line_count = 0   \n",
    "            continue\n",
    "\n",
    "with open (\"ground_truths/\" + dataset + \"_lines.txt\", \"w\") as f:\n",
    "     for line in line_file:\n",
    "          f.write(f\"{line}\")\n",
    "\n",
    "with open (\"ground_truths/\" + dataset + \"_summaries.txt\", \"w\") as f:\n",
    "     for line in summary_file:\n",
    "          f.write(f\"{line}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transforms files for general topic modeling, parsing before\n",
    "\n",
    "## Drain parameters\n",
    "\n",
    "import DrainMethod\n",
    "import os\n",
    "\n",
    "## Step 1 - Log Parsing Using Drain\n",
    "\n",
    "dataset = \"Zookeeper\"\n",
    "input_dir = os.path.join(os.getcwd(), \"ground_truths/\") # The input directory of raw logs\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "output_dir = input_dir  # The output directory of parsing results\n",
    "logName = dataset + \"_lines.txt\" # Name of file to be parsed\n",
    "#file = open(\"ground_truths/\" + dataset + \"_lines.txt\", \"r\")\n",
    "regex = [] # Regex strings for Drain execution\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "st = 0.6 # Drain similarity threshold\n",
    "\n",
    "\n",
    "## Code\n",
    "\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')   \n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method to find the most representative line inside the cluster\n",
    "\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "tk = WhitespaceTokenizer()\n",
    "\n",
    "## raw_lines = list of lines inside LogSummary's cluster\n",
    "## word_list = list of tokens composed by the LDA/BertTopic\n",
    "def find_best_line(raw_lines, word_list):\n",
    "    closest_line = 0\n",
    "    similar_tokens = 0\n",
    "    max_similarity = 0\n",
    "    for idx, line in enumerate(raw_lines):\n",
    "        tokenized_line = tk.tokenize(line.lower())\n",
    "        for token in tokenized_line:\n",
    "            if token in word_list:\n",
    "                similar_tokens += 1\n",
    "        #print (\"Line {} has {} identical tokens\".format(idx, similar_tokens))\n",
    "        if similar_tokens > max_similarity:\n",
    "           max_similarity = similar_tokens\n",
    "           closest_line = idx\n",
    "        similar_tokens = 0\n",
    "    print(\"The closest line is {}, with {} identical tokens to the topic\".format(closest_line, max_similarity)) \n",
    "    print(\"Line {} is: {}\".format(closest_line, raw_lines[closest_line]))   \n",
    "    return (raw_lines[closest_line])        \n",
    "\n",
    "#find_similar_lines(cluster_lines, 0, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LDA Tests\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import gensim\n",
    "\n",
    "dataset = \"Zookeeper\"\n",
    "csv = pd.read_csv(\"ground_truths/Zookeeper_lines.txt_structured.csv\")\n",
    "content = csv[\"EventTemplate\"]\n",
    "num_topics = 10\n",
    "line_file = []\n",
    "line_set = []\n",
    "\n",
    "# Converts sentences to words\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "for idx, line in enumerate(content):\n",
    "    line_set.append(line + '\\n')\n",
    "\n",
    "    if (idx % 20 == 19):\n",
    "  \n",
    "        # Converts to words\n",
    "        data_words = list(sent_to_words(line_set))\n",
    "        # Creates dictionary\n",
    "        id2word = corpora.Dictionary(data_words)\n",
    "        # Creates corpora\n",
    "        corpus = [id2word.doc2bow(text) for text in data_words]\n",
    "        # Builds LDA model\n",
    "        lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=id2word,num_topics=num_topics)\n",
    "        # Gets word topics\n",
    "        x = lda_model.show_topics(num_topics=1, num_words=10,formatted=False)\n",
    "        topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "        #Below Code Prints Only Words \n",
    "        for topic,words in topics_words:\n",
    "            summary =  \" \".join(words)    \n",
    "\n",
    "        # #Appends summary to general line file\n",
    "        # for num in range(20):\n",
    "        #     line_file.append(summary)\n",
    "\n",
    "        #Finds most representative line inside the cluster\n",
    "        best_line = find_best_line(line_set, summary)\n",
    "\n",
    "        for num in range(20):\n",
    "            line_file.append(summary)\n",
    "\n",
    "        line_set = []\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_lda_topics.txt\", \"w\") as f:\n",
    "     for line in line_file:\n",
    "          f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests with BerTopic\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "dataset = \"Zookeeper\"\n",
    "csv = pd.read_csv(\"ground_truths/Zookeeper_lines.txt_structured.csv\")\n",
    "content = csv[\"EventTemplate\"]\n",
    "num_topics = 10\n",
    "line_file = []\n",
    "line_set = []\n",
    "cluster_model = KMedoids(n_clusters = 1)\n",
    "topic_model = BERTopic(hdbscan_model=cluster_model)\n",
    "\n",
    "for idx, line in enumerate(content):\n",
    "\n",
    "    line_set.append(line + '\\n')\n",
    "\n",
    "    if (idx % 20 == 19):\n",
    "  \n",
    "        print(\"Chegamos ao idx {}\".format(idx))\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(line_set)\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        #Finds most representative line inside the cluster\n",
    "        best_line = find_best_line(line_set, summary)\n",
    "\n",
    "        for num in range(20):\n",
    "            line_file.append(summary)\n",
    "\n",
    "        line_set = []\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics.txt\", \"w\") as f:\n",
    "     for line in line_file:\n",
    "          f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tests with Bertopic, using own embeddings\n",
    "\n",
    "from sklearn_extra.cluster import KMedoids\n",
    "from bertopic import BERTopic\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance, TextGeneration\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from torch import bfloat16\n",
    "from torch import cuda\n",
    "#from umap import UMAP\n",
    "import umap.umap_ as UMAP\n",
    "import transformers\n",
    "import accelerate\n",
    "\n",
    "# Informações do dataset\n",
    "dataset = \"Zookeeper\"\n",
    "csv = pd.read_csv(\"ground_truths/Zookeeper_lines.txt_structured.csv\")\n",
    "content = csv[\"EventTemplate\"]\n",
    "\n",
    "# Models parameters\n",
    "model_id = 'meta-llama/Llama-2-13b-chat-hf'\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'; print(device)\n",
    "\n",
    "# # Quantization to load an LLM with less GPU memory\n",
    "# bnb_config = transformers.BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,  # 4-bit quantization\n",
    "#     bnb_4bit_quant_type='nf4',  # Normalized float 4\n",
    "#     bnb_4bit_use_double_quant=True,  # Second quantization after the first\n",
    "#     bnb_4bit_compute_dtype=bfloat16  # Computation type\n",
    "# )\n",
    "\n",
    "# System Prompt\n",
    "system_prompt = \"\"\"\n",
    "<s>[INST] <<SYS>>\n",
    "You are a helpful, respectful and honest assistant for labeling topics.\n",
    "<</SYS>>\n",
    "\"\"\"\n",
    "example_prompt = \"\"\"\n",
    "I have a topic that contains the following documents:\n",
    "- Traditional diets in most cultures were primarily plant-based with a little meat on top, but with the rise of industrial style meat production and factory farming, meat has become a staple food.\n",
    "- Meat, but especially beef, is the word food in terms of emissions.\n",
    "- Eating meat doesn't make you a bad person, not eating meat doesn't make you a good one.\n",
    "\n",
    "The topic is described by the following keywords: 'meat, beef, eat, eating, emissions, steak, food, health, processed, chicken'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "[/INST] Environmental impacts of eating meat\n",
    "\"\"\"\n",
    "# Our main prompt with documents ([DOCUMENTS]) and keywords ([KEYWORDS]) tags\n",
    "main_prompt = \"\"\"\n",
    "[INST]\n",
    "I have a topic that contains the following documents:\n",
    "[DOCUMENTS]\n",
    "\n",
    "The topic is described by the following keywords: '[KEYWORDS]'.\n",
    "\n",
    "Based on the information about the topic above, please create a short label of this topic. Make sure you to only return the label and nothing more.\n",
    "[/INST]\n",
    "\"\"\"\n",
    "# Final prompt\n",
    "prompt = system_prompt + example_prompt + main_prompt\n",
    "# Llama 2 Tokenizerumap\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "# Llama 2 Model\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    #quantization_config=bnb_config,\n",
    "    device_map='auto',\n",
    ")\n",
    "model.eval()\n",
    "# Our text generator\n",
    "generator = transformers.pipeline(\n",
    "    model=model, tokenizer=tokenizer,\n",
    "    task='text-generation',\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=500,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "# KeyBERT\n",
    "keybert = KeyBERTInspired()\n",
    "# MMR\n",
    "mmr = MaximalMarginalRelevance(diversity=0.3)\n",
    "# Text generation with Llama 2\n",
    "llama2 = TextGeneration(generator, prompt=prompt)\n",
    "# All representation models\n",
    "representation_model = {\n",
    "    \"KeyBERT\": keybert,\n",
    "    \"Llama2\": llama2,\n",
    "    \"MMR\": mmr,\n",
    "}\n",
    "# Pre-calculate embeddings\n",
    "embedding_model = SentenceTransformer(\"BAAI/bge-small-en\")\n",
    "#embeddings = embedding_model.encode(content, show_progress_bar=True)\n",
    "umap_model = UMAP.UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric='cosine', random_state=42)\n",
    "# Pre-reduce embeddings for visualization purposes\n",
    "reduced_embeddings = UMAP.UMAP(n_neighbors=15, n_components=2, min_dist=0.0, metric='cosine', random_state=42).fit_transform(embeddings)\n",
    "# Uses K-Medoids\n",
    "cluster_model = KMedoids(n_clusters = 1)\n",
    "\n",
    "topic_model = BERTopic(\n",
    "\n",
    "  # Sub-models\n",
    "  embedding_model=embedding_model,\n",
    "  umap_model=umap_model,\n",
    "  hdbscan_model=cluster_model,\n",
    "  representation_model=representation_model,\n",
    "\n",
    "  # Hyperparameters\n",
    "  top_n_words=10,\n",
    "  verbose=True\n",
    ")\n",
    "\n",
    "num_topics = 10\n",
    "line_file = []\n",
    "line_set = []\n",
    "\n",
    "for idx, line in enumerate(content):\n",
    "\n",
    "    line_set.append(line + '\\n')\n",
    "\n",
    "    if (idx % 20 == 19):\n",
    "  \n",
    "        print(\"Chegamos ao idx {}\".format(idx))\n",
    "\n",
    "        #Creates Embeddings\n",
    "        embeddings = embedding_model.encode(line_set, show_progress_bar=True)\n",
    "\n",
    "        #Applies BertTopic\n",
    "        topics, probs = topic_model.fit_transform(line_set, embeddings)\n",
    "\n",
    "        #Gets summary of topics\n",
    "        topic_model.get_topic(0)\n",
    "        top_topic = topic_model.get_topic(0)\n",
    "        words = [i[0] for i in top_topic]\n",
    "        summary = ' '.join(words)\n",
    "\n",
    "        #Finds most representative line inside the cluster\n",
    "        best_line = find_best_line(line_set, summary)\n",
    "\n",
    "        for num in range(20):\n",
    "            line_file.append(summary)\n",
    "\n",
    "        line_set = []\n",
    "\n",
    "## Writes external file with created topics\n",
    "with open (\"ground_truths/\" + dataset + \"_bert_topics.txt\", \"w\") as f:\n",
    "     for line in line_file:\n",
    "          f.write(f\"{line}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import accelerate as ac\n",
    "\n",
    "print(ac.__version__)\n",
    "# 2.0.1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
