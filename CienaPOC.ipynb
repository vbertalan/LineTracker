{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 0 - Loading Parameters and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import DrainMethod\n",
    "import sys\n",
    "import os\n",
    "\n",
    "## General parameters \n",
    "\n",
    "input_dir = os.path.join(os.getcwd(), \"logs\") # The input directory of raw logs\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "vector_dir = os.path.join(os.getcwd(), \"vectors\")  # The vector directory of converted logs\n",
    "\n",
    "#logName = 'Ciena_error_lin0es_20220701-20220715.txt' # Name of file to be parsed\n",
    "logName = 'ciena-mini.txt' # Name of file to be parsed\n",
    "#logName = 'Andriod_2k.log' # Name of file to be parsed\n",
    "#logName = 'trat3_production_1650_1700_20231411_raw.txt' # Name of file to be parsed\n",
    "\n",
    "log_format = '<Content>' # Format of the file, if there are different fields\n",
    "regex = [] # Regex strings for Drain execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1 - Log Parsing Using Drain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Starting Drain Parsing ===\n",
      "c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker\\LineTracker\\logs\\\n",
      "Parsing file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker\\LineTracker\\logs\\ciena-mini.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing Progress: 100%|██████████| 15710/15710 [00:01<00:00, 11755.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. [Time taken: 0:00:03.776132]\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "## Drain parameters\n",
    "st = 0.5 # Drain similarity threshold\n",
    "depth = 5 # Max depth of the parsing tree\n",
    "\n",
    "## Parses file, using DrainMethod\n",
    "print('\\n=== Starting Drain Parsing ===')\n",
    "indir = os.path.join(input_dir, os.path.dirname(logName))\n",
    "print(indir)\n",
    "log_file = os.path.basename(logName)\n",
    "\n",
    "parser = DrainMethod.LogParser(log_format=log_format, indir=indir, outdir=output_dir, rex=regex, depth=depth, st=st)\n",
    "parser.parse(log_file)\n",
    "\n",
    "parsedresult=os.path.join(output_dir, log_file + '_structured.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2 - Creating Embeddings Using TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforming file: c:\\Users\\vbert\\OneDrive\\DOUTORADO Poly Mtl\\Projeto\\LineTracker\\LineTracker\\logs\\ciena-mini.txt\n",
      "Using Previous Embeddings File\n",
      "The dimensions of the embedding matrix are (15710, 8224)\n"
     ]
    }
   ],
   "source": [
    "## Step 2 - Vector Creation Using TFIDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import regex as re\n",
    "import contextlib\n",
    "import pickle\n",
    "\n",
    "# Calls conversion from data to dataframe\n",
    "def load_data():\n",
    "    headers, regex = generate_logformat_regex(log_format)\n",
    "    return log_to_dataframe(os.path.join(indir, logName), regex, headers, log_format)\n",
    "\n",
    "# Preprocesses dataframe with regexes, if necessary - more preprocessing to add\n",
    "def preprocess_df(df_log):\n",
    "    for idx, content in df_log[\"Content\"].items():\n",
    "        for currentRex in regex:\n",
    "            df_log.at[idx,'Content'] = re.sub(currentRex, '<*>', content)\n",
    "    return df_log\n",
    "\n",
    "# Function to generate regular expression to split log messages\n",
    "def generate_logformat_regex(log_format):\n",
    "    headers = []\n",
    "    splitters = re.split(r'(<[^<>]+>)', log_format)\n",
    "    regex = ''\n",
    "    for k in range(len(splitters)):\n",
    "        if k % 2 == 0:\n",
    "            splitter = re.sub(' +', '\\\\\\s+', splitters[k])\n",
    "            regex += splitter\n",
    "        else:\n",
    "            header = splitters[k].strip('<').strip('>')\n",
    "            regex += f'(?P<{header}>.*?)'\n",
    "            headers.append(header)\n",
    "    regex = re.compile('^' + regex + '$')\n",
    "    return headers, regex\n",
    "\n",
    "# Function to transform log file to dataframe \n",
    "def log_to_dataframe(log_file, regex, headers, logformat):\n",
    "    log_messages = []\n",
    "    linecount = 0\n",
    "    with open(log_file, 'r') as fin:\n",
    "        for line in fin.readlines():\n",
    "            with contextlib.suppress(Exception):\n",
    "                match = regex.search(line.strip())\n",
    "                message = [match.group(header) for header in headers]\n",
    "                log_messages.append(message)\n",
    "                linecount += 1\n",
    "    logdf = pd.DataFrame(log_messages, columns=headers)\n",
    "    logdf.insert(0, 'LineId', None)\n",
    "    logdf['LineId'] = [i + 1 for i in range(linecount)]\n",
    "    return logdf\n",
    "\n",
    "# Transforms the dataset, creating raw vector file\n",
    "def transform_dataset(raw_content):\n",
    "    \n",
    "    path_to_file = os.path.join(vector_dir, logName + '_vectors_TFIDF.vec')\n",
    "    path = Path(path_to_file)\n",
    "    vectors_tfidf = []\n",
    "\n",
    "    # Saves transformed file, for further executions\n",
    "    if (path.is_file()):\n",
    "        print(\"Using Previous Embeddings File\")\n",
    "        vectors_tfidf = pickle.load(open(path_to_file, 'rb'))\n",
    "    else:\n",
    "        # Using TFIDF Vectorizer \n",
    "        print(\"Starting Encoding\")\n",
    "        tr_idf_model  = TfidfVectorizer()\n",
    "        vectors_tfidf = tr_idf_model.fit_transform(raw_content)\n",
    "        pickle.dump(vectors_tfidf, open(path_to_file, 'wb'))\n",
    "    \n",
    "    return vectors_tfidf\n",
    "\n",
    "# Creates embeddings for log file\n",
    "def transform(logName):\n",
    "    print('Transforming file: ' + os.path.join(input_dir, logName))\n",
    "    log_df = load_data()\n",
    "    log_df = preprocess_df(log_df)\n",
    "    return transform_dataset(log_df[\"Content\"])\n",
    "\n",
    "vector_df = transform(os.path.basename(logName))\n",
    "print(\"The dimensions of the embedding matrix are ({}, {})\".format(vector_df.shape[0], vector_df.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Creating Distance Matrix, Using Cosine Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the distance matrix are (15710, 15710)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import numpy as np\n",
    "\n",
    "tfidf_distance = cosine_distances(vector_df)\n",
    "print(\"The dimensions of the distance matrix are {}\".format(tfidf_distance.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 - Creating Matrix of Parsed Items (Variable Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of the variable matrix are (15710, 15710)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from ast import literal_eval\n",
    "import pandas as pd \n",
    "\n",
    "## General Parameters\n",
    "output_dir = os.path.join(os.getcwd(), \"results\")  # The output directory of parsing results\n",
    "output_csv = os.path.join(output_dir, log_file + '_structured.csv') \n",
    "\n",
    "# Reads parameters list\n",
    "full_df = pd.read_csv(output_csv)\n",
    "var_df = full_df[\"ParameterList\"]\n",
    "\n",
    "# Breaks the string into lists\n",
    "for i, line in var_df.items():\n",
    "    var_df.at[i] = literal_eval(var_df.at[i])\n",
    "\n",
    "# Transforms variable list to variable SPARSE matrix\n",
    "mlb = MultiLabelBinarizer(sparse_output=True)\n",
    "var_df = mlb.fit_transform(var_df)\n",
    "\n",
    "# Code for Jaccard Distance\n",
    "def jaccard_distance(mat):\n",
    "    rows_sum = mat.getnnz(axis=1)\n",
    "    ab = mat * mat.T\n",
    "\n",
    "    # For rows\n",
    "    aa = np.repeat(rows_sum, ab.getnnz(axis=1))\n",
    "    # For columns\n",
    "    bb = rows_sum[ab.indices]\n",
    "\n",
    "    # Calculates Jaccard similarity\n",
    "    similarities = ab.copy()\n",
    "    similarities.data = similarities.data/(aa + bb - ab.data)\n",
    "\n",
    "    # Calculated Jaccard distance\n",
    "    distance = 1 - similarities.todense()\n",
    "\n",
    "    return distance\n",
    "\n",
    "var_distance = jaccard_distance(var_df)\n",
    "print(\"The dimensions of the variable matrix are {}\".format(var_distance.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4 - Creates Closeness Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
